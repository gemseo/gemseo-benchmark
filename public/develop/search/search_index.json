{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#gemseo-benchmark","title":"gemseo-benchmark","text":""},{"location":"#overview","title":"Overview","text":"<p>A GEMSEO-based package to benchmark optimization algorithms.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest version with <code>pip install gemseo-benchmark</code>.</p> <p>See pip for more information.</p>"},{"location":"#bugs-and-questions","title":"Bugs and questions","text":"<p>Please use the gitlab issue tracker to submit bugs or questions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the contributing section of GEMSEO.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Benoit Pauwels</li> <li>Antoine Dechaume</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes of this project will be documented here.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#version-400-august-2025","title":"Version 4.0.0 (August 2025)","text":""},{"location":"changelog/#added","title":"Added","text":""},{"location":"changelog/#problems","title":"Problems","text":"<ul> <li>Multidisciplinary analysis problem configurations   can now be implemented with <code>MDAProblemConfiguration</code>.</li> <li>Multidisciplinary optimization problem configurations   can now be implemented with <code>MDOProblemConfiguration</code>.</li> </ul>"},{"location":"changelog/#report","title":"Report","text":"<ul> <li>The plot options (ex: color, marker) of each algorithm configuration   can now be customized at the execution of a <code>Scenario</code>   thanks to the new argument <code>plot_settings</code>.</li> <li>The user can now request that <code>Scenario.execute</code> or <code>Report.generate</code>   plot only the median of the performance measure rather than its whole range   thanks to the new boolean argument <code>plot_only_median</code>.</li> <li>On the page dedicated to the benchmarking problems,   the infeasibility measure of infeasible target values is now displayed.</li> <li>Graphs and tables have been added to the pages dedicated to each problem:   they show a focus on the performance measure near the target values,   the execution time,   the infeasibility measure,   and the number of unsatisfied constraints.</li> <li>Pages dedicated to the results of each algorithm configuration on each problem   have been added.   They feature graphs and tables representing the performance measure,   the infeasibility measure, and the number of unsatisfied constraints.</li> <li>The scale of the axis showing the number of function evaluations   can now be made logarithmic.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>The phrasing \"problem configuration\" is now used instead of \"benchmarking problem\".   Thus the two main types of inputs to be defined by the user   are algorithm configurations and problem configurations.</li> </ul>"},{"location":"changelog/#problems_1","title":"Problems","text":"<ul> <li>The class to implement optimization benchmarking problems is now called   <code>OptimizationBenchmarkingProblem</code> (rather than <code>Problem</code> formerly).</li> </ul>"},{"location":"changelog/#benchmarker","title":"Benchmarker","text":"<ul> <li>Argument <code>databases_path</code> of <code>Benchmarker.__init__</code> is renamed into <code>hdf_path</code>   as the saved files could represent caches rather than databases.</li> <li>Arguments <code>problems</code> and <code>algorithm</code> of <code>Benchmarker.execute</code> are renamed into   <code>problem_configurations</code> and <code>algorithm_configurations</code>   to avoid confusion with optimization problems and algorithm names respectively.</li> <li>Argument <code>number_of_processes</code> of <code>Benchmarker.execute</code> is renamed into   <code>n_processes</code> for consistency with GEMSEO.</li> <li>The stopping criteria of the algorithms are no longer automatically disabled.   The user is now free to disable (or not) the stopping criteria of their choice   in the options of the algorithm configurations.</li> </ul>"},{"location":"changelog/#report_1","title":"Report","text":"<ul> <li>The results on each problem are now displayed on separate pages   rather than on the page of the problems group.</li> <li>Setting the optimum of a problem is no longer mandatory.</li> <li>The performance histories returned by <code>PerformanceHistory.compute_cumulated_minimum</code>   and <code>PerformanceHistory.extend</code> now contain copies of history items   rather than replications of the same objects.</li> </ul>"},{"location":"changelog/#scenario","title":"Scenario","text":"<ul> <li>Argument <code>number_of_processes</code> of <code>Scenario.execute</code> is renamed into   <code>n_processes</code> for consistency with GEMSEO.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":""},{"location":"changelog/#results","title":"Results","text":"<ul> <li>Removing leading infeasible items from an infeasible performance history   now returns an empty performance history.</li> <li>Path options are now properly supported.</li> </ul>"},{"location":"changelog/#report_2","title":"Report","text":"<ul> <li>The performance measures and target values of maximization problems   are now correctly displayed instead of being treated as minimization data.</li> <li>Negative performance measures are now properly represented on logarithmic scales.</li> </ul>"},{"location":"changelog/#benchmarker_1","title":"Benchmarker","text":"<ul> <li>When overwriting histories,   the paths already in the <code>Results</code> are now effectively removed.</li> <li>When threading, a log file is written in the performance history directory.</li> <li>When multiprocessing, a log file is written next to each performance history.</li> </ul>"},{"location":"changelog/#scenario_1","title":"Scenario","text":"<ul> <li>An algorithm configuration can now belong to several groups   of algorithm configurations.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":""},{"location":"changelog/#problems_2","title":"Problems","text":"<ul> <li> <p>Method <code>Problem.plot_histories</code> was removed as it was redundant with <code>Figures.plot</code>.   To obtain a figure similar to the one formerly returned by   <pre><code>problem.plot_histories(\n  algos_configurations,\n  results,\n  False,\n  file_path,\n  plot_all_histories,\n  alpha,\n  markevery,\n  infeasibility_tolerance,\n  max_eval_number,\n  use_log_scale\n)\n</code></pre>   one can make the following call instead:   <pre><code>Figures(\n  algos_configurations,\n  ProblemsGroup(problem.name, [problem]),\n  results,\n  file_path.parent,\n  infeasibility_tolerance,\n  max_eval_number,\n  {\"alpha\": alpha, \"markevery\": markevery}\n).plot(plot_all_histories, use_log_scale, False, False, False)\n</code></pre></p> </li> <li> <p>Method <code>Problem.compute_performance</code> was removed as is was redundant with <code>PerformanceHistory.from_problem</code>.   To obtain values similar to the former   <pre><code>objective_values, infeasibility_measures, feasibility_statuses = Problem.compute_performance(problem)\n</code></pre>   one can use the following instructions instead:   <pre><code>performance_history = PerformanceHistory.from_problem(problem)\nobjective_values = performance_history.objective_values\ninfeasibility_measures = performance_history.infeasibility_measures\nfeasibility_statuses = [item.is_feasible for item in performance_history]\n</code></pre></p> </li> </ul>"},{"location":"changelog/#results_1","title":"Results","text":"<ul> <li>Methods <code>PerformanceHistories.plot_algorithm_histories</code>   and <code>PerformanceHistory.plot</code> were removed   as they were redundant with <code>Figures.plot</code>.   To obtain a figure similar to the one formerly returned by   <pre><code>performance_histories.plot_algorithm_histories(\n  axes,\n  algorithm_name,\n  max_feasible_objective,\n  plot_all,\n  color,\n  marker,\n  alpha,\n  markevery\n)\n</code></pre>   one can use the following instructions instead:   <pre><code>results = Results()\nfor index, performance_history in enumerate(performance_histories):\n  path = f\"{index}.json\"\n  performance_history.to_file(path)\n  results.add_path(algorithm_name, problem.name, path)\n\nFigures(\n  AlgorithmsConfigurations(AlgorithmConfiguration(algorithm_name)),\n  ProblemsGroup(problem.name, [problem]),\n  results,\n  \".\",\n  0,\n  0,\n  {\"color\": color, \"marker\": marker, \"alpha\": alpha, \"markevery\": markevery}\n).plot(plot_all, False, False, False, False)\n</code></pre></li> </ul>"},{"location":"changelog/#version-300-november-2024","title":"Version 3.0.0 (November 2024)","text":""},{"location":"changelog/#added_1","title":"Added","text":""},{"location":"changelog/#benchmarker_2","title":"Benchmarker","text":"<ul> <li>The option <code>log_gemseo_to_file</code> has been added to <code>Benchmarker.execute</code>   and <code>Scenario.execute</code> to save the GEMSEO log of each algorithm execution   to a file in the same directory as its performance history file.</li> </ul>"},{"location":"changelog/#data-profiles","title":"Data profiles","text":"<ul> <li>Target values can be plotted on existing axes as horizontal lines with   <code>TargetValues.plot_on_axes</code>.</li> </ul>"},{"location":"changelog/#results_2","title":"Results","text":"<ul> <li>The distribution of a collection of performance histories can be plotted in terms of   performance measure (<code>PerformanceHistories.plot_performance_measure_distribution``),   infeasibility measure (</code>PerformanceHistories.plot_infeasibility_measure_distribution<code>)   and number of unsatisfied constraints   (```PerformanceHistories.plot_number_of_unsatisfied_constraints_distribution</code>).</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":""},{"location":"changelog/#results_3","title":"Results","text":"<ul> <li>Methods   <code>PerformanceHistory.compute_cumulated_minimum</code>,   <code>PerformanceHistory.extend</code>,   <code>PerformanceHistory.remove_leading_infeasible</code>,   and <code>PerformanceHistory.shorten</code>   preserve the attributes other than <code>PerformanceHistory.items</code>.</li> </ul>"},{"location":"changelog/#version-200-december-2023","title":"Version 2.0.0 (December 2023)","text":""},{"location":"changelog/#changed_2","title":"Changed","text":""},{"location":"changelog/#benchmarker_3","title":"Benchmarker","text":"<ul> <li>The option to automatically save the logs of pSeven has been removed   from classes <code>Scenario</code> and <code>Benchmarker</code>.   However, the user can still save these logs   by passing an instance-specific option to <code>AlgorithmConfiguration</code>   (refer to the \"Added\" section of the present changelog).   For example:   <code>instance_algorithm_options   ={\"log_path\": lambda problem, index: f\"my/log/files/{problem.name}.{index}.log\"}</code>.   N.B. the user is now responsible for the creation of the parent directories.</li> <li>Class <code>Worker</code> no longer sets <code>PerformanceHistory.doe_size</code>   to the length of the value of the pSeven option <code>\"sample_x\"</code>.   Note that this does not affect the behavior of <code>gemseo-benchmark</code>:   <code>PerformanceHistory.doe_size</code> is only used as convenience   when loading/saving a <code>PerformanceHistory</code> using a file.   In particular, the behavior of <code>Report</code> is not changed.   The user can still set the value of <code>PerformanceHistory.doe_size</code>   by themselves since it is a public attribute.</li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Support for Python 3.11.</li> </ul>"},{"location":"changelog/#algorithms","title":"Algorithms","text":"<ul> <li>Algorithm options specific to problem instances (e.g. paths for output files)   can be passed to <code>AlgorithmConfiguration</code> in the new argument <code>instance_algorithm_options</code>.</li> </ul>"},{"location":"changelog/#benchmarker_4","title":"Benchmarker","text":"<ul> <li>One can get the path to a performance history file with <code>Benchmarker.get_history_path</code>.</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li>Support for Python 3.8.</li> </ul>"},{"location":"changelog/#version-110-september-2023","title":"Version 1.1.0 (September 2023)","text":""},{"location":"changelog/#added_3","title":"Added","text":""},{"location":"changelog/#results_4","title":"Results","text":"<ul> <li>The names of functions and the number of variables are stored in the     performance history files.</li> </ul>"},{"location":"changelog/#report_3","title":"Report","text":"<ul> <li>The optimization histories can be displayed on a logarithmic scale.</li> </ul>"},{"location":"changelog/#scenario_2","title":"Scenario","text":"<ul> <li>The options <code>custom_algos_descriptions</code> and     <code>max_eval_number_per_group</code> of <code>Report</code>{.interpreted-text     role=\"class\"} can be passed through <code>Scenario</code>{.interpreted-text     role=\"class\"}.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":""},{"location":"changelog/#report_4","title":"Report","text":"<ul> <li>The sections of the PDF report are correctly numbered.</li> <li>The graphs of the PDF report are anchored to their expected     locations.</li> </ul>"},{"location":"changelog/#version-100-june-2023","title":"Version 1.0.0 (June 2023)","text":"<p>First version.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>These projects were used to build gemseo-benchmark. Thank you!</p> <p>Python | uv</p>"},{"location":"credits/#exec-1--runtime-dependencies","title":"Runtime dependencies","text":"Project Summary Version (accepted) Version (last resolved) License alabaster A light, configurable Sphinx theme <code>~=0.7.14</code> <code>0.7.16</code> BSD License annotated-types Reusable constraint types to use with typing.Annotated <code>&gt;=0.6.0</code> <code>0.7.0</code> MIT License babel Internationalization utilities <code>~=2.10, &gt;=2.13</code> <code>2.17.0</code> BSD-3-Clause certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2025.8.3</code> MPL-2.0 charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.3</code> MIT colorama Cross-platform colored terminal text. <code>~=0.4, &gt;=0.4.6</code> <code>0.4.6</code> BSD License contourpy Python library for calculating contours of 2D quadrilateral grids <code>&gt;=1.0.1</code> <code>1.3.3</code> BSD License cycler Composable style cycles <code>&gt;=0.10</code> <code>0.12.1</code> BSD License dill serialize all of Python <code>0.4.0</code> BSD-3-Clause docstring-inheritance Avoid writing and maintaining duplicated docstrings. <code>&gt;=2.0.0, &lt;=2.2.2</code> <code>2.2.2</code> MIT docutils Docutils -- Python Documentation Utilities <code>&gt;=0.20, &lt;0.22</code> <code>0.21.2</code> Public Domain + Python Software Foundation License + BSD License + GNU General Public License (GPL) et_xmlfile An implementation of lxml.xmlfile for the standard library <code>2.0.0</code> MIT fastjsonschema Fastest Python implementation of JSON schema <code>&gt;=2.14.5, &lt;=2.21.1</code> <code>2.21.1</code> BSD fonttools Tools to manipulate font files <code>&gt;=4.22.0</code> <code>4.60.0</code> MIT gemseo Generic Engine for Multi-disciplinary Scenarios, Exploration and Optimization <code>&gt;=6, &lt;7</code> <code>6.2.1.dev430+g39d9cb929</code> LGPL-3.0 genson GenSON is a powerful, user-friendly JSON Schema generator. <code>&gt;=1.2.2, &lt;=1.3.0</code> <code>1.3.0</code> MIT graphviz Simple Python interface for Graphviz <code>&gt;=0.19, &lt;=0.21</code> <code>0.21</code> MIT h5py Read and write HDF5 files from Python <code>&gt;=3.0.0, &lt;=3.14.0</code> <code>3.14.0</code> BSD-3-Clause idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.10</code> BSD License imagesize Getting image size from png/jpeg/jpeg2000/gif file <code>&gt;=1.3</code> <code>1.4.1</code> MIT importlib_metadata Read metadata from Python packages <code>&gt;=6.0, &gt;=4.6</code> <code>8.7.0</code> Apache Software License Jinja2 A very fast and expressive template engine. <code>&gt;=3.1, &gt;=2.11.1</code> <code>3.1.6</code> BSD License joblib Lightweight pipelining with Python functions <code>&gt;=1.2.0</code> <code>1.5.2</code> BSD 3-Clause kiwisolver A fast implementation of the Cassowary constraint solver <code>&gt;=1.3.1</code> <code>1.4.9</code> BSD License MarkupSafe Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0, &gt;=1.1</code> <code>3.0.2</code> BSD License matplotlib Python plotting package <code>3.10.3</code> Python Software Foundation License MiniSom Minimalistic implementation of the Self Organizing Maps (SOM) <code>&gt;=2.3.5, &lt;2.4</code> <code>2.3.5</code> MIT mpmath Python library for arbitrary-precision floating-point arithmetic <code>&gt;=1.1.0, &lt;1.4</code> <code>1.3.0</code> BSD networkx Python package for creating and manipulating graphs and networks <code>&gt;=2.5, &lt;=3.5</code> <code>3.5</code> BSD License nlopt Library for nonlinear optimization, wrapping many algorithms for global and local, constrained or unconstrained, optimization <code>&gt;=2.7.0, &lt;=2.9.1</code> <code>2.9.1</code> MIT numpy Fundamental package for array computing in Python <code>2.3.1</code> BSD License openpyxl A Python library to read/write Excel 2010 xlsx/xlsm files <code>&gt;=3.0.7, &lt;=3.1.5</code> <code>3.1.5</code> MIT openturns Uncertainty treatment library <code>&gt;=1.20, &lt;=1.25</code> <code>1.25</code> LGPL packaging Core utilities for Python packages <code>&gt;=23.0, &gt;=20.5</code> <code>25.0</code> Apache Software License + BSD License pandas Powerful data structures for data analysis, time series, and statistics <code>&gt;=1.5.0, &lt;=2.3.0</code> <code>2.3.0</code> BSD License pillow Python Imaging Library (Fork) <code>&gt;=8</code> <code>11.2.1</code> MIT-CMU plotly An open-source, interactive data visualization library for Python <code>&gt;=5.7.0, &lt;=5.24.1</code> <code>5.24.1</code> MIT prettytable A simple Python library for easily displaying tabular data in a visually appealing ASCII table format <code>&gt;=2.3.0, &lt;=3.16.0</code> <code>3.16.0</code> BSD-3-Clause psutil Cross-platform lib for process and system monitoring. <code>7.1.0</code> BSD-3-Clause pydantic Data validation using Python type hints <code>&gt;=2.6, &lt;=2.11.7</code> <code>2.11.7</code> MIT pydantic-settings Settings management using Pydantic <code>&gt;=2.0.0, &lt;=2.9.1</code> <code>2.9.1</code> MIT pydantic_core Core functionality for Pydantic validation and serialization <code>==2.33.2</code> <code>2.33.2</code> MIT pyDOE3 Design of experiments for Python <code>&gt;=1.0.1, &lt;=1.0.5</code> <code>1.0.5</code> BSD-3-Clause Pygments Pygments is a syntax highlighting package written in Python. <code>~=2.16, &gt;=2.17</code> <code>2.19.2</code> BSD-2-Clause pyparsing pyparsing - Classes and methods to define and execute parsing grammars <code>&gt;=3.0, &gt;=2.3.1</code> <code>3.2.5</code> MIT python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.1, &gt;=2.7</code> <code>2.9.0.post0</code> BSD License + Apache Software License python-dotenv Read key-value pairs from a .env file and set them as environment variables <code>&gt;=0.21.0</code> <code>1.1.1</code> BSD-3-Clause pytz World timezone definitions, modern and historical <code>&gt;=2015.7</code> <code>2025.2</code> MIT pyXDSM Python script to generate PDF XDSM diagrams using TikZ and LaTeX <code>&gt;=2.2.1, &lt;=2.3.1</code> <code>2.3.1</code> Apache License Version 2.0 requests Python HTTP for Humans. <code>~=2.26, &gt;=2.30.0</code> <code>2.32.5</code> Apache-2.0 scikit-learn A set of python modules for machine learning and data mining <code>&gt;=1.2, &lt;=1.7.0</code> <code>1.7.0</code> BSD License scipy Fundamental algorithms for scientific computing in Python <code>&gt;=1.11, &lt;=1.15.2</code> <code>1.15.2</code> BSD License six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.17.0</code> MIT snowballstemmer This package provides 32 stemmers for 30 languages generated from Snowball algorithms. <code>&gt;=2.2</code> <code>3.0.1</code> BSD-3-Clause spgl1 SPGL1: A solver for large-scale sparse reconstruction. <code>&gt;=0.0.3, &lt;=1.0.0</code> <code>0.0.3</code> GNU Lesser General Public License v3 (LGPLv3) Sphinx Python documentation generator <code>&lt;8</code> <code>7.4.7</code> BSD License sphinxcontrib-applehelp sphinxcontrib-applehelp is a Sphinx extension which outputs Apple help books <code>2.0.0</code> BSD License sphinxcontrib-devhelp sphinxcontrib-devhelp is a sphinx extension which outputs Devhelp documents <code>2.0.0</code> BSD License sphinxcontrib-htmlhelp sphinxcontrib-htmlhelp is a sphinx extension which renders HTML help files <code>&gt;=2.0.0</code> <code>2.1.0</code> BSD License sphinxcontrib-jsmath A sphinx extension which renders display math in HTML via JavaScript <code>1.0.1</code> BSD sphinxcontrib-qthelp sphinxcontrib-qthelp is a sphinx extension which outputs QtHelp documents <code>2.0.0</code> BSD License sphinxcontrib-serializinghtml sphinxcontrib-serializinghtml is a sphinx extension which outputs \"serialized\" HTML files (json and pickle) <code>&gt;=1.1.9</code> <code>2.0.0</code> BSD License StrEnum An Enum that inherits from str. <code>&gt;=0.4.9, &lt;=0.4.15</code> <code>0.4.15</code> MIT License sympy Computer algebra system (CAS) in Python <code>&gt;=1.5, &lt;=1.14.0</code> <code>1.14.0</code> BSD tenacity Retry code until it succeeds <code>&gt;=6.2.0</code> <code>9.1.2</code> Apache 2.0 threadpoolctl threadpoolctl <code>&gt;=3.1.0</code> <code>3.6.0</code> BSD-3-Clause tqdm Fast, Extensible Progress Meter <code>&gt;=4.50, &lt;=4.67.1</code> <code>4.67.1</code> MPL-2.0 AND MIT typing-inspection Runtime typing introspection tools <code>&gt;=0.4.0</code> <code>0.4.1</code> MIT typing_extensions Backported and Experimental Type Hints for Python 3.9+ <code>&gt;=4.0, &gt;=4, &lt;5</code> <code>4.15.0</code> PSF-2.0 tzdata Provider of IANA time zone data <code>&gt;=2022.7</code> <code>2025.2</code> Apache-2.0 urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.21.1, &lt;3</code> <code>2.5.0</code> MIT wcwidth Measures the displayed width of unicode strings in a terminal <code>0.2.14</code> MIT xdsmjs XDSMjs Python module <code>&gt;=1.0.0, &lt;=2.0.0</code> <code>2.0.0</code> Apache License, Version 2.0 xxhash Python binding for xxHash <code>&gt;=3.0.0, &lt;=3.5.0</code> <code>3.5.0</code> BSD zipp Backport of pathlib-compatible object wrapper for zip files <code>&gt;=3.20</code> <code>3.23.0</code> MIT"},{"location":"credits/#exec-1--development-dependencies","title":"Development dependencies","text":"Project Summary Version (accepted) Version (last resolved) License babel Internationalization utilities <code>~=2.10, &gt;=2.13</code> <code>2.17.0</code> BSD-3-Clause backrefs A wrapper around re and regex that adds additional back references. <code>~=5.7.post1</code> <code>5.9</code> MIT black The uncompromising code formatter. <code>25.9.0</code> MIT bracex Bash style brace expander. <code>&gt;=2.1.1</code> <code>2.6</code> MIT certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2025.8.3</code> MPL-2.0 charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.3</code> MIT click Composable command line interface toolkit <code>&lt;8.2.2</code> <code>8.2.1</code> BSD-3-Clause colorama Cross-platform colored terminal text. <code>~=0.4, &gt;=0.4.6</code> <code>0.4.6</code> BSD License fieldz Utilities for providing compatibility with many dataclass-like libraries <code>&gt;=0.1.0</code> <code>0.1.3</code> BSD-3-Clause ghp-import Copy your docs directly to the gh-pages branch. <code>&gt;=1.0</code> <code>2.1.0</code> Apache Software License griffe Signatures for entire Python programs. Extract the structure, the frame, the skeleton of your project, to generate API documentation or find breaking changes in your API. <code>&gt;=1.13</code> <code>1.14.0</code> ISC griffe-fieldz Griffe extension adding support for data-class like things (pydantic, attrs, etc...) <code>0.3.0</code> BSD-3-Clause griffe-inherited-docstrings Griffe extension for inheriting docstrings. <code>1.1.2</code> ISC idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.10</code> BSD License importlib_metadata Read metadata from Python packages <code>&gt;=6.0, &gt;=4.6</code> <code>8.7.0</code> Apache Software License importlib_resources Read resources from Python packages <code>6.5.2</code> Apache Software License Jinja2 A very fast and expressive template engine. <code>&gt;=3.1, &gt;=2.11.1</code> <code>3.1.6</code> BSD License latexcodec A lexer and codec to work with LaTeX code in Python. <code>&gt;=1.0.4</code> <code>3.0.1</code> MIT Markdown Python implementation of John Gruber's Markdown. <code>&gt;=3.6</code> <code>3.9</code> BSD-3-Clause markdown-exec Utilities to execute code blocks in Markdown files. <code>1.11.0</code> ISC MarkupSafe Safely add untrusted strings to HTML/XML markup. <code>&gt;=2.0, &gt;=1.1</code> <code>3.0.2</code> BSD License mergedeep A deep merge function for \ud83d\udc0d. <code>&gt;=1.3.4</code> <code>1.3.4</code> MIT License mike Manage multiple versions of your MkDocs-powered documentation <code>2.1.3</code> BSD-3-Clause mkdocs Project documentation with Markdown. <code>&gt;=1.2</code> <code>1.6.1</code> BSD-2-Clause mkdocs-autorefs Automatically link across pages in MkDocs. <code>&gt;=1.4</code> <code>1.4.3</code> ISC mkdocs-bibtex An MkDocs plugin that enables managing citations with BibTex <code>4.4.0</code> BSD-3-Clause-LBNL mkdocs-gallery a <code>mkdocs</code> plugin to generate example galleries from python scripts, similar to <code>sphinx-gallery</code>. <code>0.10.4</code> BSD 3-Clause mkdocs-gen-files MkDocs plugin to programmatically generate documentation pages during the build <code>0.5.0</code> MIT mkdocs-get-deps MkDocs extension that lists all dependencies according to a mkdocs.yml file <code>&gt;=0.2.0</code> <code>0.2.0</code> MIT mkdocs-include-markdown-plugin Mkdocs Markdown includer plugin. <code>7.1.8</code> Apache-2.0 mkdocs-literate-nav MkDocs plugin to specify the navigation in Markdown instead of YAML <code>0.6.2</code> MIT mkdocs-material Documentation that simply works <code>9.6.20</code> MIT mkdocs-material-extensions Extension pack for Python Markdown and MkDocs Material. <code>~=1.3</code> <code>1.3.1</code> MIT mkdocs-section-index MkDocs plugin to allow clickable sections that lead to an index page <code>0.3.10</code> MIT mkdocstrings Automatic documentation from sources, for MkDocs. <code>0.30.1</code> ISC mkdocstrings-python A Python handler for mkdocstrings. <code>&gt;=1.16.2</code> <code>1.18.2</code> ISC mypy_extensions Type system extensions for programs checked with the mypy type checker. <code>&gt;=0.4.3</code> <code>1.1.0</code> MIT packaging Core utilities for Python packages <code>&gt;=23.0, &gt;=20.5</code> <code>25.0</code> Apache Software License + BSD License paginate Divides large result sets into pages for easier browsing <code>~=0.5</code> <code>0.5.7</code> MIT pathspec Utility library for gitignore style pattern matching of file paths. <code>&gt;=0.11.1</code> <code>0.12.1</code> Mozilla Public License 2.0 (MPL 2.0) platformdirs A small Python package for determining appropriate platform-specific dirs, e.g. a <code>user data dir</code>. <code>&gt;=2.2.0</code> <code>4.4.0</code> MIT pybtex A BibTeX-compatible bibliography processor in Python <code>&gt;=0.22</code> <code>0.25.1</code> MIT Pygments Pygments is a syntax highlighting package written in Python. <code>~=2.16, &gt;=2.17</code> <code>2.19.2</code> BSD-2-Clause pygments-ansi-color <code>&gt;=0.3</code> <code>0.3.0</code> Apache Software License pymdown-extensions Extension pack for Python Markdown. <code>&gt;=6.3</code> <code>10.16.1</code> MIT pypandoc Thin wrapper for pandoc. <code>&gt;=1.5</code> <code>1.15</code> MIT pyparsing pyparsing - Classes and methods to define and execute parsing grammars <code>&gt;=3.0, &gt;=2.3.1</code> <code>3.2.5</code> MIT python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.1, &gt;=2.7</code> <code>2.9.0.post0</code> BSD License + Apache Software License pytokens A Fast, spec compliant Python 3.12+ tokenizer that runs on older Pythons. <code>&gt;=0.1.10</code> <code>0.1.10</code> MIT pytz World timezone definitions, modern and historical <code>&gt;=2015.7</code> <code>2025.2</code> MIT PyYAML YAML parser and emitter for Python <code>&gt;=5.1</code> <code>6.0.3</code> MIT pyyaml_env_tag A custom YAML tag for referencing environment variables in YAML files. <code>&gt;=0.1</code> <code>1.1</code> MIT requests Python HTTP for Humans. <code>~=2.26, &gt;=2.30.0</code> <code>2.32.5</code> Apache-2.0 responses A utility library for mocking out the <code>requests</code> Python library. <code>&gt;=0.25.6</code> <code>0.25.8</code> Apache 2.0 setuptools Easily download, build, install, upgrade, and uninstall Python packages <code>&gt;=68.0.0</code> <code>80.9.0</code> MIT six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.17.0</code> MIT tqdm Fast, Extensible Progress Meter <code>&gt;=4.50, &lt;=4.67.1</code> <code>4.67.1</code> MPL-2.0 AND MIT typing_extensions Backported and Experimental Type Hints for Python 3.9+ <code>&gt;=4.0, &gt;=4, &lt;5</code> <code>4.15.0</code> PSF-2.0 urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.21.1, &lt;3</code> <code>2.5.0</code> MIT validators Python Data Validation for Humans\u2122 <code>&gt;=0.19.0</code> <code>0.35.0</code> MIT verspec Flexible version handling <code>0.1.0</code> BSD 2-Clause or Apache-2.0 watchdog Filesystem events monitoring <code>&gt;=2.0</code> <code>6.0.0</code> Apache-2.0 wcmatch Wildcard/glob file name matcher. <code>10.1</code> MIT zipp Backport of pathlib-compatible object wrapper for zip files <code>&gt;=3.20</code> <code>3.23.0</code> MIT"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#licenses","title":"Licenses","text":""},{"location":"licenses/#gnu-lgpl-v30","title":"GNU LGPL v3.0","text":"<p>The <code>gemseo-benchmark</code> source code is distributed under the GNU LGPL v3.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense version 3 as published by the Free Software Foundation.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with this program; if not, write to the Free Software Foundation,\nInc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n</code></pre></p>"},{"location":"licenses/#bsd-0-clause","title":"BSD 0-Clause","text":"<p>The <code>gemseo-benchmark</code> examples are distributed under the BSD 0-Clause <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under a BSD 0-Clause License.\n\nPermission to use, copy, modify, and/or distribute this software\nfor any purpose with or without fee is hereby granted.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL\nWARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\nTHE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,\nOR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\nFROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\nNEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\nWITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre></p>"},{"location":"licenses/#cc-by-sa-40","title":"CC BY-SA 4.0","text":"<p>The <code>gemseo-benchmark</code> documentation is distributed under the CC BY-SA 4.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0\nInternational License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/4.0/ or send a letter to Creative\nCommons, PO Box 1866, Mountain View, CA 94042, USA.\n</code></pre></p>"},{"location":"generated/examples/all/","title":"all","text":""},{"location":"generated/examples/all/#examples","title":"Examples","text":"<p> Generate target values </p> <p> Compute data profiles </p> <p> Generate a benchmarking report </p> <p> Download all examples in Python source code: all_python.zip</p> <p> Download all examples in Jupyter notebooks: all_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/all/mg_execution_times/","title":"Computation times","text":"<p>00:16.566 total execution time for generated_examples_all files:</p> <p>+-----------------------------------------------------------------------------------------+-----------+--------+ | plot_report (docs/examples/all/plot_report.py)                      | 00:09.803 | 0.0 MB | +-----------------------------------------------------------------------------------------+-----------+--------+ | plot_target_values (docs/examples/all/plot_target_values.py) | 00:06.091 | 0.0 MB | +-----------------------------------------------------------------------------------------+-----------+--------+ | plot_data_profiles (docs/examples/all/plot_data_profiles.py) | 00:00.672 | 0.0 MB | +-----------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/all/plot_data_profiles/","title":"Compute data profiles","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/all/plot_data_profiles/#compute-data-profiles","title":"Compute data profiles","text":"<p>In this example, we compute the data profiles of three algorithms configurations based on two reference problems.</p>"},{"location":"generated/examples/all/plot_data_profiles/#imports","title":"Imports","text":"<p>We start by making the necessary imports.</p> <pre><code>from __future__ import annotations\n\nimport shutil\nimport tempfile\nfrom pathlib import Path\n\nfrom gemseo import configure\nfrom gemseo.problems.optimization.rastrigin import Rastrigin\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n\nfrom gemseo_benchmark.algorithms.algorithm_configuration import AlgorithmConfiguration\nfrom gemseo_benchmark.algorithms.algorithms_configurations import (\n    AlgorithmsConfigurations,\n)\nfrom gemseo_benchmark.data_profiles.target_values import TargetValues\nfrom gemseo_benchmark.problems.optimization_problem_configuration import (\n    OptimizationProblemConfiguration,\n)\nfrom gemseo_benchmark.problems.problems_group import ProblemsGroup\nfrom gemseo_benchmark.scenario import Scenario\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#set-the-algorithms-configurations","title":"Set the algorithms configurations","text":"<p>Let us define the algorithms configurations for which we want to compute data profiles.</p> <p>For example, let us choose a configuration of the L-BFGS-B algorithm with a number of Hessian corrections limited to 2. (This option is called <code>maxcor</code>.)</p> <pre><code>lbfgsb_2_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 2 Hessian corrections\",\n    maxcor=2,\n)\n</code></pre> <p>Note:     The customized name <code>\"L-BFGS-B with 2 Hessian corrections\"</code>     will serve as label in the plot of the data profiles.</p> <p>To investigate the influence of the <code>maxcor</code> option, let us consider a different configuration of L-BFGS-B with up to 20 Hessian corrections.</p> <pre><code>lbfgsb_20_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 20 Hessian corrections\",\n    maxcor=20,\n)\n</code></pre> <p>Additionally, let us choose the SLSQP algorithm, with all its options set to their default values, to compare it against L-BFGS-B.</p> <pre><code>slsqp_default = AlgorithmConfiguration(\"SLSQP\")\n</code></pre> <p>Finally, we gather our selection of algorithms configurations in a group.</p> <pre><code>algorithms_configurations = AlgorithmsConfigurations(\n    lbfgsb_2_corrections,\n    lbfgsb_20_corrections,\n    slsqp_default,\n    name=\"Derivative-based algorithms\",\n)\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#set-the-reference-problems","title":"Set the reference problems","text":"<p>Let us choose two problems already implemented in GEMSEO as references to measure the performances of our selection of algorithms configurations: Rastrigin and Rosenbrock.</p> <p>We define target values as an exponential scale of values decreasing towards zero, the minimum value of both Rastrigin's and Rosenbrock's functions.</p> <pre><code>optimum = 0.0\ntarget_values = TargetValues([10**-i for i in range(4, 7)] + [optimum])\n</code></pre> <p>Note:     It could be preferable to customize a different scale of target values     for each problem, although we keep it simple here.</p> <p>We now have all the elements to define the problem configurations.</p> <pre><code>rastrigin = OptimizationProblemConfiguration(\n    \"Rastrigin\",\n    Rastrigin,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\nrosenbrock = OptimizationProblemConfiguration(\n    \"Rosenbrock\",\n    Rosenbrock,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\n</code></pre> <p>Here we configure a design of experiments (DOE) to generate five starting points by optimized Latin hypercube sampling (LHS).</p> <p>Finally, we gather our reference problems in a group.</p> <pre><code>problems = ProblemsGroup(\"Reference problems\", [rastrigin, rosenbrock])\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#generate-the-benchmarking-results","title":"Generate the benchmarking results","text":"<p>Now that the algorithms configurations and the reference problems are properly set, we can measure the performances of the former on the latter.</p> <p>We set up a Scenario with our group of algorithms configurations and a path to a directory where to save the performance histories.</p> <pre><code>scenario_dir = Path(tempfile.mkdtemp())\nscenario = Scenario([algorithms_configurations], scenario_dir)\n</code></pre> <p>Here we choose to deactivate the functions counters, progress bars and bounds check of GEMSEO to accelerate the script.</p> <pre><code>configure(\n    enable_function_statistics=False,\n    enable_progress_bar=False,\n    check_desvars_bounds=False,\n)\n</code></pre> <p>Let us execute the benchmarking scenario on our group of reference problems.</p> <p>Note:     Here we skip the generation of the report     as we only intend to compute the data profiles.</p> <pre><code>results = scenario.execute([problems], skip_report=True)\n</code></pre> <p>Out:</p> <pre><code>/usr/lib64/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=295) is multi-threaded, use of fork() may lead to deadlocks in the child.\n  self.pid = os.fork()\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#compute-the-datas-profiles","title":"Compute the datas profiles","text":"<p>Now that the performances histories are generated for the reference problems, the data profiles of the algorithms configurations can be computed.</p> <pre><code>problems.compute_data_profile(algorithms_configurations, results, show=True)\n</code></pre> <p></p> <p>Here we remove the performances histories as we do not wish to keep them.</p> <pre><code>shutil.rmtree(scenario_dir)\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.672 seconds)</p> <p> Download Python source code: plot_data_profiles.py</p> <p> Download Jupyter notebook: plot_data_profiles.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/all/plot_report/","title":"Generate a benchmarking report","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/all/plot_report/#generate-a-benchmarking-report","title":"Generate a benchmarking report","text":"<p>In this example, we generate a benchmarking report based on the performances of three algorithms configurations on three reference problems.</p>"},{"location":"generated/examples/all/plot_report/#imports","title":"Imports","text":"<p>We start by making the necessary imports.</p> <pre><code>from __future__ import annotations\n\nimport shutil\nimport tempfile\nfrom pathlib import Path\n\nfrom gemseo import configure\nfrom gemseo.problems.optimization.rastrigin import Rastrigin\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n\nfrom gemseo_benchmark.algorithms.algorithm_configuration import AlgorithmConfiguration\nfrom gemseo_benchmark.algorithms.algorithms_configurations import (\n    AlgorithmsConfigurations,\n)\nfrom gemseo_benchmark.data_profiles.target_values import TargetValues\nfrom gemseo_benchmark.problems.optimization_problem_configuration import (\n    OptimizationProblemConfiguration,\n)\nfrom gemseo_benchmark.problems.problems_group import ProblemsGroup\nfrom gemseo_benchmark.scenario import Scenario\n</code></pre>"},{"location":"generated/examples/all/plot_report/#set-the-algorithms-configurations","title":"Set the algorithms configurations","text":"<p>Let us define the algorithms configurations that we want to benchmark.</p> <p>For example, let us choose a configuration of the L-BFGS-B algorithm with a number of Hessian corrections limited to 2. (this option is called <code>maxcor</code>.)</p> <pre><code>lbfgsb_2_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 2 Hessian corrections\",\n    maxcor=2,\n)\n</code></pre> <p>Note:     The customized name \"L-BFGS-B with 2 Hessian corrections\"     will serve to refer to this algorithm configuration in the report.</p> <p>To investigate the influence of the <code>maxcor</code> option, let us consider a different configuration of L-BFGS-B with up to 20 Hessian corrections.</p> <pre><code>lbfgsb_20_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 20 Hessian corrections\",\n    maxcor=20,\n)\n</code></pre> <p>Let us put these two configurations of L-BFGS-B in a same group of algorithms configurations so that a section of the report will be dedicated to them.</p> <pre><code>lbfgsb_configurations = AlgorithmsConfigurations(\n    lbfgsb_2_corrections,\n    lbfgsb_20_corrections,\n    name=\"L-BFGS-B configurations\",\n)\n</code></pre> <p>Additionally, let us choose the SLSQP algorithm, with all its options set to their default values, to compare it against L-BFGS-B. Let us put it in a group of its own.</p> <pre><code>slsqp_default = AlgorithmConfiguration(\"SLSQP\")\nslsqp_configurations = AlgorithmsConfigurations(slsqp_default, name=\"SLSQP\")\n</code></pre>"},{"location":"generated/examples/all/plot_report/#set-the-reference-problems","title":"Set the reference problems","text":"<p>Let us choose two problems already implemented in GEMSEO as references to measure the performances of our selection of algorithms configurations: Rastrigin and Rosenbrock.</p> <p>We define target values as an exponential scale of values decreasing towards zero, the minimum value of both Rastrigin's and Rosenbrock's functions.</p> <pre><code>optimum = 0.0\ntarget_values = TargetValues([10**-i for i in range(4, 7)] + [optimum])\n</code></pre> <p>N.B. it could be preferable to customize a different scale of target values for each problem, although we keep it simple here.</p> <p>We now have all the elements to define the problem configurations.</p> <pre><code>rastrigin_2d = OptimizationProblemConfiguration(\n    \"Rastrigin\",\n    Rastrigin,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\nrosenbrock_2d = OptimizationProblemConfiguration(\n    \"Rosenbrock\",\n    Rosenbrock,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\n</code></pre> <p>Here we configure a design of experiments (DOE) to generate five starting points by optimized Latin hypercube sampling (LHS).</p> <p>Let us gather these two two-variables problems in a group so that they will be treated together.</p> <pre><code>problems_2d = ProblemsGroup(\"2D problems\", [rastrigin_2d, rosenbrock_2d])\n</code></pre> <p>We add a five-variables problem, also based on Rosenbrock's function, to compare the algorithms configurations in higher dimension. Let us put it in a group of its own.</p> <pre><code>def create_problem():\n    return Rosenbrock(5)\n\n\nrosenbrock_5d = OptimizationProblemConfiguration(\n    \"Rosenbrock 5D\",\n    create_problem,\n    target_values=target_values,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n)\nproblems_5d = ProblemsGroup(\"5D problems\", [rosenbrock_5d])\n</code></pre>"},{"location":"generated/examples/all/plot_report/#generate-the-benchmarking-results","title":"Generate the benchmarking results","text":"<p>Now that the algorithms configurations and the reference problems are properly set, we can measure the performances of the former on the latter.</p> <p>We set up a Scenario with our two groups of algorithms configurations and a path to a directory where to save the performance histories and the report.</p> <pre><code>scenario_dir = Path(tempfile.mkdtemp())\nscenario = Scenario([lbfgsb_configurations, slsqp_configurations], scenario_dir)\n</code></pre> <p>Here we choose to deactivate the functions counters, progress bars and bounds check of GEMSEO to accelerate the script.</p> <pre><code>configure(\n    enable_function_statistics=False,\n    enable_progress_bar=False,\n    check_desvars_bounds=False,\n)\n</code></pre> <p>Let us execute the benchmarking scenario on our two groups of reference problems.</p> <pre><code>scenario.execute([problems_2d, problems_5d])\n</code></pre> <p>Out:</p> <pre><code>/usr/lib64/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=295) is multi-threaded, use of fork() may lead to deadlocks in the child.\n  self.pid = os.fork()\n\n&lt;gemseo_benchmark.results.results.Results object at 0x7d572c3048c0&gt;\n</code></pre> <p>The root the HTML report is the following.</p> <pre><code>str((scenario_dir / \"report\" / \"_build\" / \"html\" / \"index.html\").absolute())\n</code></pre> <p>Out:</p> <pre><code>'/tmp/tmp3jgtusig/report/_build/html/index.html'\n</code></pre> <p>Here we remove the data as we do not intend to keep it.</p> <pre><code>shutil.rmtree(scenario_dir)\n</code></pre> <p>Total running time of the script: ( 0 minutes  9.803 seconds)</p> <p> Download Python source code: plot_report.py</p> <p> Download Jupyter notebook: plot_report.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/all/plot_target_values/","title":"Generate target values","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/all/plot_target_values/#generate-target-values","title":"Generate target values","text":"<p>In this example, we generate target values for an optimization problem configuration based on the performances of an algorithm configuration.</p>"},{"location":"generated/examples/all/plot_target_values/#imports","title":"Imports","text":"<p>We start by making the necessary imports.</p> <pre><code>from __future__ import annotations\n\nfrom gemseo import compute_doe\nfrom gemseo import configure\nfrom gemseo.problems.optimization.power_2 import Power2\n\nfrom gemseo_benchmark.algorithms.algorithm_configuration import AlgorithmConfiguration\nfrom gemseo_benchmark.algorithms.algorithms_configurations import (\n    AlgorithmsConfigurations,\n)\nfrom gemseo_benchmark.problems.optimization_problem_configuration import (\n    OptimizationProblemConfiguration,\n)\n</code></pre> <p>Out:</p> <pre><code>&lt;frozen importlib._bootstrap&gt;:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n&lt;frozen importlib._bootstrap&gt;:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n</code></pre> <p>Let us consider the optimization problem Power2 already implemented in GEMSEO.</p> <pre><code>problem = OptimizationProblemConfiguration(\n    \"Power2\", Power2, optimum=Power2.get_solution()[1]\n)\n</code></pre> <p>We define ten starting points by optimized Latin hypercube sampling (LHS).</p> <pre><code>design_space = problem.create_problem().design_space\nproblem.starting_points = compute_doe(\n    design_space, algo_name=\"OT_OPT_LHS\", n_samples=10\n)\n</code></pre> <p>Out:</p> <pre><code>&lt;frozen importlib._bootstrap&gt;:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n&lt;frozen importlib._bootstrap&gt;:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n&lt;frozen importlib._bootstrap&gt;:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n</code></pre> <p>Let use the optimizer COBYLA to generate performance histories on the problem.</p> <pre><code>algorithms_configurations = AlgorithmsConfigurations(\n    AlgorithmConfiguration(\n        \"NLOPT_COBYLA\",\n        max_iter=65,\n        eq_tolerance=1e-4,\n        ineq_tolerance=0.0,\n        xtol_abs=0,\n        xtol_rel=0,\n        ftol_abs=0,\n        ftol_rel=0,\n    )\n)\n</code></pre> <p>Here we choose to deactivate the functions counters, progress bars and bounds check of GEMSEO to accelerate the script.</p> <pre><code>configure(\n    enable_function_statistics=False,\n    enable_progress_bar=False,\n    check_desvars_bounds=False,\n)\n</code></pre> <p>Let us compute five target values for the problem. This automatic procedure has two stages:</p> <ol> <li>execution of the specified algorithm configurations    once for each of the starting points,</li> <li>automatic selection of target values based on the performance histories.</li> </ol> <p>These target values represent the milestones of the problem resolution.</p> <pre><code>problem.compute_target_values(5, algorithms_configurations, best_target_tolerance=1e-5)\n</code></pre> <p>Out:</p> <pre><code>    INFO - 22:13:03: Optimization problem:\n    INFO - 22:13:03:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:03:    with respect to x\n    INFO - 22:13:03:    under the equality constraints\n    INFO - 22:13:03:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:03:    under the inequality constraints\n    INFO - 22:13:03:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:03:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:03:    over the design space:\n    INFO - 22:13:03:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:03:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:03:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:03:       | x[0] |      -1     | -0.1704435994449301 |      1      | float |\n    INFO - 22:13:03:       | x[1] |      -1     |  0.1464534296219673 |      1      | float |\n    INFO - 22:13:03:       | x[2] |      -1     |  0.9072885843570564 |      1      | float |\n    INFO - 22:13:03:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:03: Optimization result:\n    INFO - 22:13:03:    Optimizer info:\n    INFO - 22:13:03:       Status: None\n    INFO - 22:13:03:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:03:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:03:    Solution:\n    INFO - 22:13:03:       The solution is feasible.\n    INFO - 22:13:03:       Objective: 2.192090686228317\n    INFO - 22:13:03:       Standardized constraints:\n    INFO - 22:13:03:          eq = [1.86354934e-07]\n    INFO - 22:13:03:          ineq1 = [-4.18011858e-09]\n    INFO - 22:13:03:          ineq2 = [-1.1564497e-08]\n    INFO - 22:13:03:       Design space:\n    INFO - 22:13:03:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:03:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:03:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:03:          | x[0] |      -1     | 0.7937005281959413 |      1      | float |\n    INFO - 22:13:03:          | x[1] |      -1     | 0.7937005321032646 |      1      | float |\n    INFO - 22:13:03:          | x[2] |      -1     | 0.9654893179672139 |      1      | float |\n    INFO - 22:13:03:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:03: Optimization problem:\n    INFO - 22:13:03:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:03:    with respect to x\n    INFO - 22:13:03:    under the equality constraints\n    INFO - 22:13:03:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:03:    under the inequality constraints\n    INFO - 22:13:03:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:03:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:03:    over the design space:\n    INFO - 22:13:03:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:03:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:03:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:03:       | x[0] |      -1     | 0.04792699455581295 |      1      | float |\n    INFO - 22:13:03:       | x[1] |      -1     | -0.9687257927744917 |      1      | float |\n    INFO - 22:13:03:       | x[2] |      -1     |  -0.574093850045964 |      1      | float |\n    INFO - 22:13:03:       +------+-------------+---------------------+-------------+-------+\n WARNING - 22:13:03: Optimization found no feasible point; the least infeasible point is selected.\n    INFO - 22:13:03: Optimization result:\n    INFO - 22:13:03:    Optimizer info:\n    INFO - 22:13:03:       Status: None\n    INFO - 22:13:03:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:03:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:03:    Solution:\n WARNING - 22:13:03:       The solution is not feasible.\n    INFO - 22:13:03:       Objective: 0.9021945710301897\n    INFO - 22:13:03:       Standardized constraints:\n    INFO - 22:13:03:          eq = [0.4018313]\n    INFO - 22:13:03:          ineq1 = [0.44935408]\n    INFO - 22:13:03:          ineq2 = [0.44935481]\n    INFO - 22:13:03:       Design space:\n    INFO - 22:13:03:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:03:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:03:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:03:          | x[0] |      -1     | 0.3699827708813155 |      1      | float |\n    INFO - 22:13:03:          | x[1] |      -1     | 0.3699809827627125 |      1      | float |\n    INFO - 22:13:03:          | x[2] |      -1     | 0.7927303404532409 |      1      | float |\n    INFO - 22:13:03:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+----------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+----------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     |  0.8109111660370278  |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     | -0.5151088299597564  |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     | -0.03003068569333855 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+----------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.192078157906169\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [2.15612154e-05]\n    INFO - 22:13:04:          ineq1 = [-6.14900015e-07]\n    INFO - 22:13:04:          ineq2 = [-2.05695942e-06]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937008513482764 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.7937016143891218 |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.965481674497912  |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     | -0.9343763559895029 |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     |  0.417608203368014  |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     | 0.06938845014781836 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.1920907849655835\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [4.47386216e-07]\n    INFO - 22:13:04:          ineq1 = [-3.47712432e-07]\n    INFO - 22:13:04:          ineq2 = [-1.70145897e-10]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937007099704172 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.7937005260741297 |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.9654892246253652 |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     | -0.2661421977367893 |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     |  0.847838561713693  |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     | -0.2376288922677025 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.1920907816905304\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [3.77610671e-08]\n    INFO - 22:13:04:          ineq1 = [-1.92824157e-09]\n    INFO - 22:13:04:          ineq2 = [-5.3143191e-09]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937005270043973 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.793700528796085  |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.9654893711027002 |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     | -0.6890693929149758 |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     | -0.1805266676001036 |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     | -0.6027624897747317 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.1920806657873584\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [1.99259394e-05]\n    INFO - 22:13:04:          ineq1 = [-6.97805588e-07]\n    INFO - 22:13:04:          ineq2 = [-3.61549391e-06]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937008952163696 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.7937024390590997 |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.9654822592630732 |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     | 0.678587790861062  |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     | 0.6860971177899764 |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     | 0.6647060866590593 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.1920880296373157\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [7.74129463e-06]\n    INFO - 22:13:04:          ineq1 = [-2.91987556e-06]\n    INFO - 22:13:04:          ineq2 = [-1.4376901e-07]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937020709856033 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.7937006020571185 |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.9654866163985121 |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     |  0.4998138351635664 |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     |  0.3866788199528071 |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     | -0.8066727983928847 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.192090669652096\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [2.67522434e-07]\n    INFO - 22:13:04:          ineq1 = [-6.27351793e-08]\n    INFO - 22:13:04:          ineq2 = [-3.26405569e-14]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937005591793949 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.793700525984117  |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.9654892889426312 |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     |  0.2608056031567285 |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     | -0.2402315359654206 |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     |  0.3932631012345964 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.1920876633266353\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [5.48684163e-06]\n    INFO - 22:13:04:          ineq1 = [-6.1708306e-07]\n    INFO - 22:13:04:          ineq2 = [-1.57123048e-07]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937008525033984 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.7937006091231882 |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.9654874225692918 |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04: Optimization problem:\n    INFO - 22:13:04:    minimize pow2(x) = x[0]**2 + x[1]**2 + x[2]**2\n    INFO - 22:13:04:    with respect to x\n    INFO - 22:13:04:    under the equality constraints\n    INFO - 22:13:04:       eq(x): 0.9 - x[2]**3 = 0.0\n    INFO - 22:13:04:    under the inequality constraints\n    INFO - 22:13:04:       ineq1(x): 0.5 - x[0]**3 &lt;= 0.0\n    INFO - 22:13:04:       ineq2(x): 0.5 - x[1]**3 &lt;= 0.0\n    INFO - 22:13:04:    over the design space:\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04:       | x[0] |      -1     | -0.5119431216715998 |      1      | float |\n    INFO - 22:13:04:       | x[1] |      -1     | -0.7604787405137763 |      1      | float |\n    INFO - 22:13:04:       | x[2] |      -1     |  0.5450674352746387 |      1      | float |\n    INFO - 22:13:04:       +------+-------------+---------------------+-------------+-------+\n    INFO - 22:13:04: Optimization result:\n    INFO - 22:13:04:    Optimizer info:\n    INFO - 22:13:04:       Status: None\n    INFO - 22:13:04:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 22:13:04:       Number of calls to the objective function by the optimizer: 0\n    INFO - 22:13:04:    Solution:\n    INFO - 22:13:04:       The solution is feasible.\n    INFO - 22:13:04:       Objective: 2.1920745744630694\n    INFO - 22:13:04:       Standardized constraints:\n    INFO - 22:13:04:          eq = [7.4912028e-05]\n    INFO - 22:13:04:          ineq1 = [-3.59754148e-05]\n    INFO - 22:13:04:          ineq2 = [-6.28953966e-06]\n    INFO - 22:13:04:       Design space:\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n    INFO - 22:13:04:          | x[0] |      -1     | 0.7937195613313415 |      1      | float |\n    INFO - 22:13:04:          | x[1] |      -1     | 0.7937038539774341 |      1      | float |\n    INFO - 22:13:04:          | x[2] |      -1     | 0.9654625961705716 |      1      | float |\n    INFO - 22:13:04:          +------+-------------+--------------------+-------------+-------+\n\n[(np.float64(2.19302808026987), 0.0), (np.float64(2.19302808026987), 0.0), (np.float64(2.1922071109988592), 0.0), (np.float64(2.1922071109988592), 0.0), (np.float64(2.1920987389246234), 0.0)]\n</code></pre> <p>We can plot the performace histories used as reference for the computation of the target values, with the objective value on the vertical axis and the number of functions evaluations on the horizontal axis.</p> <pre><code>problem.targets_generator.plot_histories(problem.optimum, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 1 Axes&gt;\n</code></pre> <p>Finally, we can plot the target values: the objective value of each of the five targets is represented on the vertical axis with a marker indicating whether the target is feasible or not.</p> <pre><code>problem.target_values.plot()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 1 Axes&gt;\n</code></pre> <p>Total running time of the script: ( 0 minutes  6.091 seconds)</p> <p> Download Python source code: plot_target_values.py</p> <p> Download Jupyter notebook: plot_target_values.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gemseo_benchmark<ul> <li>algorithms<ul> <li>algorithm_configuration</li> <li>algorithms_configurations</li> </ul> </li> <li>benchmarker<ul> <li>base_worker</li> <li>benchmarker</li> <li>mda_worker</li> <li>mdo_worker</li> <li>optimization_worker</li> </ul> </li> <li>data_profiles<ul> <li>data_profile</li> <li>target_values</li> <li>targets_generator</li> </ul> </li> <li>problems<ul> <li>base_problem_configuration</li> <li>mda_problem_configuration</li> <li>mdo_problem_configuration</li> <li>optimization_problem_configuration</li> <li>problems_group</li> </ul> </li> <li>report<ul> <li>axis_data</li> <li>conf</li> <li>range_plot</li> <li>report</li> <li>report_plot</li> </ul> </li> <li>results<ul> <li>history_item</li> <li>performance_histories</li> <li>performance_history</li> <li>results</li> </ul> </li> <li>scenario</li> </ul> </li> </ul>"},{"location":"reference/gemseo_benchmark/","title":"API documentation","text":""},{"location":"reference/gemseo_benchmark/#gemseo_benchmark","title":"gemseo_benchmark","text":"<p>Benchmarking of algorithms.</p>"},{"location":"reference/gemseo_benchmark/#gemseo_benchmark-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/#gemseo_benchmark.get_markers_cycle","title":"get_markers_cycle","text":"<pre><code>get_markers_cycle() -&gt; Iterator\n</code></pre> <p>Return the markers cycle for the plots.</p> <p>Returns:</p> <ul> <li> <code>Iterator</code>           \u2013            <p>The markers cycle.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/__init__.py</code> <pre><code>def get_markers_cycle() -&gt; Iterator:\n    \"\"\"Return the markers cycle for the plots.\n\n    Returns:\n        The markers cycle.\n    \"\"\"\n    return itertools.cycle(MARKERS)\n</code></pre>"},{"location":"reference/gemseo_benchmark/#gemseo_benchmark.join_substrings","title":"join_substrings","text":"<pre><code>join_substrings(string: str) -&gt; str\n</code></pre> <p>Join sub-strings with underscores.</p> <p>Parameters:</p> <ul> <li> <code>string</code>               (<code>str</code>)           \u2013            <p>The string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The joined sub-strings.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/__init__.py</code> <pre><code>def join_substrings(string: str) -&gt; str:\n    \"\"\"Join sub-strings with underscores.\n\n    Args:\n        string: The string.\n\n    Returns:\n        The joined sub-strings.\n    \"\"\"\n    return re.sub(r\"\\s+\", \"_\", string)\n</code></pre>"},{"location":"reference/gemseo_benchmark/scenario/","title":"Scenario","text":""},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario","title":"scenario","text":"<p>A class to implement a benchmarking scenario (solving and reporting).</p>"},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario.Scenario","title":"Scenario","text":"<pre><code>Scenario(\n    algorithms_configurations_groups: Iterable[\n        AlgorithmsConfigurations\n    ],\n    outputs_path: str | Path,\n)\n</code></pre> <p>A benchmarking scenario, including running of solvers and reporting.</p> <p>Parameters:</p> <ul> <li> <code>algorithms_configurations_groups</code>               (<code>Iterable[AlgorithmsConfigurations]</code>)           \u2013            <p>The groups of algorithms configurations to be benchmarked.</p> </li> <li> <code>outputs_path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the directory where to save the output files (histories and report).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the path to outputs directory does not exist.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/scenario.py</code> <pre><code>def __init__(\n    self,\n    algorithms_configurations_groups: Iterable[AlgorithmsConfigurations],\n    outputs_path: str | Path,\n) -&gt; None:\n    \"\"\"\n    Args:\n        algorithms_configurations_groups: The groups of algorithms configurations\n            to be benchmarked.\n        outputs_path: The path to the directory where to save the output files\n            (histories and report).\n\n    Raises:\n        ValueError: If the path to outputs directory does not exist.\n    \"\"\"  # noqa: D205, D212, D415\n    if not Path(outputs_path).is_dir():\n        msg = f\"The path to the outputs directory does not exist: {outputs_path}.\"\n        raise NotADirectoryError(msg)\n\n    self._algorithms_configurations_groups = algorithms_configurations_groups\n    self._outputs_path = Path(outputs_path).resolve()\n    self._histories_path = self._get_dir_path(self.__HISTORIES_DIRNAME)\n    self._results_path = self._outputs_path / self.__RESULTS_FILENAME\n</code></pre>"},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario.Scenario-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario.Scenario.execute","title":"execute","text":"<pre><code>execute(\n    problems_groups: Iterable[ProblemsGroup],\n    overwrite_histories: bool = False,\n    skip_solvers: bool = False,\n    skip_report: bool = False,\n    generate_html_report: bool = True,\n    generate_pdf_report: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    save_databases: bool = False,\n    n_processes: int = 1,\n    use_threading: bool = False,\n    custom_algos_descriptions: (\n        Mapping[str, str] | None\n    ) = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_all_histories: bool = False,\n    use_log_scale: bool = False,\n    log_gemseo_to_file: bool = False,\n    directory_path: Path | None = None,\n    plot_only_median: bool = False,\n    use_abscissa_log_scale: bool = False,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n) -&gt; Results\n</code></pre> <p>Execute the benchmarking scenario.</p> <p>Parameters:</p> <ul> <li> <code>problems_groups</code>               (<code>Iterable[ProblemsGroup]</code>)           \u2013            <p>The groups of problem configurations.</p> </li> <li> <code>overwrite_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite the performance histories.</p> </li> <li> <code>skip_solvers</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to skip the running of solvers.</p> </li> <li> <code>skip_report</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to skip the generation of the report.</p> </li> <li> <code>generate_html_report</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate the report in HTML format.</p> </li> <li> <code>generate_pdf_report</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to generate the report in PDF format.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>save_databases</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the databases of the optimizations.</p> </li> <li> <code>n_processes</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The maximum number of simultaneous threads or processes used to parallelize the execution.</p> </li> <li> <code>use_threading</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use threads instead of processes to parallelize the execution.</p> </li> <li> <code>custom_algos_descriptions</code>               (<code>Mapping[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom descriptions of the algorithms, to be printed in the report instead of the default ones coded in GEMSEO.</p> </li> <li> <code>max_eval_number_per_group</code>               (<code>dict[str, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum evaluations numbers to be displayed on the graphs of each group. The keys are the groups names and the values are the maximum evaluations numbers for the graphs of the group. If <code>None</code>, all the evaluations are displayed. If the key of a group is missing, all the evaluations are displayed for the group.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale on the value axis.</p> </li> <li> <code>log_gemseo_to_file</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the GEMSEO log to a file next to the performance history file.</p> </li> <li> <code>directory_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the directory where the report will be generated.</p> </li> <li> <code>plot_only_median</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot only the median and no other centile.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Results</code>           \u2013            <p>The performance histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/scenario.py</code> <pre><code>def execute(\n    self,\n    problems_groups: Iterable[ProblemsGroup],\n    overwrite_histories: bool = False,\n    skip_solvers: bool = False,\n    skip_report: bool = False,\n    generate_html_report: bool = True,\n    generate_pdf_report: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    save_databases: bool = False,\n    n_processes: int = 1,\n    use_threading: bool = False,\n    custom_algos_descriptions: Mapping[str, str] | None = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_all_histories: bool = False,\n    use_log_scale: bool = False,\n    log_gemseo_to_file: bool = False,\n    directory_path: Path | None = None,\n    plot_only_median: bool = False,\n    use_abscissa_log_scale: bool = False,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n) -&gt; Results:\n    \"\"\"Execute the benchmarking scenario.\n\n    Args:\n        problems_groups: The groups of problem configurations.\n        overwrite_histories: Whether to overwrite the performance histories.\n        skip_solvers: Whether to skip the running of solvers.\n        skip_report: Whether to skip the generation of the report.\n        generate_html_report: Whether to generate the report in HTML format.\n        generate_pdf_report: Whether to generate the report in PDF format.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        save_databases: Whether to save the databases of the optimizations.\n        n_processes: The maximum number of simultaneous threads or processes\n            used to parallelize the execution.\n        use_threading: Whether to use threads instead of processes\n            to parallelize the execution.\n        custom_algos_descriptions: Custom descriptions of the algorithms,\n            to be printed in the report instead of the default ones coded in GEMSEO.\n        max_eval_number_per_group: The maximum evaluations numbers to be displayed\n            on the graphs of each group.\n            The keys are the groups names and the values are the maximum\n            evaluations numbers for the graphs of the group.\n            If ``None``, all the evaluations are displayed.\n            If the key of a group is missing, all the evaluations are displayed\n            for the group.\n        plot_all_histories: Whether to plot all the performance histories.\n        use_log_scale: Whether to use a logarithmic scale on the value axis.\n        log_gemseo_to_file: Whether to save the GEMSEO log to a file\n            next to the performance history file.\n        directory_path: The path to the directory where the report\n            will be generated.\n        plot_only_median: Whether to plot only the median and no other centile.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n\n    Returns:\n        The performance histories.\n    \"\"\"\n    if not skip_solvers:\n        LOGGER.info(\"Run the solvers on the problem configurations\")\n        self._run_solvers(\n            problems_groups,\n            overwrite_histories,\n            save_databases,\n            n_processes,\n            use_threading,\n            log_gemseo_to_file,\n        )\n\n    if not skip_report:\n        LOGGER.info(\"Generate the benchmarking report\")\n        self.__generate_report(\n            problems_groups,\n            generate_html_report,\n            generate_pdf_report,\n            infeasibility_tolerance,\n            custom_algos_descriptions,\n            max_eval_number_per_group,\n            plot_all_histories,\n            use_log_scale,\n            directory_path,\n            plot_only_median,\n            use_abscissa_log_scale,\n            plot_settings,\n        )\n\n    return Results(self._results_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/","title":"Algorithms","text":""},{"location":"reference/gemseo_benchmark/algorithms/#gemseo_benchmark.algorithms","title":"algorithms","text":"<p>The configurations of the algorithms from their options.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/","title":"Algorithm configuration","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration","title":"algorithm_configuration","text":"<p>Configuration of an algorithm defined by the values of its options.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration","title":"AlgorithmConfiguration","text":"<pre><code>AlgorithmConfiguration(\n    algorithm_name: str,\n    configuration_name: str = \"\",\n    instance_algorithm_options: InstanceAlgorithmOptions = READ_ONLY_EMPTY_DICT,\n    **algorithm_options: Any\n)\n</code></pre> <p>The configuration of an algorithm.</p> <p>An algorithm depends on the values of its options. A value set defines a configuration of the algorithm.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> <li> <code>configuration_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the configuration of the algorithm. If empty, a name will be generated based on the algorithm name and its options, based on the pattern <code>\"algorithm_name[option_name=option_value, ...]\"</code>.</p> </li> <li> <code>instance_algorithm_options</code>               (<code>InstanceAlgorithmOptions</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the algorithm specific to instances of a problem. They shall be passed as a mapping that links the name of an algorithm option to a callable that takes the 0-based index of the instance as argument and returns the value of the option.</p> </li> <li> <code>**algorithm_options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options of the algorithm.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>def __init__(\n    self,\n    algorithm_name: str,\n    configuration_name: str = \"\",\n    instance_algorithm_options: InstanceAlgorithmOptions = READ_ONLY_EMPTY_DICT,\n    **algorithm_options: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        algorithm_name: The name of the algorithm.\n        configuration_name: The name of the configuration of the algorithm.\n            If empty, a name will be generated based on the algorithm name and\n            its options, based on the pattern\n            ``\"algorithm_name[option_name=option_value, ...]\"``.\n        instance_algorithm_options: The options of the algorithm specific to\n            instances of a problem.\n            They shall be passed as a mapping\n            that links the name of an algorithm option\n            to a callable that takes the 0-based index of the instance as argument\n            and returns the value of the option.\n        **algorithm_options: The options of the algorithm.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__algorithm_name = algorithm_name\n    self.__algorithm_options = algorithm_options\n    self.__configuration_name = configuration_name or self.__get_configuration_name(\n        algorithm_name, **algorithm_options\n    )\n    self.__instance_algorithm_options = instance_algorithm_options\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.algorithm_name","title":"algorithm_name  <code>property</code>","text":"<pre><code>algorithm_name: str\n</code></pre> <p>The name of the algorithm.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.algorithm_options","title":"algorithm_options  <code>property</code>","text":"<pre><code>algorithm_options: dict[str, Any]\n</code></pre> <p>The options of the algorithm.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.instance_algorithm_options","title":"instance_algorithm_options  <code>property</code>","text":"<pre><code>instance_algorithm_options: InstanceAlgorithmOptions\n</code></pre> <p>The instance-specific options of the algorithm.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the algorithm configuration.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.copy","title":"copy","text":"<pre><code>copy() -&gt; AlgorithmConfiguration\n</code></pre> <p>Return a copy of the algorithm configuration.</p> <p>Returns:</p> <ul> <li> <code>AlgorithmConfiguration</code>           \u2013            <p>A copy of the algorithm configuration.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>def copy(self) -&gt; AlgorithmConfiguration:\n    \"\"\"Return a copy of the algorithm configuration.\n\n    Returns:\n        A copy of the algorithm configuration.\n    \"\"\"\n    return AlgorithmConfiguration(\n        self.algorithm_name,\n        self.name,\n        self.instance_algorithm_options,\n        **self.algorithm_options,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    algorithm_configuration: dict[\n        str, str | dict[str, Any]\n    ],\n) -&gt; AlgorithmConfiguration\n</code></pre> <p>Load an algorithm configuration from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>dict[str, str | dict[str, Any]]</code>)           \u2013            <p>The algorithm configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AlgorithmConfiguration</code>           \u2013            <p>The algorithm configuration.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls, algorithm_configuration: dict[str, str | dict[str, Any]]\n) -&gt; AlgorithmConfiguration:\n    \"\"\"Load an algorithm configuration from a dictionary.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n\n    Returns:\n        The algorithm configuration.\n    \"\"\"\n    return AlgorithmConfiguration(\n        algorithm_configuration[cls.__ALGORITHM_NAME],\n        algorithm_configuration[cls.__CONFIGURATION_NAME],\n        algorithm_configuration.get(cls.__INSTANCE_ALGORITHM_OPTIONS, {}),\n        **algorithm_configuration[cls.__ALGORITHM_OPTIONS],\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.to_dict","title":"to_dict","text":"<pre><code>to_dict(\n    skip_instance_algorithm_options: bool = False,\n) -&gt; dict[str, str | dict[str, Any]]\n</code></pre> <p>Return the algorithm configuration as a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>skip_instance_algorithm_options</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to skip the algorithm options specific to problem instances.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str | dict[str, Any]]</code>           \u2013            <p>The algorithm configuration as a dictionary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>def to_dict(\n    self, skip_instance_algorithm_options: bool = False\n) -&gt; dict[str, str | dict[str, Any]]:\n    \"\"\"Return the algorithm configuration as a dictionary.\n\n    Args:\n        skip_instance_algorithm_options: Whether to skip the algorithm options\n            specific to problem instances.\n\n    Returns:\n        The algorithm configuration as a dictionary.\n    \"\"\"\n    dictionary = {\n        self.__CONFIGURATION_NAME: self.__configuration_name,\n        self.__ALGORITHM_NAME: self.__algorithm_name,\n        self.__ALGORITHM_OPTIONS: self.__make_json_serializable(\n            self.__algorithm_options\n        ),\n    }\n    if not skip_instance_algorithm_options:\n        dictionary[self.__INSTANCE_ALGORITHM_OPTIONS] = (\n            self.__instance_algorithm_options\n        )\n\n    return dictionary\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/","title":"Algorithms configurations","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations","title":"algorithms_configurations","text":"<p>A collection of algorithms configurations.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations","title":"AlgorithmsConfigurations","text":"<pre><code>AlgorithmsConfigurations(\n    *algorithms_configurations: AlgorithmConfiguration,\n    name: str = \"\"\n)\n</code></pre> <p>               Bases: <code>MutableSet[AlgorithmConfiguration]</code></p> <p>A collection of algorithms configurations.</p> <p>Parameters:</p> <ul> <li> <code>*algorithms_configurations</code>               (<code>AlgorithmConfiguration</code>, default:                   <code>()</code> )           \u2013            <p>The algorithms configurations.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithms_configurations.py</code> <pre><code>def __init__(\n    self, *algorithms_configurations: AlgorithmConfiguration, name: str = \"\"\n) -&gt; None:\n    \"\"\"\n    Args:\n        *algorithms_configurations: The algorithms configurations.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__algorithms = []\n    self.__configurations = []\n    self.__name = name\n    self.__names = []\n    for configuration in algorithms_configurations:\n        self.add(configuration)\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.algorithms","title":"algorithms  <code>property</code>","text":"<pre><code>algorithms: list[str]\n</code></pre> <p>The names of the algorithms in alphabetical order.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.configurations","title":"configurations  <code>property</code>","text":"<pre><code>configurations: list[AlgorithmConfiguration]\n</code></pre> <p>The algorithms configurations.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the collection of algorithms configurations.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the collection of algorithms configurations has no name.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>The names of the algorithms configurations in alphabetical order.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.add","title":"add","text":"<pre><code>add(\n    algorithm_configuration: AlgorithmConfiguration,\n) -&gt; None\n</code></pre> <p>Add an algorithm configuration to the collection.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the collection already contains an algorithm configuration with the same name.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithms_configurations.py</code> <pre><code>def add(self, algorithm_configuration: AlgorithmConfiguration) -&gt; None:\n    \"\"\"Add an algorithm configuration to the collection.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n\n    Raises:\n        ValueError: If the collection already contains an algorithm configuration\n            with the same name.\n    \"\"\"\n    if algorithm_configuration in self:\n        msg = (\n            \"The collection already contains an algorithm configuration named \"\n            f\"{algorithm_configuration.name}.\"\n        )\n        raise ValueError(msg)\n\n    index = bisect.bisect(self.__names, algorithm_configuration.name)\n    self.__configurations.insert(index, algorithm_configuration)\n    bisect.insort(self.__names, algorithm_configuration.name)\n    bisect.insort(self.__algorithms, algorithm_configuration.algorithm_name)\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.discard","title":"discard","text":"<pre><code>discard(\n    algorithm_configuration: AlgorithmConfiguration,\n) -&gt; None\n</code></pre> <p>Remove an algorithm configuration.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration to remove.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithms_configurations.py</code> <pre><code>def discard(self, algorithm_configuration: AlgorithmConfiguration) -&gt; None:\n    \"\"\"Remove an algorithm configuration.\n\n    Args:\n        algorithm_configuration: The algorithm configuration to remove.\n    \"\"\"\n    self.__configurations.remove(algorithm_configuration)\n    self.__names.remove(algorithm_configuration.name)\n    if algorithm_configuration.algorithm_name not in [\n        algo_config.algorithm_name for algo_config in self\n    ]:\n        self.__algorithms.remove(algorithm_configuration.algorithm_name)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/","title":"Benchmarker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/#gemseo_benchmark.benchmarker","title":"benchmarker","text":"<p>A benchmarker to run algorithms on problems.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/","title":"Base worker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker","title":"base_worker","text":"<p>The interface for benchmarking workers.</p> <p>A benchmarking worker is responsible for:</p> <ol> <li>the execution of algorithm configurations on problem configurations,</li> <li>the creation of the associated performance histories.</li> </ol>"},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker.ProblemType","title":"ProblemType  <code>module-attribute</code>","text":"<pre><code>ProblemType = Any\n</code></pre> <p>The type of problem.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker.BaseWorker","title":"BaseWorker","text":"<p>Base class for benchmarking workers.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker.BaseWorker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker.BaseWorker.check_algorithm_availability","title":"check_algorithm_availability  <code>classmethod</code>","text":"<pre><code>check_algorithm_availability(algorithm_name: str) -&gt; None\n</code></pre> <p>Check whether an algorithm is available.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm is not available.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef check_algorithm_availability(cls, algorithm_name: str) -&gt; None:\n    \"\"\"Check whether an algorithm is available.\n\n    Args:\n        algorithm_name: The name of the algorithm.\n\n    Raises:\n        ValueError: If the algorithm is not available.\n    \"\"\"\n    if not cls._algorithm_factory.is_available(algorithm_name):\n        msg = f\"The algorithm {algorithm_name!r} is not available.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/base_worker/#gemseo_benchmark.benchmarker.base_worker.BaseWorker.execute","title":"execute  <code>classmethod</code>","text":"<pre><code>execute(\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: Logger,\n) -&gt; None\n</code></pre> <p>Create a performance history from a problem.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> <li> <code>problem_configuration</code>               (<code>BaseProblemConfiguration</code>)           \u2013            <p>The problem configuration.</p> </li> <li> <code>starting_point</code>               (<code>RealArray</code>)           \u2013            <p>The starting point of the algorithm.</p> </li> <li> <code>gemseo_log_message</code>               (<code>str</code>)           \u2013            <p>The message to log before benchmarking.</p> </li> <li> <code>log_path</code>               (<code>Path | None</code>)           \u2013            <p>The file path to save the log. If <code>None</code>, the log is not saved.</p> </li> <li> <code>performance_history_path</code>               (<code>Path</code>)           \u2013            <p>The file path to save the performance history.</p> </li> <li> <code>hdf_file_path</code>               (<code>Path | None</code>)           \u2013            <p>The HDF file path. If <code>None</code>, no HDF file will be written.</p> </li> <li> <code>benchmarking_logger</code>               (<code>Logger</code>)           \u2013            <p>The benchmarking logger.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef execute(\n    cls,\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: logging.Logger,\n) -&gt; None:\n    \"\"\"Create a performance history from a problem.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n        problem_configuration: The problem configuration.\n        starting_point: The starting point of the algorithm.\n        gemseo_log_message: The message to log before benchmarking.\n        log_path: The file path to save the log.\n            If ``None``, the log is not saved.\n        performance_history_path: The file path to save the performance history.\n        hdf_file_path: The HDF file path.\n            If ``None``, no HDF file will be written.\n        benchmarking_logger: The benchmarking logger.\n    \"\"\"\n    # Start writing in the log file.\n    if log_path is not None:\n        file_handler = logging.FileHandler(log_path, \"w\")\n        loggers = [benchmarking_logger, GEMSEO_LOGGER]\n        for logger in loggers:\n            logger.addHandler(file_handler)\n\n    problem = cls._get_problem(\n        algorithm_configuration,\n        problem_configuration,\n        starting_point,\n        hdf_file_path,\n    )\n\n    metrics_listeners = cls._add_metrics_listeners(problem)\n\n    benchmarking_logger.info(gemseo_log_message)\n    with Timer() as timer:\n        cls._execute(\n            algorithm_configuration,\n            problem_configuration,\n            starting_point,\n            problem,\n        )\n\n    # Stop writing in the log file.\n    if log_path is not None:\n        for logger in loggers:\n            logger.removeHandler(file_handler)\n\n        file_handler.close()\n\n    cls._create_performance_history(\n        algorithm_configuration,\n        problem_configuration,\n        problem,\n        timer,\n        metrics_listeners,\n    ).to_file(performance_history_path)\n    cls._post_execute(problem, hdf_file_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/","title":"Benchmarker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker","title":"benchmarker","text":"<p>Benchmarking algorithm configurations on problem configurations.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker","title":"Benchmarker","text":"<pre><code>Benchmarker(\n    histories_path: Path,\n    results_path: Path | None = None,\n    hdf_path: Path | None = None,\n)\n</code></pre> <p>A class to benchmark algorithm configurations on problem configurations.</p> <p>Parameters:</p> <ul> <li> <code>histories_path</code>               (<code>Path</code>)           \u2013            <p>The path to the directory where to save the performance histories.</p> </li> <li> <code>results_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the file for saving the performance histories paths. If exists, the file is updated with the new performance histories paths. If <code>None</code>, no performance history path will be saved.</p> </li> <li> <code>hdf_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the destination directory for the HDF files. If <code>None</code>, no HDF file will be saved.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/benchmarker.py</code> <pre><code>def __init__(\n    self,\n    histories_path: Path,\n    results_path: Path | None = None,\n    hdf_path: Path | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        histories_path: The path to the directory where to save the performance\n            histories.\n        results_path: The path to the file for saving the performance histories\n            paths.\n            If exists, the file is updated with the new performance histories paths.\n            If ``None``, no performance history path will be saved.\n        hdf_path: The path to the destination directory for the HDF files.\n            If ``None``, no HDF file will be saved.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__hdf_path = hdf_path\n    self.__histories_path = histories_path\n    self.__results_path = results_path\n    if results_path is not None and results_path.is_file():\n        self._results = Results(results_path)\n    else:\n        self._results = Results()\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker.execute","title":"execute","text":"<pre><code>execute(\n    problem_configurations: Iterable[\n        BaseProblemConfiguration\n    ],\n    algorithm_configurations: AlgorithmsConfigurations,\n    overwrite_histories: bool = False,\n    n_processes: int = 1,\n    use_threading: bool = False,\n    save_log: bool = False,\n) -&gt; Results\n</code></pre> <p>Execute algorithm configurations on problem configurations.</p> <p>Parameters:</p> <ul> <li> <code>problem_configurations</code>               (<code>Iterable[BaseProblemConfiguration]</code>)           \u2013            <p>The problem configurations.</p> </li> <li> <code>algorithm_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>overwrite_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite the existing performance histories.</p> </li> <li> <code>n_processes</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The maximum simultaneous number of threads or processes used to parallelize the execution.</p> </li> <li> <code>use_threading</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use threads instead of processes to parallelize the execution.</p> </li> <li> <code>save_log</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the log to a file. If <code>use_threading</code> is <code>True</code>, a single global file will be saved in the performance histories directory. Otherwise, one file per optimization will be saved next to each performance history file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Results</code>           \u2013            <p>The results of the benchmarking.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/benchmarker.py</code> <pre><code>def execute(\n    self,\n    problem_configurations: Iterable[BaseProblemConfiguration],\n    algorithm_configurations: AlgorithmsConfigurations,\n    overwrite_histories: bool = False,\n    n_processes: int = 1,\n    use_threading: bool = False,\n    save_log: bool = False,\n) -&gt; Results:\n    \"\"\"Execute algorithm configurations on problem configurations.\n\n    Args:\n        problem_configurations: The problem configurations.\n        algorithm_configurations: The algorithms configurations.\n        overwrite_histories: Whether to overwrite the existing performance\n            histories.\n        n_processes: The maximum simultaneous number of threads or processes\n            used to parallelize the execution.\n        use_threading: Whether to use threads instead of processes\n            to parallelize the execution.\n        save_log: Whether to save the log to a file.\n            If ``use_threading`` is ``True``, a single global file will be saved\n            in the performance histories directory.\n            Otherwise, one file per optimization will be saved\n            next to each performance history file.\n\n    Returns:\n        The results of the benchmarking.\n    \"\"\"\n    if save_log and use_threading:\n        # Set one file handler for all threads.\n        file_handler = logging.FileHandler(\n            self.__histories_path / \"gemseo.log\", \"w\"\n        )\n        file_handler.setFormatter(logging.Formatter(\"%(threadName)s %(message)s\"))\n        loggers = [LOGGER, GEMSEO_LOGGER]\n        for logger in loggers:\n            logger.addHandler(file_handler)\n\n    executor_class = ThreadPoolExecutor if use_threading else ProcessPoolExecutor\n    with executor_class(max_workers=n_processes) as executor:\n        future_to_path = {}\n        for original_algorithm_configuration in algorithm_configurations:\n            algorithm_configuration = original_algorithm_configuration.copy()\n            for problem_configuration in problem_configurations:\n                worker = problem_configuration.worker\n                worker.check_algorithm_availability(\n                    algorithm_configuration.algorithm_name\n                )\n                future_to_path.update(\n                    self.__execute(\n                        executor,\n                        worker,\n                        algorithm_configuration,\n                        problem_configuration,\n                        overwrite_histories,\n                        save_log,\n                        use_threading,\n                    )\n                )\n\n    for future in as_completed(future_to_path):\n        exception = future.exception()\n        if exception is None:\n            self._results.add_path(*future_to_path[future][1:])\n        else:\n            LOGGER.warning(\n                \"%s raised: %s\", future_to_path[future][0][:-1], exception\n            )\n\n    if save_log and use_threading:\n        for logger in loggers:\n            logger.removeHandler(file_handler)\n\n        file_handler.close()\n\n    if future_to_path and self.__results_path:\n        self._results.to_file(self.__results_path, 4)\n\n    return self._results\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker.get_history_path","title":"get_history_path","text":"<pre><code>get_history_path(\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration_name: str,\n    index: int,\n    make_parents: bool = False,\n) -&gt; Path\n</code></pre> <p>Return a path for a history file.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> <li> <code>problem_configuration_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem configuration.</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The index of the problem.</p> </li> <li> <code>make_parents</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to make the parent directories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>           \u2013            <p>The path for the history file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/benchmarker.py</code> <pre><code>def get_history_path(\n    self,\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration_name: str,\n    index: int,\n    make_parents: bool = False,\n) -&gt; Path:\n    \"\"\"Return a path for a history file.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n        problem_configuration_name: The name of the problem configuration.\n        index: The index of the problem.\n        make_parents: Whether to make the parent directories.\n\n    Returns:\n        The path for the history file.\n    \"\"\"\n    return self._get_path(\n        self.__histories_path,\n        algorithm_configuration,\n        problem_configuration_name,\n        index,\n        \"json\",\n        make_parents=make_parents,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/mda_worker/","title":"Mda worker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/mda_worker/#gemseo_benchmark.benchmarker.mda_worker","title":"mda_worker","text":"<p>Benchmarking worker for multidisciplinary analysis.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/mda_worker/#gemseo_benchmark.benchmarker.mda_worker-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/mda_worker/#gemseo_benchmark.benchmarker.mda_worker.MDAWorker","title":"MDAWorker","text":"<p>               Bases: <code>BaseWorker</code></p> <p>A benchmarking worker for multidisciplinary analysis.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/mda_worker/#gemseo_benchmark.benchmarker.mda_worker.MDAWorker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/mda_worker/#gemseo_benchmark.benchmarker.mda_worker.MDAWorker.check_algorithm_availability","title":"check_algorithm_availability  <code>classmethod</code>","text":"<pre><code>check_algorithm_availability(algorithm_name: str) -&gt; None\n</code></pre> <p>Check whether an algorithm is available.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm is not available.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef check_algorithm_availability(cls, algorithm_name: str) -&gt; None:\n    \"\"\"Check whether an algorithm is available.\n\n    Args:\n        algorithm_name: The name of the algorithm.\n\n    Raises:\n        ValueError: If the algorithm is not available.\n    \"\"\"\n    if not cls._algorithm_factory.is_available(algorithm_name):\n        msg = f\"The algorithm {algorithm_name!r} is not available.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/mda_worker/#gemseo_benchmark.benchmarker.mda_worker.MDAWorker.execute","title":"execute  <code>classmethod</code>","text":"<pre><code>execute(\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: Logger,\n) -&gt; None\n</code></pre> <p>Create a performance history from a problem.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> <li> <code>problem_configuration</code>               (<code>BaseProblemConfiguration</code>)           \u2013            <p>The problem configuration.</p> </li> <li> <code>starting_point</code>               (<code>RealArray</code>)           \u2013            <p>The starting point of the algorithm.</p> </li> <li> <code>gemseo_log_message</code>               (<code>str</code>)           \u2013            <p>The message to log before benchmarking.</p> </li> <li> <code>log_path</code>               (<code>Path | None</code>)           \u2013            <p>The file path to save the log. If <code>None</code>, the log is not saved.</p> </li> <li> <code>performance_history_path</code>               (<code>Path</code>)           \u2013            <p>The file path to save the performance history.</p> </li> <li> <code>hdf_file_path</code>               (<code>Path | None</code>)           \u2013            <p>The HDF file path. If <code>None</code>, no HDF file will be written.</p> </li> <li> <code>benchmarking_logger</code>               (<code>Logger</code>)           \u2013            <p>The benchmarking logger.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef execute(\n    cls,\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: logging.Logger,\n) -&gt; None:\n    \"\"\"Create a performance history from a problem.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n        problem_configuration: The problem configuration.\n        starting_point: The starting point of the algorithm.\n        gemseo_log_message: The message to log before benchmarking.\n        log_path: The file path to save the log.\n            If ``None``, the log is not saved.\n        performance_history_path: The file path to save the performance history.\n        hdf_file_path: The HDF file path.\n            If ``None``, no HDF file will be written.\n        benchmarking_logger: The benchmarking logger.\n    \"\"\"\n    # Start writing in the log file.\n    if log_path is not None:\n        file_handler = logging.FileHandler(log_path, \"w\")\n        loggers = [benchmarking_logger, GEMSEO_LOGGER]\n        for logger in loggers:\n            logger.addHandler(file_handler)\n\n    problem = cls._get_problem(\n        algorithm_configuration,\n        problem_configuration,\n        starting_point,\n        hdf_file_path,\n    )\n\n    metrics_listeners = cls._add_metrics_listeners(problem)\n\n    benchmarking_logger.info(gemseo_log_message)\n    with Timer() as timer:\n        cls._execute(\n            algorithm_configuration,\n            problem_configuration,\n            starting_point,\n            problem,\n        )\n\n    # Stop writing in the log file.\n    if log_path is not None:\n        for logger in loggers:\n            logger.removeHandler(file_handler)\n\n        file_handler.close()\n\n    cls._create_performance_history(\n        algorithm_configuration,\n        problem_configuration,\n        problem,\n        timer,\n        metrics_listeners,\n    ).to_file(performance_history_path)\n    cls._post_execute(problem, hdf_file_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/mdo_worker/","title":"Mdo worker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/mdo_worker/#gemseo_benchmark.benchmarker.mdo_worker","title":"mdo_worker","text":"<p>Benchmarking worker for multidisciplinary optimization.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/mdo_worker/#gemseo_benchmark.benchmarker.mdo_worker-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/mdo_worker/#gemseo_benchmark.benchmarker.mdo_worker.MDOWorker","title":"MDOWorker","text":"<p>               Bases: <code>BaseWorker</code></p> <p>A benchmarking worker for multidisciplinary optimization.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/mdo_worker/#gemseo_benchmark.benchmarker.mdo_worker.MDOWorker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/mdo_worker/#gemseo_benchmark.benchmarker.mdo_worker.MDOWorker.check_algorithm_availability","title":"check_algorithm_availability  <code>classmethod</code>","text":"<pre><code>check_algorithm_availability(algorithm_name: str) -&gt; None\n</code></pre> <p>Check whether an algorithm is available.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm is not available.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef check_algorithm_availability(cls, algorithm_name: str) -&gt; None:\n    \"\"\"Check whether an algorithm is available.\n\n    Args:\n        algorithm_name: The name of the algorithm.\n\n    Raises:\n        ValueError: If the algorithm is not available.\n    \"\"\"\n    if not cls._algorithm_factory.is_available(algorithm_name):\n        msg = f\"The algorithm {algorithm_name!r} is not available.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/mdo_worker/#gemseo_benchmark.benchmarker.mdo_worker.MDOWorker.execute","title":"execute  <code>classmethod</code>","text":"<pre><code>execute(\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: Logger,\n) -&gt; None\n</code></pre> <p>Create a performance history from a problem.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> <li> <code>problem_configuration</code>               (<code>BaseProblemConfiguration</code>)           \u2013            <p>The problem configuration.</p> </li> <li> <code>starting_point</code>               (<code>RealArray</code>)           \u2013            <p>The starting point of the algorithm.</p> </li> <li> <code>gemseo_log_message</code>               (<code>str</code>)           \u2013            <p>The message to log before benchmarking.</p> </li> <li> <code>log_path</code>               (<code>Path | None</code>)           \u2013            <p>The file path to save the log. If <code>None</code>, the log is not saved.</p> </li> <li> <code>performance_history_path</code>               (<code>Path</code>)           \u2013            <p>The file path to save the performance history.</p> </li> <li> <code>hdf_file_path</code>               (<code>Path | None</code>)           \u2013            <p>The HDF file path. If <code>None</code>, no HDF file will be written.</p> </li> <li> <code>benchmarking_logger</code>               (<code>Logger</code>)           \u2013            <p>The benchmarking logger.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef execute(\n    cls,\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: logging.Logger,\n) -&gt; None:\n    \"\"\"Create a performance history from a problem.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n        problem_configuration: The problem configuration.\n        starting_point: The starting point of the algorithm.\n        gemseo_log_message: The message to log before benchmarking.\n        log_path: The file path to save the log.\n            If ``None``, the log is not saved.\n        performance_history_path: The file path to save the performance history.\n        hdf_file_path: The HDF file path.\n            If ``None``, no HDF file will be written.\n        benchmarking_logger: The benchmarking logger.\n    \"\"\"\n    # Start writing in the log file.\n    if log_path is not None:\n        file_handler = logging.FileHandler(log_path, \"w\")\n        loggers = [benchmarking_logger, GEMSEO_LOGGER]\n        for logger in loggers:\n            logger.addHandler(file_handler)\n\n    problem = cls._get_problem(\n        algorithm_configuration,\n        problem_configuration,\n        starting_point,\n        hdf_file_path,\n    )\n\n    metrics_listeners = cls._add_metrics_listeners(problem)\n\n    benchmarking_logger.info(gemseo_log_message)\n    with Timer() as timer:\n        cls._execute(\n            algorithm_configuration,\n            problem_configuration,\n            starting_point,\n            problem,\n        )\n\n    # Stop writing in the log file.\n    if log_path is not None:\n        for logger in loggers:\n            logger.removeHandler(file_handler)\n\n        file_handler.close()\n\n    cls._create_performance_history(\n        algorithm_configuration,\n        problem_configuration,\n        problem,\n        timer,\n        metrics_listeners,\n    ).to_file(performance_history_path)\n    cls._post_execute(problem, hdf_file_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/optimization_worker/","title":"Optimization worker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/optimization_worker/#gemseo_benchmark.benchmarker.optimization_worker","title":"optimization_worker","text":"<p>Benchmarking worker for optimization algorithms.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/optimization_worker/#gemseo_benchmark.benchmarker.optimization_worker-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/optimization_worker/#gemseo_benchmark.benchmarker.optimization_worker.OptimizationWorker","title":"OptimizationWorker","text":"<p>               Bases: <code>BaseWorker</code></p> <p>A benchmarking worker for optimization.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/optimization_worker/#gemseo_benchmark.benchmarker.optimization_worker.OptimizationWorker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/optimization_worker/#gemseo_benchmark.benchmarker.optimization_worker.OptimizationWorker.check_algorithm_availability","title":"check_algorithm_availability  <code>classmethod</code>","text":"<pre><code>check_algorithm_availability(algorithm_name: str) -&gt; None\n</code></pre> <p>Check whether an algorithm is available.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm is not available.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef check_algorithm_availability(cls, algorithm_name: str) -&gt; None:\n    \"\"\"Check whether an algorithm is available.\n\n    Args:\n        algorithm_name: The name of the algorithm.\n\n    Raises:\n        ValueError: If the algorithm is not available.\n    \"\"\"\n    if not cls._algorithm_factory.is_available(algorithm_name):\n        msg = f\"The algorithm {algorithm_name!r} is not available.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/optimization_worker/#gemseo_benchmark.benchmarker.optimization_worker.OptimizationWorker.execute","title":"execute  <code>classmethod</code>","text":"<pre><code>execute(\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: Logger,\n) -&gt; None\n</code></pre> <p>Create a performance history from a problem.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> <li> <code>problem_configuration</code>               (<code>BaseProblemConfiguration</code>)           \u2013            <p>The problem configuration.</p> </li> <li> <code>starting_point</code>               (<code>RealArray</code>)           \u2013            <p>The starting point of the algorithm.</p> </li> <li> <code>gemseo_log_message</code>               (<code>str</code>)           \u2013            <p>The message to log before benchmarking.</p> </li> <li> <code>log_path</code>               (<code>Path | None</code>)           \u2013            <p>The file path to save the log. If <code>None</code>, the log is not saved.</p> </li> <li> <code>performance_history_path</code>               (<code>Path</code>)           \u2013            <p>The file path to save the performance history.</p> </li> <li> <code>hdf_file_path</code>               (<code>Path | None</code>)           \u2013            <p>The HDF file path. If <code>None</code>, no HDF file will be written.</p> </li> <li> <code>benchmarking_logger</code>               (<code>Logger</code>)           \u2013            <p>The benchmarking logger.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/base_worker.py</code> <pre><code>@classmethod\ndef execute(\n    cls,\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_configuration: BaseProblemConfiguration,\n    starting_point: RealArray,\n    gemseo_log_message: str,\n    log_path: Path | None,\n    performance_history_path: Path,\n    hdf_file_path: Path | None,\n    benchmarking_logger: logging.Logger,\n) -&gt; None:\n    \"\"\"Create a performance history from a problem.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n        problem_configuration: The problem configuration.\n        starting_point: The starting point of the algorithm.\n        gemseo_log_message: The message to log before benchmarking.\n        log_path: The file path to save the log.\n            If ``None``, the log is not saved.\n        performance_history_path: The file path to save the performance history.\n        hdf_file_path: The HDF file path.\n            If ``None``, no HDF file will be written.\n        benchmarking_logger: The benchmarking logger.\n    \"\"\"\n    # Start writing in the log file.\n    if log_path is not None:\n        file_handler = logging.FileHandler(log_path, \"w\")\n        loggers = [benchmarking_logger, GEMSEO_LOGGER]\n        for logger in loggers:\n            logger.addHandler(file_handler)\n\n    problem = cls._get_problem(\n        algorithm_configuration,\n        problem_configuration,\n        starting_point,\n        hdf_file_path,\n    )\n\n    metrics_listeners = cls._add_metrics_listeners(problem)\n\n    benchmarking_logger.info(gemseo_log_message)\n    with Timer() as timer:\n        cls._execute(\n            algorithm_configuration,\n            problem_configuration,\n            starting_point,\n            problem,\n        )\n\n    # Stop writing in the log file.\n    if log_path is not None:\n        for logger in loggers:\n            logger.removeHandler(file_handler)\n\n        file_handler.close()\n\n    cls._create_performance_history(\n        algorithm_configuration,\n        problem_configuration,\n        problem,\n        timer,\n        metrics_listeners,\n    ).to_file(performance_history_path)\n    cls._post_execute(problem, hdf_file_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/","title":"Data profiles","text":""},{"location":"reference/gemseo_benchmark/data_profiles/#gemseo_benchmark.data_profiles","title":"data_profiles","text":"<p>Computation of data profiles for algorithms comparison.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/","title":"Data profile","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile","title":"data_profile","text":"<p>Class to compute data profiles for algorithms comparison.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile","title":"DataProfile","text":"<pre><code>DataProfile(target_values: Mapping[str, TargetValues])\n</code></pre> <p>Data profile that compares iterative algorithms on reference problems.</p> <p>A data profile is a graphical tool to compare iterative algorithms, e.g. optimization algorithms or root-finding algorithms, on reference problems.</p> <p>Each of the reference problems must be assigned targets, i.e. values of the objective function or values of the residual norm, ranging from a first acceptable value to the best known value for the problem.</p> <p>The algorithms will be compared based on the number of targets they reach, cumulated over all the reference problems, relative to the number of problems functions evaluations they make.</p> <p>The data profile is the empirical cumulated distribution function of the number of functions evaluations made by an algorithm to reach a problem target.</p> <p>Parameters:</p> <ul> <li> <code>target_values</code>               (<code>Mapping[str, TargetValues]</code>)           \u2013            <p>The target values of each of the reference problems.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def __init__(self, target_values: Mapping[str, TargetValues]) -&gt; None:\n    \"\"\"\n    Args:\n        target_values: The target values of each of the reference problems.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__targets_number = 0\n    self.target_values = target_values\n    self.__values_histories = {}\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.target_values","title":"target_values  <code>property</code> <code>writable</code>","text":"<pre><code>target_values: dict[str, TargetValues]\n</code></pre> <p>The target values of each reference problem.</p> <p>Target values are a scale of objective function values, ranging from an easily achievable one to the best known value. A data profile is computed by counting the number of targets reached by an algorithm at each iteration.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the reference problems have different numbers of target values.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.add_history","title":"add_history","text":"<pre><code>add_history(\n    problem_name: str,\n    algorithm_configuration_name: str,\n    performance_measures: Sequence[float],\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n) -&gt; None\n</code></pre> <p>Add a history of performance values.</p> <p>Parameters:</p> <ul> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> <li> <code>algorithm_configuration_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration.</p> </li> <li> <code>performance_measures</code>               (<code>Sequence[float]</code>)           \u2013            <p>A history of performance measures. N.B. the value at index <code>i</code> is assumed to have been obtained with <code>i+1</code> evaluations.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of infeasibility measures. If <code>None</code> then measures are set to zero in case of feasibility and set to infinity otherwise.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of (boolean) feasibility statuses. If <code>None</code> then feasibility is always assumed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the problem name is not the name of a reference problem.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def add_history(\n    self,\n    problem_name: str,\n    algorithm_configuration_name: str,\n    performance_measures: Sequence[float],\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n) -&gt; None:\n    \"\"\"Add a history of performance values.\n\n    Args:\n        problem_name: The name of the problem.\n        algorithm_configuration_name: The name of the algorithm configuration.\n        performance_measures: A history of performance measures.\n            N.B. the value at index ``i`` is assumed to have been obtained with\n            ``i+1`` evaluations.\n        infeasibility_measures: A history of infeasibility measures.\n            If ``None`` then measures are set to zero in case of feasibility and set\n            to infinity otherwise.\n        feasibility_statuses: A history of (boolean) feasibility statuses.\n            If ``None`` then feasibility is always assumed.\n\n    Raises:\n        ValueError: If the problem name is not the name of a reference problem.\n    \"\"\"\n    if problem_name not in self.__target_values:\n        msg = f\"{problem_name!r} is not the name of a reference problem\"\n        raise ValueError(msg)\n    if algorithm_configuration_name not in self.__values_histories:\n        self.__values_histories[algorithm_configuration_name] = {\n            pb_name: [] for pb_name in self.__target_values\n        }\n    history = PerformanceHistory(\n        performance_measures, infeasibility_measures, feasibility_statuses\n    )\n    self.__values_histories[algorithm_configuration_name][problem_name].append(\n        history\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.compute_data_profiles","title":"compute_data_profiles","text":"<pre><code>compute_data_profiles(\n    *algo_names: str,\n) -&gt; dict[str, list[Number]]\n</code></pre> <p>Compute the data profiles of the required algorithms.</p> <p>For each algorithm, compute the cumulative distribution function of the number of evaluations required by the algorithm to reach a reference target.</p> <p>Parameters:</p> <ul> <li> <code>algo_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>The names of the algorithms. If <code>None</code> then all the algorithms are considered.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list[Number]]</code>           \u2013            <p>The data profiles.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def compute_data_profiles(self, *algo_names: str) -&gt; dict[str, list[Number]]:\n    \"\"\"Compute the data profiles of the required algorithms.\n\n    For each algorithm, compute the cumulative distribution function of the number\n    of evaluations required by the algorithm to reach a reference target.\n\n    Args:\n        algo_names: The names of the algorithms.\n            If ``None`` then all the algorithms are considered.\n\n    Returns:\n        The data profiles.\n    \"\"\"\n    data_profiles = {}\n    if not algo_names:\n        algo_names = self.__values_histories.keys()\n\n    for name in algo_names:\n        total_hits_history = self.__compute_hits_history(name)\n        problems_number = len(self.__target_values)\n        repeat_number = self.__get_repeat_number(name)\n        targets_total = self.__targets_number * problems_number * repeat_number\n        ratios = total_hits_history / targets_total\n        data_profiles[name] = ratios.tolist()\n    return data_profiles\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.plot","title":"plot","text":"<pre><code>plot(\n    algo_names: Iterable[str] | None = None,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    markevery: MarkeveryType | None = None,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Plot the data profiles of the required algorithms.</p> <p>Parameters:</p> <ul> <li> <code>algo_names</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>The names of the algorithms. If <code>None</code> then all the algorithms are considered.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, show the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>markevery</code>               (<code>MarkeveryType | None</code>, default:                   <code>None</code> )           \u2013            <p>The sampling parameter for the markers of the plot. Refer to the Matplotlib documentation.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def plot(\n    self,\n    algo_names: Iterable[str] | None = None,\n    show: bool = True,\n    file_path: str | Path = \"\",\n    markevery: MarkeveryType | None = None,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Plot the data profiles of the required algorithms.\n\n    Args:\n        algo_names: The names of the algorithms.\n            If ``None`` then all the algorithms are considered.\n        show: If True, show the plot.\n        file_path: The path where to save the plot.\n            If empty, the plot is not saved.\n        markevery: The sampling parameter for the markers of the plot.\n            Refer to the Matplotlib documentation.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n    \"\"\"\n    if algo_names is None:\n        algo_names = ()\n\n    data_profiles = self.compute_data_profiles(*algo_names)\n    plot_settings_copy = plot_settings.copy()\n    for settings in plot_settings_copy.values():\n        if \"markevery\" not in settings:\n            settings[\"markevery\"] = markevery\n\n    figure = self._plot_data_profiles(\n        data_profiles, plot_settings_copy, grid_settings, use_abscissa_log_scale\n    )\n    save_show_figure(figure, show, file_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/","title":"Target values","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values","title":"target_values","text":"<p>Computation of target values out of algorithms performance histories.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues","title":"TargetValues","text":"<pre><code>TargetValues(\n    performance_measures: Sequence[float] = (),\n    infeasibility_measures: Sequence[float] = (),\n    feasibility_statuses: Sequence[bool] = (),\n    n_unsatisfied_constraints: Sequence[int] = (),\n    problem_name: str = \"\",\n    objective_name: str = \"\",\n    constraints_names: Sequence[str] = (),\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: (\n        AlgorithmConfiguration | None\n    ) = None,\n    number_of_variables: int | None = None,\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n)\n</code></pre> <p>               Bases: <code>PerformanceHistory</code></p> <p>Target values of a problem.</p> <p>Consider a problem to be solved by an iterative algorithm, e.g. an optimization problem or a root-finding problem. Targets are values, i.e. values of the objective function or values of the residual norm, ranging from a first acceptable value to the best known value for the problem. Targets are used to estimate the efficiency (relative to the number of problem functions evaluations) of an algorithm to solve a problem (or several) and computes its data profile (see :mod:<code>.data_profiles.data_profile</code>).</p> <p>Parameters:</p> <ul> <li> <code>performance_measures</code>               (<code>Sequence[float]</code>, default:                   <code>()</code> )           \u2013            <p>The history of performance measures.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float]</code>, default:                   <code>()</code> )           \u2013            <p>The history of infeasibility measures. An infeasibility measure is a non-negative real number representing the gap between the design and the feasible space, a zero value meaning feasibility. If empty and <code>feasibility_statuses</code> is not empty then the infeasibility measures are set to zero in case of feasibility, and set to infinity otherwise. If empty and <code>feasibility_statuses</code> is empty then every infeasibility measure is set to zero.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the (boolean) feasibility statuses. If <code>infeasibility_measures</code> is not empty then <code>feasibility_statuses</code> is disregarded. If empty and 'infeasibility_measures' is empty then every infeasibility measure is set to zero.</p> </li> <li> <code>n_unsatisfied_constraints</code>               (<code>Sequence[int]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the number of unsatisfied constraints. If empty, the entries will be set to 0 for feasible entries and <code>None</code> for infeasible entries.</p> </li> <li> <code>problem_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the problem.</p> </li> <li> <code>objective_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the objective function.</p> </li> <li> <code>constraints_names</code>               (<code>Sequence[str]</code>, default:                   <code>()</code> )           \u2013            <p>The names the scalar constraints. Each name must correspond to a scalar value. If empty, they will not be set.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the initial design of experiments. If <code>None</code>, it will not be set.</p> </li> <li> <code>total_time</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The total time of the optimization, in seconds. If <code>None</code>, it will not be set.</p> </li> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the algorithm which generated the history. If <code>None</code>, it will not be set.</p> </li> <li> <code>number_of_variables</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of optimization variables. If <code>None</code>, it will not be set.</p> </li> <li> <code>elapsed_times</code>               (<code>Sequence[timedelta]</code>, default:                   <code>()</code> )           \u2013            <p>The history of elapsed times. If empty, the elapsed times are set to zero.</p> </li> <li> <code>number_of_discipline_executions</code>               (<code>Sequence[int]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the number of discipline executions. If <code>empty</code>, the number of discipline executions are set to zero.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the lengths of the histories do not match.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def __init__(\n    self,\n    performance_measures: Sequence[float] = (),\n    infeasibility_measures: Sequence[float] = (),\n    feasibility_statuses: Sequence[bool] = (),\n    n_unsatisfied_constraints: Sequence[int] = (),\n    problem_name: str = \"\",\n    objective_name: str = \"\",\n    constraints_names: Sequence[str] = (),\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: AlgorithmConfiguration | None = None,\n    number_of_variables: int | None = None,\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n) -&gt; None:\n    \"\"\"\n    Args:\n        performance_measures: The history of performance measures.\n        infeasibility_measures: The history of infeasibility measures.\n            An infeasibility measure is a non-negative real number representing\n            the gap between the design and the feasible space,\n            a zero value meaning feasibility.\n            If empty and `feasibility_statuses` is not empty\n            then the infeasibility measures are set to zero in case of feasibility,\n            and set to infinity otherwise.\n            If empty and `feasibility_statuses` is empty\n            then every infeasibility measure is set to zero.\n        feasibility_statuses: The history of the (boolean) feasibility statuses.\n            If `infeasibility_measures` is not empty then `feasibility_statuses` is\n            disregarded.\n            If empty and 'infeasibility_measures' is empty\n            then every infeasibility measure is set to zero.\n        n_unsatisfied_constraints: The history of the number of unsatisfied\n            constraints.\n            If empty, the entries will be set to 0 for feasible entries\n            and ``None`` for infeasible entries.\n        problem_name: The name of the problem.\n        objective_name: The name of the objective function.\n        constraints_names: The names the scalar constraints.\n            Each name must correspond to a scalar value.\n            If empty, they will not be set.\n        doe_size: The size of the initial design of experiments.\n            If ``None``, it will not be set.\n        total_time: The total time of the optimization, in seconds.\n            If ``None``, it will not be set.\n        algorithm_configuration: The name of the algorithm which generated the\n            history.\n            If ``None``, it will not be set.\n        number_of_variables: The number of optimization variables.\n            If ``None``, it will not be set.\n        elapsed_times: The history of elapsed times.\n            If empty, the elapsed times are set to zero.\n        number_of_discipline_executions: The history of the number\n            of discipline executions.\n            If `empty`, the number of discipline executions are set to zero.\n\n    Raises:\n        ValueError: If the lengths of the histories do not match.\n    \"\"\"  # noqa: D205, D212, D415\n    self._constraints_names = constraints_names\n    self._objective_name = objective_name\n    self.algorithm_configuration = algorithm_configuration\n    self.doe_size = doe_size\n    self.items = self.__get_history_items(\n        performance_measures,\n        infeasibility_measures,\n        feasibility_statuses,\n        n_unsatisfied_constraints,\n        elapsed_times,\n        number_of_discipline_executions,\n    )\n    self.problem_name = problem_name\n    self._number_of_variables = number_of_variables\n    self.total_time = total_time\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.infeasibility_measures","title":"infeasibility_measures  <code>property</code>","text":"<pre><code>infeasibility_measures: list[float]\n</code></pre> <p>The infeasibility measures.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.items","title":"items  <code>property</code> <code>writable</code>","text":"<pre><code>items: list[HistoryItem]\n</code></pre> <p>The history items.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.n_unsatisfied_constraints","title":"n_unsatisfied_constraints  <code>property</code>","text":"<pre><code>n_unsatisfied_constraints: list[int]\n</code></pre> <p>The numbers of unsatisfied constraints.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.performance_measures","title":"performance_measures  <code>property</code>","text":"<pre><code>performance_measures: list[float]\n</code></pre> <p>The performance measures.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.apply_infeasibility_tolerance","title":"apply_infeasibility_tolerance","text":"<pre><code>apply_infeasibility_tolerance(\n    infeasibility_tolerance: float,\n) -&gt; None\n</code></pre> <p>Apply a tolerance on the infeasibility measures of the history items.</p> <p>Mark the history items with an infeasibility measure below the tolerance as feasible.</p> <p>Parameters:</p> <ul> <li> <code>infeasibility_tolerance</code>               (<code>float</code>)           \u2013            <p>the tolerance on the infeasibility measure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def apply_infeasibility_tolerance(self, infeasibility_tolerance: float) -&gt; None:\n    \"\"\"Apply a tolerance on the infeasibility measures of the history items.\n\n    Mark the history items with an infeasibility measure below the tolerance\n    as feasible.\n\n    Args:\n        infeasibility_tolerance: the tolerance on the infeasibility measure.\n    \"\"\"\n    for item in self.items:\n        item.apply_infeasibility_tolerance(infeasibility_tolerance)\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.compute_cumulated_minimum","title":"compute_cumulated_minimum","text":"<pre><code>compute_cumulated_minimum() -&gt; PerformanceHistory\n</code></pre> <p>Return the history of the cumulated minimum.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The history of the cumulated minimum.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def compute_cumulated_minimum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history of the cumulated minimum.\n\n    Returns:\n        The history of the cumulated minimum.\n    \"\"\"\n    minimum_history = copy(self)\n    minimum_history.items = minimum_history.items[:1]\n    for item in self.items[1:]:\n        last_item = minimum_history.items[-1]\n        if item &gt;= last_item:\n            new_item = last_item.copy()\n            # N.B. The copy ensures the new items are independent objects.\n            new_item.elapsed_time = item.elapsed_time\n        else:\n            new_item = item\n\n        minimum_history.items.append(new_item)\n\n    return minimum_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.compute_target_hits_history","title":"compute_target_hits_history","text":"<pre><code>compute_target_hits_history(\n    values_history: PerformanceHistory,\n) -&gt; list[int]\n</code></pre> <p>Compute the history of the number of target hits for a performance history.</p> <p>Parameters:</p> <ul> <li> <code>values_history</code>               (<code>PerformanceHistory</code>)           \u2013            <p>The history of values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>The history of the number of target hits.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/target_values.py</code> <pre><code>def compute_target_hits_history(\n    self, values_history: PerformanceHistory\n) -&gt; list[int]:\n    \"\"\"Compute the history of the number of target hits for a performance history.\n\n    Args:\n        values_history: The history of values.\n\n    Returns:\n        The history of the number of target hits.\n    \"\"\"\n    minimum_history = values_history.compute_cumulated_minimum()\n    return [\n        [minimum &lt;= target for target in self].count(True)\n        for minimum in minimum_history\n    ]\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.extend","title":"extend","text":"<pre><code>extend(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Extend the performance history by repeating its last item.</p> <p>If the history is longer than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the extended performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The extended performance history.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the expected size is smaller than the history size.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def extend(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Extend the performance history by repeating its last item.\n\n    If the history is longer than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the extended performance history.\n\n    Returns:\n        The extended performance history.\n\n    Raises:\n        ValueError: If the expected size is smaller than the history size.\n    \"\"\"\n    if size &lt; len(self):\n        msg = (\n            f\"The expected size ({size}) is smaller than \"\n            f\"the history size ({len(self)}).\"\n        )\n        raise ValueError(msg)\n\n    history = copy(self)\n    history.items = list(self)\n    for _ in range(size - len(self)):\n        history.items.append(self[-1].copy())\n        # N.B. The copy ensures the new items are independent objects.\n\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str | Path) -&gt; PerformanceHistory\n</code></pre> <p>Create a new performance history from a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str | Path) -&gt; PerformanceHistory:\n    \"\"\"Create a new performance history from a file.\n\n    Args:\n        path: The path to the file.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    with Path(path).open(\"r\") as file:\n        data = json.load(file)\n\n    history = cls()\n    history.problem_name = data.get(cls.__PROBLEM)\n    history._number_of_variables = data.get(cls.__NUMBER_OF_VARIABLES)\n    history._objective_name = data.get(cls.__OBJECTIVE_NAME)\n    history._constraints_names = data.get(cls.__CONSTRAINTS_NAMES, [])\n    if cls.__ALGORITHM_CONFIGURATION in data:\n        history.algorithm_configuration = AlgorithmConfiguration.from_dict(\n            data[cls.__ALGORITHM_CONFIGURATION]\n        )\n\n    history.doe_size = data.get(cls.__DOE_SIZE)\n    history.total_time = data.get(cls.__EXECUTION_TIME)\n    history.items = [\n        HistoryItem.from_dict(item_data) for item_data in data[cls.__HISTORY_ITEMS]\n    ]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.from_problem","title":"from_problem  <code>classmethod</code>","text":"<pre><code>from_problem(\n    problem: OptimizationProblem,\n    problem_name: str = \"\",\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n) -&gt; PerformanceHistory\n</code></pre> <p>Create a performance history from a solved optimization problem.</p> <p>Parameters:</p> <ul> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> <li> <code>problem_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the problem.</p> </li> <li> <code>elapsed_times</code>               (<code>Sequence[timedelta]</code>, default:                   <code>()</code> )           \u2013            <p>The history of elapsed times. If empty, the elapsed times are set to zero.</p> </li> <li> <code>number_of_discipline_executions</code>               (<code>Sequence[int]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the number of discipline executions. If <code>empty</code>, the number of discipline executions are set to zero.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_problem(\n    cls,\n    problem: OptimizationProblem,\n    problem_name: str = \"\",\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n) -&gt; PerformanceHistory:\n    \"\"\"Create a performance history from a solved optimization problem.\n\n    Args:\n        problem: The optimization problem.\n        problem_name: The name of the problem.\n        elapsed_times: The history of elapsed times.\n            If empty, the elapsed times are set to zero.\n        number_of_discipline_executions: The history of the number\n            of discipline executions.\n            If `empty`, the number of discipline executions are set to zero.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    obj_name = problem.objective.name\n    obj_values = []\n    infeas_measures = []\n    feas_statuses = []\n    n_unsatisfied_constraints = []\n    retained_elapsed_times = []\n    retained_number_of_discipline_executions = []\n    functions_names = {obj_name, *problem.constraints.get_names()}\n    for index, (design_values, output_values) in enumerate(\n        problem.database.items()\n    ):\n        # Only consider points with all functions values\n        if not functions_names &lt;= set(output_values.keys()):\n            continue\n\n        x_vect = design_values.unwrap()\n        obj_values.append(atleast_1d(output_values[obj_name]).real[0])\n        feasibility, measure = problem.history.check_design_point_is_feasible(\n            x_vect\n        )\n        number_of_unsatisfied_constraints = (\n            problem.constraints.get_number_of_unsatisfied_constraints(output_values)\n        )\n        infeas_measures.append(measure)\n        feas_statuses.append(feasibility)\n        n_unsatisfied_constraints.append(number_of_unsatisfied_constraints)\n        if elapsed_times:\n            retained_elapsed_times.append(elapsed_times[index])\n\n        if number_of_discipline_executions:\n            retained_number_of_discipline_executions.append(\n                number_of_discipline_executions[index]\n            )\n\n    return cls(\n        obj_values,\n        infeas_measures,\n        feas_statuses,\n        n_unsatisfied_constraints,\n        problem_name,\n        problem.objective.name,\n        problem.scalar_constraint_names,\n        number_of_variables=problem.design_space.dimension,\n        elapsed_times=retained_elapsed_times,\n        number_of_discipline_executions=retained_number_of_discipline_executions,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.get_plot_data","title":"get_plot_data","text":"<pre><code>get_plot_data(\n    feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]\n</code></pre> <p>Return the data to plot the performance history.</p> <p>Parameters:</p> <ul> <li> <code>feasible</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get only feasible values.</p> </li> <li> <code>minimum_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get the history of the cumulated minimum instead of the history of the performance measure.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[int], list[HistoryItem]]</code>           \u2013            <p>The abscissas and the ordinates of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def get_plot_data(\n    self, feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]:\n    \"\"\"Return the data to plot the performance history.\n\n    Args:\n        feasible: Whether to get only feasible values.\n        minimum_history: Whether to get the history of the cumulated minimum\n            instead of the history of the performance measure.\n\n    Returns:\n        The abscissas and the ordinates of the plot.\n    \"\"\"\n    history = self.compute_cumulated_minimum() if minimum_history else self\n\n    # Find the index of the first feasible history item\n    if feasible:\n        first_feasible_index = len(history)\n        for index, item in enumerate(history):\n            if item.is_feasible:\n                first_feasible_index = index\n                break\n    else:\n        first_feasible_index = 0\n\n    return (\n        list(range(first_feasible_index + 1, len(history) + 1)),\n        history[first_feasible_index:],\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.plot","title":"plot","text":"<pre><code>plot(\n    show: bool = True, file_path: str | Path = \"\"\n) -&gt; Figure\n</code></pre> <p>Plot the target values.</p> <p>Parameters:</p> <ul> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>A figure showing the target values.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/target_values.py</code> <pre><code>def plot(self, show: bool = True, file_path: str | Path = \"\") -&gt; Figure:\n    \"\"\"Plot the target values.\n\n    Args:\n        show: Whether to show the plot.\n        file_path: The path where to save the plot.\n            If empty, the plot is not saved.\n\n    Returns:\n        A figure showing the target values.\n    \"\"\"\n    targets_number = len(self)\n    fig = plt.figure()\n    axes = fig.add_subplot(1, 1, 1)\n    axes.set_title(\"Target values\")\n    plt.xlabel(\"Target index\")\n    plt.xlim([0, targets_number + 1])\n    plt.xticks(linspace(1, targets_number, dtype=int))\n    plt.ylabel(\"Target value\")\n    indexes, history_items = self.get_plot_data()\n\n    # Plot the feasible target values\n    performance_measures = [item.performance_measure for item in history_items]\n    is_feasible = array([item.is_feasible for item in history_items])\n    if is_feasible.any():\n        axes.plot(\n            array(indexes)[is_feasible],\n            array(performance_measures)[is_feasible],\n            color=\"black\",\n            marker=\"o\",\n            linestyle=\"\",\n            label=\"feasible\",\n        )\n\n    # Plot the infeasible target values\n    is_infeasible = logical_not(is_feasible)\n    if is_infeasible.any():\n        axes.plot(\n            array(indexes)[is_infeasible],\n            array(performance_measures)[is_infeasible],\n            color=\"red\",\n            marker=\"x\",\n            linestyle=\"\",\n            label=\"infeasible\",\n        )\n\n    plt.legend()\n\n    save_show_figure(fig, show, file_path)\n\n    return fig\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.plot_on_axes","title":"plot_on_axes","text":"<pre><code>plot_on_axes(\n    axes: Axes,\n    axhline_settings: Mapping[\n        str, str | int | float\n    ] = MappingProxyType(\n        {\"color\": \"red\", \"linestyle\": \":\"}\n    ),\n    yticklabels_format: str = \".4g\",\n    set_ylabel_settings: Mapping[\n        str, str | int\n    ] = MappingProxyType({\"rotation\": 270, \"labelpad\": 12}),\n) -&gt; None\n</code></pre> <p>Plot target values as horizontal lines.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>axhline_settings</code>               (<code>Mapping[str, str | int | float]</code>, default:                   <code>MappingProxyType({'color': 'red', 'linestyle': ':'})</code> )           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.axhline</code>.</p> </li> <li> <code>yticklabels_format</code>               (<code>str</code>, default:                   <code>'.4g'</code> )           \u2013            <p>The string format for the target values labels.</p> </li> <li> <code>set_ylabel_settings</code>               (<code>Mapping[str, str | int]</code>, default:                   <code>MappingProxyType({'rotation': 270, 'labelpad': 12})</code> )           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.set_ylabel</code>.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/target_values.py</code> <pre><code>def plot_on_axes(\n    self,\n    axes: matplotlib.axes.Axes,\n    axhline_settings: Mapping[str, str | int | float] = MappingProxyType({\n        \"color\": \"red\",\n        \"linestyle\": \":\",\n    }),\n    yticklabels_format: str = \".4g\",\n    set_ylabel_settings: Mapping[str, str | int] = MappingProxyType({\n        \"rotation\": 270,\n        \"labelpad\": 12,\n    }),\n) -&gt; None:\n    \"\"\"Plot target values as horizontal lines.\n\n    Args:\n        axes: The axes of the plot.\n        axhline_settings: Keyword arguments\n            for ``matplotlib.axes.Axes.axhline``.\n        yticklabels_format: The string format for the target values labels.\n        set_ylabel_settings: Keyword arguments\n            for ``matplotlib.axes.Axes.set_ylabel``.\n    \"\"\"\n    twin_axes = axes.twinx()\n    twin_axes.set_yscale(axes.get_yscale())\n    values = [target.performance_measure for target in self if target.is_feasible]\n    for value in values:\n        axes.axhline(value, **axhline_settings)\n\n    twin_axes.set_yticks(values)\n    twin_axes.set_yticklabels([\n        f\"{{value:{yticklabels_format}}}\".format(value=value) for value in values\n    ])\n    twin_axes.set_ylabel(\"Target values\", **set_ylabel_settings)\n    twin_axes.set_ylim(axes.get_ylim())\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.remove_leading_infeasible","title":"remove_leading_infeasible","text":"<pre><code>remove_leading_infeasible() -&gt; PerformanceHistory\n</code></pre> <p>Return the history starting from the first feasible item.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The truncated performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def remove_leading_infeasible(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history starting from the first feasible item.\n\n    Returns:\n        The truncated performance history.\n    \"\"\"\n    first_feasible = len(self)\n    for index, item in enumerate(self):\n        if item.is_feasible:\n            first_feasible = index\n            break\n\n    truncated_history = copy(self)\n    truncated_history.items = self.items[first_feasible:]\n    return truncated_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.shorten","title":"shorten","text":"<pre><code>shorten(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Shorten the performance history to a given size.</p> <p>If the history is shorter than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the shortened performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The shortened performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def shorten(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Shorten the performance history to a given size.\n\n    If the history is shorter than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the shortened performance history.\n\n    Returns:\n        The shortened performance history.\n    \"\"\"\n    history = copy(self)\n    history.items = self.items[:size]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.spread_over_numbers_of_discipline_executions","title":"spread_over_numbers_of_discipline_executions","text":"<pre><code>spread_over_numbers_of_discipline_executions(\n    numbers_of_discipline_executions: Iterable[int],\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistory\n</code></pre> <p>Spread the history over a scale of number discipline executions.</p> <p>Note</p> <p>The scale is assumed to be sorted in increasing order.</p> <p>Parameters:</p> <ul> <li> <code>numbers_of_discipline_executions</code>               (<code>Iterable[int]</code>)           \u2013            <p>An increasing sequence of numbers of discipline executions.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history with has as many items as elements in the scale.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def spread_over_numbers_of_discipline_executions(\n    self,\n    numbers_of_discipline_executions: Iterable[int],\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistory:\n    \"\"\"Spread the history over a scale of number discipline executions.\n\n    !!! note\n\n        The scale is assumed to be sorted in increasing order.\n\n    Args:\n        numbers_of_discipline_executions: An increasing sequence\n            of numbers of discipline executions.\n        number_of_scalar_constraints: The number of scalar constraints.\n\n    Returns:\n        The performance history with has as many items as elements in the scale.\n    \"\"\"\n    return self.__spread_over_scale(\n        numbers_of_discipline_executions,\n        number_of_scalar_constraints,\n        HistoryItem.number_of_discipline_executions,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.spread_over_timeline","title":"spread_over_timeline","text":"<pre><code>spread_over_timeline(\n    timeline: Iterable[timedelta],\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistory\n</code></pre> <p>Spread the history over a timeline of elapsed times.</p> <p>Note</p> <p>The timeline is assumed to be sorted in increasing order.</p> <p>Parameters:</p> <ul> <li> <code>timeline</code>               (<code>Iterable[timedelta]</code>)           \u2013            <p>An increasing sequence of elapsed times.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history with has as many items as elements in the timeline.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def spread_over_timeline(\n    self, timeline: Iterable[timedelta], number_of_scalar_constraints: int\n) -&gt; PerformanceHistory:\n    \"\"\"Spread the history over a timeline of elapsed times.\n\n    !!! note\n\n        The timeline is assumed to be sorted in increasing order.\n\n    Args:\n        timeline: An increasing sequence of elapsed times.\n        number_of_scalar_constraints: The number of scalar constraints.\n\n    Returns:\n        The performance history with has as many items as elements in the timeline.\n    \"\"\"\n    return self.__spread_over_scale(\n        timeline, number_of_scalar_constraints, HistoryItem.elapsed_time\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.switch_performance_measure_sign","title":"switch_performance_measure_sign","text":"<pre><code>switch_performance_measure_sign() -&gt; None\n</code></pre> <p>Switch the sign of the performance measure.</p> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def switch_performance_measure_sign(self) -&gt; None:\n    \"\"\"Switch the sign of the performance measure.\"\"\"\n    for item in self:\n        item.switch_performance_measure_sign()\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.to_file","title":"to_file","text":"<pre><code>to_file(path: str | Path) -&gt; None\n</code></pre> <p>Save the performance history in a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path where to write the file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def to_file(\n    self,\n    path: str | Path,\n) -&gt; None:\n    \"\"\"Save the performance history in a file.\n\n    Args:\n        path: The path where to write the file.\n    \"\"\"\n    data = {}\n    if self.problem_name:\n        data[self.__PROBLEM] = self.problem_name\n\n    if self._number_of_variables is not None:\n        data[self.__NUMBER_OF_VARIABLES] = self._number_of_variables\n\n    if self._objective_name:\n        data[self.__OBJECTIVE_NAME] = self._objective_name\n\n    if self._constraints_names:\n        data[self.__CONSTRAINTS_NAMES] = self._constraints_names\n\n    if self.algorithm_configuration is not None:\n        data[self.__ALGORITHM_CONFIGURATION] = self.algorithm_configuration.to_dict(\n            True\n        )\n\n    if self.doe_size is not None:\n        data[self.__DOE_SIZE] = self.doe_size\n\n    if self.total_time is not None:\n        data[self.__EXECUTION_TIME] = self.total_time\n\n    data[self.__HISTORY_ITEMS] = [item.to_dict() for item in self.items]\n    with Path(path).open(\"w\") as file:\n        json.dump(data, file, indent=2, separators=(\",\", \": \"))\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/","title":"Targets generator","text":""},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator","title":"targets_generator","text":"<p>Generation of targets for a problem to be solved by an iterative algorithm.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator","title":"TargetsGenerator","text":"<pre><code>TargetsGenerator()\n</code></pre> <p>Compute the target values for an objective to minimize.</p> <p>The targets are generated out of algorithms histories considered to be of reference: the median of the reference histories is computed and a uniformly distributed subset (of the required size) of this median history is extracted.</p> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    self.__histories = PerformanceHistories()\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator.add_history","title":"add_history","text":"<pre><code>add_history(\n    performance_measures: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    history: PerformanceHistory | None = None,\n) -&gt; None\n</code></pre> <p>Add a history of objective values.</p> <p>Parameters:</p> <ul> <li> <code>performance_measures</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of performance measures. If <code>None</code>, a performance history must be passed. N.B. the value at index i is assumed to have been obtained with i+1 evaluations.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of infeasibility measures. If <code>None</code> then measures are set to zero in case of feasibility and set to infinity otherwise.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of (boolean) feasibility statuses. If <code>None</code> then feasibility is always assumed.</p> </li> <li> <code>history</code>               (<code>PerformanceHistory | None</code>, default:                   <code>None</code> )           \u2013            <p>A performance history. If <code>None</code>, objective values must be passed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither a performance history nor objective values are passed, or if both are passed.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def add_history(\n    self,\n    performance_measures: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    history: PerformanceHistory | None = None,\n) -&gt; None:\n    \"\"\"Add a history of objective values.\n\n    Args:\n        performance_measures: A history of performance measures.\n            If ``None``, a performance history must be passed.\n            N.B. the value at index i is assumed to have been obtained with i+1\n            evaluations.\n        infeasibility_measures: A history of infeasibility measures.\n            If ``None`` then measures are set to zero in case of feasibility and set\n            to infinity otherwise.\n        feasibility_statuses: A history of (boolean) feasibility statuses.\n            If ``None`` then feasibility is always assumed.\n        history: A performance history.\n            If ``None``, objective values must be passed.\n\n    Raises:\n        ValueError: If neither a performance history nor objective values are\n            passed, or if both are passed.\n    \"\"\"\n    if history is not None:\n        if performance_measures is not None:\n            msg = \"Both a performance history and objective values were passed.\"\n            raise ValueError(msg)\n    elif performance_measures is None:\n        msg = \"Either a performance history or objective values must be passed.\"\n        raise ValueError(msg)\n    else:\n        history = PerformanceHistory(\n            performance_measures, infeasibility_measures, feasibility_statuses\n        )\n    self.__histories.append(history)\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator.compute_target_values","title":"compute_target_values","text":"<pre><code>compute_target_values(\n    targets_number: int,\n    budget_min: int = 1,\n    feasible: bool = True,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    best_target_objective: float | None = None,\n    best_target_tolerance: float = 0.0,\n) -&gt; TargetValues\n</code></pre> <p>Compute the target values for a function from the histories of its values.</p> <p>Parameters:</p> <ul> <li> <code>targets_number</code>               (<code>int</code>)           \u2013            <p>The number of targets to compute.</p> </li> <li> <code>budget_min</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of functions evaluations to be used to define the first target. If argument <code>feasible</code> is set to <code>True</code>, this argument will be disregarded and the evaluation budget defining the easiest target will be the budget of the first item in the histories reaching the best target value.</p> </li> <li> <code>feasible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate only feasible targets.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The file path to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>best_target_objective</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The objective value of the best target value. If <code>None</code>, it will be inferred from the performance histories.</p> </li> <li> <code>best_target_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The relative tolerance for comparison with the best target value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TargetValues</code>           \u2013            <p>The target values of the function.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If feasibility is required but the best target value is not feasible.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def compute_target_values(\n    self,\n    targets_number: int,\n    budget_min: int = 1,\n    feasible: bool = True,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    best_target_objective: float | None = None,\n    best_target_tolerance: float = 0.0,\n) -&gt; TargetValues:\n    \"\"\"Compute the target values for a function from the histories of its values.\n\n    Args:\n        targets_number: The number of targets to compute.\n        budget_min: The number of functions evaluations to be used to define the\n            first target.\n            If argument ``feasible`` is set to ``True``, this argument will be\n            disregarded and the evaluation budget defining the easiest target\n            will be the budget of the first item in the histories reaching the\n            best target value.\n        feasible: Whether to generate only feasible targets.\n        show: Whether to show the plot.\n        file_path: The file path to save the plot.\n            If empty, the plot is not saved.\n        best_target_objective: The objective value of the best target value.\n            If ``None``, it will be inferred from the performance histories.\n        best_target_tolerance: The relative tolerance for comparison with the\n            best target value.\n\n    Returns:\n        The target values of the function.\n\n    Raises:\n        RuntimeError: If feasibility is required but the best target value is not\n            feasible.\n    \"\"\"\n    # Get the performance histories of reference\n    reference_histories, best_target = self.__get_reference_histories(\n        self.__histories, best_target_objective, best_target_tolerance, feasible\n    )\n\n    # Compute the median of the cumulated minimum histories\n    median_history = PerformanceHistories(*reference_histories).compute_median()\n    if feasible:\n        median_history = median_history.remove_leading_infeasible()\n\n    # Truncate the values that stagnate near the best target\n    for index, item in enumerate(median_history):\n        if item &lt;= best_target:\n            median_history = median_history[: index + 1]\n            break\n\n    # Compute a budget scale\n    budget_scale = self.__compute_budget_scale(\n        budget_min, len(median_history), targets_number\n    )\n\n    # Compute the target values\n    target_values = TargetValues()\n    target_values.items = [median_history[item - 1] for item in budget_scale]\n\n    # Plot the target values\n    if show or file_path:\n        target_values.plot(show, file_path)\n\n    return target_values\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator.plot_histories","title":"plot_histories","text":"<pre><code>plot_histories(\n    best_target_value: float | None = None,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; Figure\n</code></pre> <p>Plot the histories used as a basis to compute the target values.</p> <p>Parameters:</p> <ul> <li> <code>best_target_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The best target value to be represented with a horizontal line. If <code>None</code>, no best target value will be plotted.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show the figure.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the figure. If empty, the figure will not be saved.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>The histories figure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def plot_histories(\n    self,\n    best_target_value: float | None = None,\n    show: bool = False,\n    file_path: str | Path = \"\",\n) -&gt; Figure:\n    \"\"\"Plot the histories used as a basis to compute the target values.\n\n    Args:\n        best_target_value: The best target value\n            to be represented with a horizontal line.\n            If ``None``, no best target value will be plotted.\n        show: Whether to show the figure.\n        file_path: The path where to save the figure.\n            If empty, the figure will not be saved.\n\n    Returns:\n        The histories figure.\n    \"\"\"\n    # Set up the figure\n    figure = plt.figure()\n    axes = figure.add_subplot(1, 1, 1)\n    axes.set_title(\"Reference performance histories\")\n    plt.xlabel(\"Number or evaluations\")\n    plt.ylabel(\"Performance value\")\n\n    # Plot the best target value\n    if best_target_value is not None:\n        plt.axhline(y=best_target_value, color=\"r\", linestyle=\"-\")\n\n    # Plot the histories of the cumulated minima\n    maximum_budget = max(len(history) for history in self.__histories)\n    minimum_budget = maximum_budget\n    for history in self.__histories:\n        budgets, items = history.get_plot_data(feasible=True, minimum_history=True)\n        # Update the minimum budget\n        if budgets:  # empty if there is no feasible points\n            minimum_budget = min(budgets[0], minimum_budget)\n\n        axes.plot(\n            budgets,\n            [item.performance_measure for item in items],\n            marker=\"o\",\n            linestyle=\":\",\n        )\n\n    plt.xlim(left=minimum_budget - 1, right=maximum_budget + 1)\n    axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n    save_show_figure(figure, show, file_path)\n    return figure\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/","title":"Problems","text":""},{"location":"reference/gemseo_benchmark/problems/#gemseo_benchmark.problems","title":"problems","text":"<p>Reference problems for benchmarking.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/","title":"Base problem configuration","text":""},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration","title":"base_problem_configuration","text":"<p>The interface for problem configurations.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration","title":"BaseProblemConfiguration","text":"<pre><code>BaseProblemConfiguration(\n    name: str,\n    create_problem: Callable[[], Any],\n    target_values: TargetValues | None,\n    starting_points: InputStartingPointsType,\n    variable_space: DesignSpace,\n    doe_algo_name: str,\n    doe_size: int | None,\n    doe_options: Mapping[str, Any],\n    description: str,\n    optimum: float | None,\n    number_of_scalar_constraints: int,\n)\n</code></pre> <p>Base class for problem configurations.</p> <p>A problem configuration is a problem of reference to be solved by iterative algorithms for comparison purposes. A problem configuration is characterized by the function that evaluates its performance measure (ex: the objective function of an optimization problem, or the residual for a multidisciplinary analysis), its starting points (an algorithm trajectory will start from each of them), and its target values (refer to the target_values module).</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the problem configuration.</p> </li> <li> <code>create_problem</code>               (<code>Callable[[], Any]</code>)           \u2013            <p>A function to create a problem of the configuration.</p> <p>Warning</p> <p>If multiprocessing is intended when executing algorithm configurations on the problem, <code>create_problem</code> has to be pickable.</p> </li> <li> <code>target_values</code>               (<code>TargetValues | None</code>)           \u2013            <p>The target values of the problem configuration.</p> </li> <li> <code>starting_points</code>               (<code>InputStartingPointsType</code>)           \u2013            <p>The starting points of the problem configuration. If empty: if <code>doe_algo_name</code> is not empty then the starting points will be generated as a DOE; otherwise the current value of the optimization problem will be set as the single starting point.</p> </li> <li> <code>variable_space</code>               (<code>DesignSpace</code>)           \u2013            <p>The space of the problem variables.</p> </li> <li> <code>doe_algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the DOE algorithm. If empty and <code>starting_points</code> is empty, the current point of the variable space is set as the only starting point.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>)           \u2013            <p>The number of starting points. If <code>None</code>, this number is set as the problem dimension or 10 if bigger.</p> </li> <li> <code>doe_options</code>               (<code>Mapping[str, Any]</code>)           \u2013            <p>The options of the DOE algorithm.</p> </li> <li> <code>description</code>               (<code>str</code>)           \u2013            <p>The description of the problem configuration (to appear in a benchmarking report).</p> </li> <li> <code>optimum</code>               (<code>float | None</code>)           \u2013            <p>The best feasible performance measure of the problem configuration. If <code>None</code>, it will not be set.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    create_problem: Callable[[], Any],\n    target_values: TargetValues | None,\n    starting_points: InputStartingPointsType,\n    variable_space: DesignSpace,\n    doe_algo_name: str,\n    doe_size: int | None,\n    doe_options: Mapping[str, Any],\n    description: str,\n    optimum: float | None,\n    number_of_scalar_constraints: int,\n) -&gt; None:\n    \"\"\"\n    Args:\n        name: The name of the problem configuration.\n        create_problem: A function to create a problem of the configuration.\n            !!! warning\n                If multiprocessing is intended when executing\n                algorithm configurations on the problem,\n                ``create_problem`` has to be pickable.\n        target_values: The target values of the problem configuration.\n        starting_points: The starting points of the problem configuration.\n            If empty:\n            if ``doe_algo_name`` is not empty\n            then the starting points will be generated as a DOE;\n            otherwise the current value of the optimization problem\n            will be set as the single starting point.\n        variable_space: The space of the problem variables.\n        doe_algo_name: The name of the DOE algorithm.\n            If empty and ``starting_points`` is empty,\n            the current point of the variable space\n            is set as the only starting point.\n        doe_size: The number of starting points.\n            If ``None``,\n            this number is set as the problem dimension or 10 if bigger.\n        doe_options: The options of the DOE algorithm.\n        description: The description of the problem configuration\n            (to appear in a benchmarking report).\n        optimum: The best feasible performance measure of the problem configuration.\n            If ``None``, it will not be set.\n        number_of_scalar_constraints: The number of scalar constraints.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__create_problem = create_problem\n    self.__description = description\n    self.__minimization_target_values = None\n    self.__name = name\n    self.__number_of_scalar_constraints = number_of_scalar_constraints\n    self.__optimum = optimum\n    self.__starting_points = []\n    self.__target_values = None\n    self.__variable_space = variable_space\n\n    if len(starting_points) &gt; 0:\n        self.starting_points = starting_points\n    elif doe_algo_name:\n        self.starting_points = self.__get_starting_points(\n            doe_algo_name, doe_size, doe_options\n        )\n    else:\n        default_starting_point = self._get_default_starting_point()\n        if default_starting_point is None:\n            self.starting_points = []\n        else:\n            self.starting_points = [default_starting_point]\n\n    if target_values is not None:\n        self.target_values = target_values\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.abscissa_data_type","title":"abscissa_data_type  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>abscissa_data_type: type[AbscissaData]\n</code></pre> <p>The type of abscissa axis data.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.create_problem","title":"create_problem  <code>property</code>","text":"<pre><code>create_problem: Callable[[], Any]\n</code></pre> <p>The function to create a problem of the configuration.</p> <p>The return type of this function depends on the type of the underlying |g| object (ex: OptimizationProblem, BaseMDA ).</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.description","title":"description  <code>property</code>","text":"<pre><code>description: str\n</code></pre> <p>The description of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.dimension","title":"dimension  <code>property</code>","text":"<pre><code>dimension: int\n</code></pre> <p>The dimension of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.minimization_target_values","title":"minimization_target_values  <code>property</code>","text":"<pre><code>minimization_target_values: TargetValues\n</code></pre> <p>The target values for the minimization of the performance measure.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.minimize_performance_measure","title":"minimize_performance_measure  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>minimize_performance_measure: bool\n</code></pre> <p>Whether the performance measure of the problem is to be minimized.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.number_of_scalar_constraints","title":"number_of_scalar_constraints  <code>property</code>","text":"<pre><code>number_of_scalar_constraints: int\n</code></pre> <p>The number of scalar constraints.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.optimum","title":"optimum  <code>property</code>","text":"<pre><code>optimum: float\n</code></pre> <p>The best feasible performance measure known for the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.performance_measure_label","title":"performance_measure_label  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>performance_measure_label: str\n</code></pre> <p>The label for the performance measure axis.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.starting_points","title":"starting_points  <code>property</code> <code>writable</code>","text":"<pre><code>starting_points: list[ndarray]\n</code></pre> <p>The starting points for the algorithm configurations.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the starting points are neither passed as a NumPy array nor as an iterable.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the problem has no starting point, or if the starting points are passed as a NumPy array that is not 2-dimensional, or if one of the starting points does not have the same dimension as the problem.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.target_values","title":"target_values  <code>property</code> <code>writable</code>","text":"<pre><code>target_values: TargetValues\n</code></pre> <p>The target values of the problem configuration.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the problem configuration has no target value.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.variable_space","title":"variable_space  <code>property</code>","text":"<pre><code>variable_space: DesignSpace\n</code></pre> <p>The space of the problem variables.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.worker","title":"worker  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>worker: type[BaseWorker]\n</code></pre> <p>The type of benchmarking worker.</p>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.compute_data_profile","title":"compute_data_profile","text":"<pre><code>compute_data_profile(\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Compute the data profiles of given algorithms.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>results</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_iteration_number</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum number of iterations to plot. If <code>0</code>, this value is inferred from the longest history.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def compute_data_profile(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Compute the data profiles of given algorithms.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        results: The paths to the reference histories for each algorithm.\n        show: Whether to display the plot.\n        file_path: The path where to save the plot.\n            If empty, the plot is not saved.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_iteration_number: The maximum number of iterations to plot.\n            If ``0``, this value is inferred from the longest history.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n    \"\"\"\n    data_profile = DataProfile({self.name: self.minimization_target_values})\n    for configuration_name in algos_configurations.names:\n        for history_path in results.get_paths(configuration_name, self.name):\n            history = PerformanceHistory.from_file(history_path)\n            if max_iteration_number:\n                history = history.shorten(max_iteration_number)\n\n            history.apply_infeasibility_tolerance(infeasibility_tolerance)\n            data_profile.add_history(\n                self.name,\n                configuration_name,\n                history.performance_measures,\n                history.infeasibility_measures,\n            )\n\n    data_profile.plot(\n        show=show,\n        file_path=file_path,\n        plot_settings=plot_settings,\n        grid_settings=grid_settings,\n        use_abscissa_log_scale=use_abscissa_log_scale,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.load_starting_point","title":"load_starting_point","text":"<pre><code>load_starting_point(path: Path) -&gt; None\n</code></pre> <p>Load the starting points from a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def load_starting_point(self, path: Path) -&gt; None:\n    \"\"\"Load the starting points from a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    self.starting_points = load(path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/base_problem_configuration/#gemseo_benchmark.problems.base_problem_configuration.BaseProblemConfiguration.save_starting_points","title":"save_starting_points","text":"<pre><code>save_starting_points(path: Path) -&gt; None\n</code></pre> <p>Save the starting points as a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def save_starting_points(self, path: Path) -&gt; None:\n    \"\"\"Save the starting points as a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    save(path, array(self.starting_points))\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/","title":"Mda problem configuration","text":""},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration","title":"mda_problem_configuration","text":"<p>Problem configuration for multidisciplinary analysis.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration","title":"MDAProblemConfiguration","text":"<pre><code>MDAProblemConfiguration(\n    name: str,\n    create_problem: Callable[\n        [AlgorithmConfiguration], MDAProblemType\n    ],\n    variable_space: DesignSpace,\n    target_values: TargetValues | None = None,\n    starting_points: InputStartingPointsType = (),\n    doe_algo_name: str = \"\",\n    doe_size: int | None = None,\n    doe_options: Mapping[\n        str, DriverLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n    description: str = \"No description available.\",\n)\n</code></pre> <p>               Bases: <code>BaseProblemConfiguration</code></p> <p>Problem configuration for multidisciplinary analysis.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the problem configuration.</p> </li> <li> <code>create_problem</code>               (<code>Callable[[AlgorithmConfiguration], MDAProblemType]</code>)           \u2013            <p>A function to create a problem of the configuration.</p> <p>Warning</p> <p>If multiprocessing is intended when executing algorithm configurations on the problem, <code>create_problem</code> has to be pickable.</p> </li> <li> <code>variable_space</code>               (<code>DesignSpace</code>)           \u2013            <p>The space of the problem variables.</p> </li> <li> <code>target_values</code>               (<code>TargetValues | None</code>, default:                   <code>None</code> )           \u2013            <p>The target values of the problem configuration.</p> </li> <li> <code>starting_points</code>               (<code>InputStartingPointsType</code>, default:                   <code>()</code> )           \u2013            <p>The starting points of the problem configuration. If empty: if <code>doe_algo_name</code> is not empty then the starting points will be generated as a DOE; otherwise the current value of the optimization problem will be set as the single starting point.</p> </li> <li> <code>doe_algo_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the DOE algorithm. If empty and <code>starting_points</code> is empty, the current point of the variable space is set as the only starting point.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of starting points. If <code>None</code>, this number is set as the problem dimension or 10 if bigger.</p> </li> <li> <code>doe_options</code>               (<code>Mapping[str, DriverLibraryOptionType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the DOE algorithm.</p> </li> <li> <code>description</code>               (<code>str</code>, default:                   <code>'No description available.'</code> )           \u2013            <p>The description of the problem configuration (to appear in a benchmarking report).</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/mda_problem_configuration.py</code> <pre><code>def __init__(  # noqa: D107\n    self,\n    name: str,\n    create_problem: Callable[[AlgorithmConfiguration], MDAProblemType],\n    variable_space: DesignSpace,\n    target_values: TargetValues | None = None,\n    starting_points: InputStartingPointsType = (),\n    doe_algo_name: str = \"\",\n    doe_size: int | None = None,\n    doe_options: Mapping[str, DriverLibraryOptionType] = READ_ONLY_EMPTY_DICT,\n    description: str = \"No description available.\",\n) -&gt; None:\n    super().__init__(\n        name,\n        create_problem,\n        target_values,\n        starting_points,\n        variable_space,\n        doe_algo_name,\n        doe_size,\n        doe_options,\n        description,\n        0,\n        0,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.abscissa_data_type","title":"abscissa_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>abscissa_data_type: Final[type[DisciplineData]] = (\n    DisciplineData\n)\n</code></pre> <p>The type of abscissa axis data.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.create_problem","title":"create_problem  <code>property</code>","text":"<pre><code>create_problem: Callable[[], Any]\n</code></pre> <p>The function to create a problem of the configuration.</p> <p>The return type of this function depends on the type of the underlying |g| object (ex: OptimizationProblem, BaseMDA ).</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.description","title":"description  <code>property</code>","text":"<pre><code>description: str\n</code></pre> <p>The description of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.dimension","title":"dimension  <code>property</code>","text":"<pre><code>dimension: int\n</code></pre> <p>The dimension of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.minimization_target_values","title":"minimization_target_values  <code>property</code>","text":"<pre><code>minimization_target_values: TargetValues\n</code></pre> <p>The target values for the minimization of the performance measure.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.minimize_performance_measure","title":"minimize_performance_measure  <code>property</code>","text":"<pre><code>minimize_performance_measure: bool\n</code></pre> <p>Whether the performance measure is to be minimized.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.number_of_scalar_constraints","title":"number_of_scalar_constraints  <code>property</code>","text":"<pre><code>number_of_scalar_constraints: int\n</code></pre> <p>The number of scalar constraints.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.optimum","title":"optimum  <code>property</code>","text":"<pre><code>optimum: float\n</code></pre> <p>The best feasible performance measure known for the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.performance_measure_label","title":"performance_measure_label  <code>class-attribute</code>","text":"<pre><code>performance_measure_label: str = 'Best residual norm'\n</code></pre> <p>The label for the performance measure axis.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.starting_points","title":"starting_points  <code>property</code> <code>writable</code>","text":"<pre><code>starting_points: list[ndarray]\n</code></pre> <p>The starting points for the algorithm configurations.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the starting points are neither passed as a NumPy array nor as an iterable.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the problem has no starting point, or if the starting points are passed as a NumPy array that is not 2-dimensional, or if one of the starting points does not have the same dimension as the problem.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.target_values","title":"target_values  <code>property</code> <code>writable</code>","text":"<pre><code>target_values: TargetValues\n</code></pre> <p>The target values of the problem configuration.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the problem configuration has no target value.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.variable_space","title":"variable_space  <code>property</code>","text":"<pre><code>variable_space: DesignSpace\n</code></pre> <p>The space of the problem variables.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.worker","title":"worker  <code>class-attribute</code>","text":"<pre><code>worker: type[MDAWorker] = MDAWorker\n</code></pre> <p>The type of benchmarking worker.</p>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.compute_data_profile","title":"compute_data_profile","text":"<pre><code>compute_data_profile(\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Compute the data profiles of given algorithms.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>results</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_iteration_number</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum number of iterations to plot. If <code>0</code>, this value is inferred from the longest history.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def compute_data_profile(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Compute the data profiles of given algorithms.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        results: The paths to the reference histories for each algorithm.\n        show: Whether to display the plot.\n        file_path: The path where to save the plot.\n            If empty, the plot is not saved.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_iteration_number: The maximum number of iterations to plot.\n            If ``0``, this value is inferred from the longest history.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n    \"\"\"\n    data_profile = DataProfile({self.name: self.minimization_target_values})\n    for configuration_name in algos_configurations.names:\n        for history_path in results.get_paths(configuration_name, self.name):\n            history = PerformanceHistory.from_file(history_path)\n            if max_iteration_number:\n                history = history.shorten(max_iteration_number)\n\n            history.apply_infeasibility_tolerance(infeasibility_tolerance)\n            data_profile.add_history(\n                self.name,\n                configuration_name,\n                history.performance_measures,\n                history.infeasibility_measures,\n            )\n\n    data_profile.plot(\n        show=show,\n        file_path=file_path,\n        plot_settings=plot_settings,\n        grid_settings=grid_settings,\n        use_abscissa_log_scale=use_abscissa_log_scale,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.load_starting_point","title":"load_starting_point","text":"<pre><code>load_starting_point(path: Path) -&gt; None\n</code></pre> <p>Load the starting points from a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def load_starting_point(self, path: Path) -&gt; None:\n    \"\"\"Load the starting points from a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    self.starting_points = load(path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mda_problem_configuration/#gemseo_benchmark.problems.mda_problem_configuration.MDAProblemConfiguration.save_starting_points","title":"save_starting_points","text":"<pre><code>save_starting_points(path: Path) -&gt; None\n</code></pre> <p>Save the starting points as a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def save_starting_points(self, path: Path) -&gt; None:\n    \"\"\"Save the starting points as a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    save(path, array(self.starting_points))\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/","title":"Mdo problem configuration","text":""},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration","title":"mdo_problem_configuration","text":"<p>Problem configuration for multidisciplinary optimization.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration","title":"MDOProblemConfiguration","text":"<pre><code>MDOProblemConfiguration(\n    name: str,\n    create_problem: Callable[\n        [AlgorithmConfiguration], MDOProblemType\n    ],\n    variable_space: DesignSpace,\n    minimize_objective_value: bool,\n    number_of_scalar_constraints: int,\n    target_values: TargetValues | None = None,\n    starting_points: InputStartingPointsType = (),\n    doe_algo_name: str = \"\",\n    doe_size: int | None = None,\n    doe_options: Mapping[\n        str, DriverLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n    description: str = \"No description available.\",\n    optimum: float | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseProblemConfiguration</code></p> <p>Problem configuration for multidisciplinary optimization.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the problem configuration.</p> </li> <li> <code>create_problem</code>               (<code>Callable[[AlgorithmConfiguration], MDOProblemType]</code>)           \u2013            <p>A function to create a problem of the configuration.</p> <p>Warning</p> <p>If multiprocessing is intended when executing algorithm configurations on the problem, <code>create_problem</code> has to be pickable.</p> </li> <li> <code>variable_space</code>               (<code>DesignSpace</code>)           \u2013            <p>The space of the problem variables.</p> </li> <li> <code>minimize_objective_value</code>               (<code>bool</code>)           \u2013            <p>Whether the objective function of the scenario is to be minimized.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints.</p> </li> <li> <code>target_values</code>               (<code>TargetValues | None</code>, default:                   <code>None</code> )           \u2013            <p>The target values of the problem configuration.</p> </li> <li> <code>starting_points</code>               (<code>InputStartingPointsType</code>, default:                   <code>()</code> )           \u2013            <p>The starting points of the problem configuration. If empty: if <code>doe_algo_name</code> is not empty then the starting points will be generated as a DOE; otherwise the current value of the optimization problem will be set as the single starting point.</p> </li> <li> <code>doe_algo_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the DOE algorithm. If empty and <code>starting_points</code> is empty, the current point of the variable space is set as the only starting point.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of starting points. If <code>None</code>, this number is set as the problem dimension or 10 if bigger.</p> </li> <li> <code>doe_options</code>               (<code>Mapping[str, DriverLibraryOptionType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the DOE algorithm.</p> </li> <li> <code>description</code>               (<code>str</code>, default:                   <code>'No description available.'</code> )           \u2013            <p>The description of the problem configuration (to appear in a benchmarking report).</p> </li> <li> <code>optimum</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The best feasible performance measure of the problem configuration. If <code>None</code>, it will not be set.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/mdo_problem_configuration.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    create_problem: Callable[[AlgorithmConfiguration], MDOProblemType],\n    variable_space: DesignSpace,\n    minimize_objective_value: bool,\n    number_of_scalar_constraints: int,\n    target_values: TargetValues | None = None,\n    starting_points: InputStartingPointsType = (),\n    doe_algo_name: str = \"\",\n    doe_size: int | None = None,\n    doe_options: Mapping[str, DriverLibraryOptionType] = READ_ONLY_EMPTY_DICT,\n    description: str = \"No description available.\",\n    optimum: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        minimize_objective_value: Whether the objective function of the scenario\n            is to be minimized.\n    \"\"\"  # noqa: D205, D212\n    self.__minimize_objective_value = minimize_objective_value\n    super().__init__(\n        name,\n        create_problem,\n        target_values,\n        starting_points,\n        variable_space,\n        doe_algo_name,\n        doe_size,\n        doe_options,\n        description,\n        optimum,\n        number_of_scalar_constraints,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.abscissa_data_type","title":"abscissa_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>abscissa_data_type: Final[type[DisciplineData]] = (\n    DisciplineData\n)\n</code></pre> <p>The type of abscissa axis data.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.create_problem","title":"create_problem  <code>property</code>","text":"<pre><code>create_problem: Callable[[], Any]\n</code></pre> <p>The function to create a problem of the configuration.</p> <p>The return type of this function depends on the type of the underlying |g| object (ex: OptimizationProblem, BaseMDA ).</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.description","title":"description  <code>property</code>","text":"<pre><code>description: str\n</code></pre> <p>The description of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.dimension","title":"dimension  <code>property</code>","text":"<pre><code>dimension: int\n</code></pre> <p>The dimension of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.minimization_target_values","title":"minimization_target_values  <code>property</code>","text":"<pre><code>minimization_target_values: TargetValues\n</code></pre> <p>The target values for the minimization of the performance measure.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.minimize_performance_measure","title":"minimize_performance_measure  <code>property</code>","text":"<pre><code>minimize_performance_measure: bool\n</code></pre> <p>Whether the performance measure is to be minimized.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.number_of_scalar_constraints","title":"number_of_scalar_constraints  <code>property</code>","text":"<pre><code>number_of_scalar_constraints: int\n</code></pre> <p>The number of scalar constraints.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.optimum","title":"optimum  <code>property</code>","text":"<pre><code>optimum: float\n</code></pre> <p>The best feasible performance measure known for the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.performance_measure_label","title":"performance_measure_label  <code>class-attribute</code>","text":"<pre><code>performance_measure_label: str = (\n    \"Best feasible objective value\"\n)\n</code></pre> <p>The label for the performance measure axis.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.starting_points","title":"starting_points  <code>property</code> <code>writable</code>","text":"<pre><code>starting_points: list[ndarray]\n</code></pre> <p>The starting points for the algorithm configurations.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the starting points are neither passed as a NumPy array nor as an iterable.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the problem has no starting point, or if the starting points are passed as a NumPy array that is not 2-dimensional, or if one of the starting points does not have the same dimension as the problem.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.target_values","title":"target_values  <code>property</code> <code>writable</code>","text":"<pre><code>target_values: TargetValues\n</code></pre> <p>The target values of the problem configuration.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the problem configuration has no target value.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.variable_space","title":"variable_space  <code>property</code>","text":"<pre><code>variable_space: DesignSpace\n</code></pre> <p>The space of the problem variables.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.worker","title":"worker  <code>class-attribute</code>","text":"<pre><code>worker: type[MDOWorker] = MDOWorker\n</code></pre> <p>The type of benchmarking worker.</p>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.compute_data_profile","title":"compute_data_profile","text":"<pre><code>compute_data_profile(\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Compute the data profiles of given algorithms.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>results</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_iteration_number</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum number of iterations to plot. If <code>0</code>, this value is inferred from the longest history.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def compute_data_profile(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Compute the data profiles of given algorithms.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        results: The paths to the reference histories for each algorithm.\n        show: Whether to display the plot.\n        file_path: The path where to save the plot.\n            If empty, the plot is not saved.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_iteration_number: The maximum number of iterations to plot.\n            If ``0``, this value is inferred from the longest history.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n    \"\"\"\n    data_profile = DataProfile({self.name: self.minimization_target_values})\n    for configuration_name in algos_configurations.names:\n        for history_path in results.get_paths(configuration_name, self.name):\n            history = PerformanceHistory.from_file(history_path)\n            if max_iteration_number:\n                history = history.shorten(max_iteration_number)\n\n            history.apply_infeasibility_tolerance(infeasibility_tolerance)\n            data_profile.add_history(\n                self.name,\n                configuration_name,\n                history.performance_measures,\n                history.infeasibility_measures,\n            )\n\n    data_profile.plot(\n        show=show,\n        file_path=file_path,\n        plot_settings=plot_settings,\n        grid_settings=grid_settings,\n        use_abscissa_log_scale=use_abscissa_log_scale,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.load_starting_point","title":"load_starting_point","text":"<pre><code>load_starting_point(path: Path) -&gt; None\n</code></pre> <p>Load the starting points from a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def load_starting_point(self, path: Path) -&gt; None:\n    \"\"\"Load the starting points from a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    self.starting_points = load(path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/mdo_problem_configuration/#gemseo_benchmark.problems.mdo_problem_configuration.MDOProblemConfiguration.save_starting_points","title":"save_starting_points","text":"<pre><code>save_starting_points(path: Path) -&gt; None\n</code></pre> <p>Save the starting points as a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def save_starting_points(self, path: Path) -&gt; None:\n    \"\"\"Save the starting points as a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    save(path, array(self.starting_points))\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/","title":"Optimization problem configuration","text":""},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration","title":"optimization_problem_configuration","text":"<p>Problem configuration for optimization.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration","title":"OptimizationProblemConfiguration","text":"<pre><code>OptimizationProblemConfiguration(\n    name: str,\n    create_problem: Callable[[], OptimizationProblem],\n    starting_points: InputStartingPointsType = (),\n    target_values: TargetValues | None = None,\n    doe_algo_name: str = \"\",\n    doe_size: int | None = None,\n    doe_options: Mapping[\n        str, DriverLibraryOptionType\n    ] = READ_ONLY_EMPTY_DICT,\n    description: str = \"No description available.\",\n    target_values_algorithms_configurations: (\n        AlgorithmsConfigurations | None\n    ) = None,\n    target_values_number: int | None = None,\n    optimum: float | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseProblemConfiguration</code></p> <p>Problem configuration for optimization.</p> <p>An optimization problem configuration is a problem of reference to be solved by optimization algorithms for comparison purposes. An optimization problem configuration is characterized by its objective and constraint functions, its starting points (the optimization trajectories will start from each of them), and its target values (refer to :mod:<code>.data_profiles.target_values</code>).</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the problem configuration.</p> </li> <li> <code>create_problem</code>               (<code>Callable[[], OptimizationProblem]</code>)           \u2013            <p>A function to create a problem of the configuration.</p> <p>Warning</p> <p>If multiprocessing is intended when executing algorithm configurations on the problem, <code>create_problem</code> has to be pickable.</p> </li> <li> <code>starting_points</code>               (<code>InputStartingPointsType</code>, default:                   <code>()</code> )           \u2013            <p>The starting points of the problem configuration. If empty: if <code>doe_algo_name</code> is not empty then the starting points will be generated as a DOE; otherwise the current value of the optimization problem will be set as the single starting point.</p> </li> <li> <code>target_values</code>               (<code>TargetValues | None</code>, default:                   <code>None</code> )           \u2013            <p>The target values of the problem configuration.</p> </li> <li> <code>doe_algo_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the DOE algorithm. If empty and <code>starting_points</code> is empty, the current point of the variable space is set as the only starting point.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of starting points. If <code>None</code>, this number is set as the problem dimension or 10 if bigger.</p> </li> <li> <code>doe_options</code>               (<code>Mapping[str, DriverLibraryOptionType]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the DOE algorithm.</p> </li> <li> <code>description</code>               (<code>str</code>, default:                   <code>'No description available.'</code> )           \u2013            <p>The description of the problem configuration (to appear in a benchmarking report).</p> </li> <li> <code>target_values_algorithms_configurations</code>               (<code>AlgorithmsConfigurations | None</code>, default:                   <code>None</code> )           \u2013            <p>The configurations of the optimization algorithms for the computation of target values. If <code>None</code>, the target values will not be computed.</p> </li> <li> <code>target_values_number</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of target values to compute. If <code>None</code>, the target values will not be computed. N.B. the number of target values shall be the same for all the problem configurations of a same group.</p> </li> <li> <code>optimum</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The best feasible performance measure of the problem configuration. If <code>None</code>, it will not be set.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/optimization_problem_configuration.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    create_problem: Callable[[], OptimizationProblem],\n    starting_points: InputStartingPointsType = (),\n    target_values: TargetValues | None = None,\n    doe_algo_name: str = \"\",\n    doe_size: int | None = None,\n    doe_options: Mapping[str, DriverLibraryOptionType] = READ_ONLY_EMPTY_DICT,\n    description: str = \"No description available.\",\n    target_values_algorithms_configurations: AlgorithmsConfigurations | None = None,\n    target_values_number: int | None = None,\n    optimum: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        target_values_algorithms_configurations: The configurations of the\n            optimization algorithms for the computation of target values.\n            If ``None``, the target values will not be computed.\n        target_values_number: The number of target values to compute.\n            If ``None``, the target values will not be computed.\n            N.B. the number of target values shall be the same for all the\n            problem configurations of a same group.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__optimization_problem = create_problem()\n    super().__init__(\n        name,\n        create_problem,\n        target_values,\n        starting_points,\n        self.__optimization_problem.design_space,\n        doe_algo_name,\n        doe_size,\n        doe_options,\n        description,\n        optimum,\n        len(self.__optimization_problem.scalar_constraint_names),\n    )\n    self.__targets_generator = None\n    if (\n        target_values is None\n        and target_values_algorithms_configurations is not None\n        and target_values_number is not None\n    ):\n        self.compute_target_values(\n            target_values_number, target_values_algorithms_configurations\n        )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.abscissa_data_type","title":"abscissa_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>abscissa_data_type: Final[type[IterationData]] = (\n    IterationData\n)\n</code></pre> <p>The type of abscissa axis data.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.constraints_names","title":"constraints_names  <code>property</code>","text":"<pre><code>constraints_names: list[str]\n</code></pre> <p>The names of the scalar constraints.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.create_problem","title":"create_problem  <code>property</code>","text":"<pre><code>create_problem: Callable[[], Any]\n</code></pre> <p>The function to create a problem of the configuration.</p> <p>The return type of this function depends on the type of the underlying |g| object (ex: OptimizationProblem, BaseMDA ).</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.description","title":"description  <code>property</code>","text":"<pre><code>description: str\n</code></pre> <p>The description of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.dimension","title":"dimension  <code>property</code>","text":"<pre><code>dimension: int\n</code></pre> <p>The dimension of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.minimization_target_values","title":"minimization_target_values  <code>property</code>","text":"<pre><code>minimization_target_values: TargetValues\n</code></pre> <p>The target values for the minimization of the performance measure.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.minimize_performance_measure","title":"minimize_performance_measure  <code>property</code>","text":"<pre><code>minimize_performance_measure: bool\n</code></pre> <p>Whether the performance measure is to be minimized.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.number_of_scalar_constraints","title":"number_of_scalar_constraints  <code>property</code>","text":"<pre><code>number_of_scalar_constraints: int\n</code></pre> <p>The number of scalar constraints.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.objective_name","title":"objective_name  <code>property</code>","text":"<pre><code>objective_name: str\n</code></pre> <p>The name of the objective function.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.optimum","title":"optimum  <code>property</code>","text":"<pre><code>optimum: float\n</code></pre> <p>The best feasible performance measure known for the problem configuration.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.performance_measure_label","title":"performance_measure_label  <code>class-attribute</code>","text":"<pre><code>performance_measure_label: str = (\n    \"Best feasible objective value\"\n)\n</code></pre> <p>The label for the performance measure axis.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.starting_points","title":"starting_points  <code>property</code> <code>writable</code>","text":"<pre><code>starting_points: list[ndarray]\n</code></pre> <p>The starting points for the algorithm configurations.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the starting points are neither passed as a NumPy array nor as an iterable.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the problem has no starting point, or if the starting points are passed as a NumPy array that is not 2-dimensional, or if one of the starting points does not have the same dimension as the problem.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.target_values","title":"target_values  <code>property</code> <code>writable</code>","text":"<pre><code>target_values: TargetValues\n</code></pre> <p>The target values of the problem configuration.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the problem configuration has no target value.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.targets_generator","title":"targets_generator  <code>property</code>","text":"<pre><code>targets_generator: TargetsGenerator\n</code></pre> <p>The generator for target values.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.variable_space","title":"variable_space  <code>property</code>","text":"<pre><code>variable_space: DesignSpace\n</code></pre> <p>The space of the problem variables.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.worker","title":"worker  <code>class-attribute</code>","text":"<pre><code>worker: type[OptimizationWorker] = OptimizationWorker\n</code></pre> <p>The type of benchmarking worker.</p>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.compute_data_profile","title":"compute_data_profile","text":"<pre><code>compute_data_profile(\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Compute the data profiles of given algorithms.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>results</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_iteration_number</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum number of iterations to plot. If <code>0</code>, this value is inferred from the longest history.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def compute_data_profile(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_iteration_number: int = 0,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Compute the data profiles of given algorithms.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        results: The paths to the reference histories for each algorithm.\n        show: Whether to display the plot.\n        file_path: The path where to save the plot.\n            If empty, the plot is not saved.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_iteration_number: The maximum number of iterations to plot.\n            If ``0``, this value is inferred from the longest history.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n    \"\"\"\n    data_profile = DataProfile({self.name: self.minimization_target_values})\n    for configuration_name in algos_configurations.names:\n        for history_path in results.get_paths(configuration_name, self.name):\n            history = PerformanceHistory.from_file(history_path)\n            if max_iteration_number:\n                history = history.shorten(max_iteration_number)\n\n            history.apply_infeasibility_tolerance(infeasibility_tolerance)\n            data_profile.add_history(\n                self.name,\n                configuration_name,\n                history.performance_measures,\n                history.infeasibility_measures,\n            )\n\n    data_profile.plot(\n        show=show,\n        file_path=file_path,\n        plot_settings=plot_settings,\n        grid_settings=grid_settings,\n        use_abscissa_log_scale=use_abscissa_log_scale,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.compute_target_values","title":"compute_target_values","text":"<pre><code>compute_target_values(\n    targets_number: int,\n    algorithm_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n    budget_min: int = 1,\n    show: bool = False,\n    file_path: str = \"\",\n    best_target_tolerance: float = 0.0,\n) -&gt; TargetValues\n</code></pre> <p>Compute target values based on algorithm configurations.</p> <p>Parameters:</p> <ul> <li> <code>targets_number</code>               (<code>int</code>)           \u2013            <p>The number of target values to generate.</p> </li> <li> <code>algorithm_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithm configurations.</p> </li> <li> <code>only_feasible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate only feasible target values.</p> </li> <li> <code>budget_min</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The evaluation budget to be used to define the easiest target value.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show the plot.</p> </li> <li> <code>file_path</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>best_target_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The relative tolerance for comparisons with the best target value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TargetValues</code>           \u2013            <p>The generated target values.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/optimization_problem_configuration.py</code> <pre><code>def compute_target_values(\n    self,\n    targets_number: int,\n    algorithm_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n    budget_min: int = 1,\n    show: bool = False,\n    file_path: str = \"\",\n    best_target_tolerance: float = 0.0,\n) -&gt; TargetValues:\n    \"\"\"Compute target values based on algorithm configurations.\n\n    Args:\n        targets_number: The number of target values to generate.\n        algorithm_configurations: The algorithm configurations.\n        only_feasible: Whether to generate only feasible target values.\n        budget_min: The evaluation budget to be used to define\n            the easiest target value.\n        show: Whether to show the plot.\n        file_path: The path where to save the plot.\n            If empty, the plot is not saved.\n        best_target_tolerance: The relative tolerance for comparisons with the\n            best target value.\n\n    Returns:\n        The generated target values.\n    \"\"\"\n    self.__targets_generator = TargetsGenerator()\n    for configuration in algorithm_configurations:\n        for instance in self:\n            execute_algo(\n                instance,\n                \"opt\",\n                algo_name=configuration.algorithm_name,\n                **configuration.algorithm_options,\n            )\n            history = PerformanceHistory.from_problem(instance)\n            self.__targets_generator.add_history(history=history)\n\n    target_values = self.__targets_generator.compute_target_values(\n        targets_number,\n        budget_min,\n        only_feasible,\n        show,\n        file_path,\n        self.optimum,\n        best_target_tolerance,\n    )\n    if not self.minimize_performance_measure:\n        target_values.switch_performance_measure_sign()\n\n    self.target_values = target_values\n    return self.target_values\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.load_starting_point","title":"load_starting_point","text":"<pre><code>load_starting_point(path: Path) -&gt; None\n</code></pre> <p>Load the starting points from a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def load_starting_point(self, path: Path) -&gt; None:\n    \"\"\"Load the starting points from a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    self.starting_points = load(path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/optimization_problem_configuration/#gemseo_benchmark.problems.optimization_problem_configuration.OptimizationProblemConfiguration.save_starting_points","title":"save_starting_points","text":"<pre><code>save_starting_points(path: Path) -&gt; None\n</code></pre> <p>Save the starting points as a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/base_problem_configuration.py</code> <pre><code>def save_starting_points(self, path: Path) -&gt; None:\n    \"\"\"Save the starting points as a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    save(path, array(self.starting_points))\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problems_group/","title":"Problems group","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group","title":"problems_group","text":"<p>Grouping of problem configurations.</p>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup","title":"ProblemsGroup","text":"<pre><code>ProblemsGroup(\n    name: str,\n    problems: Iterable[BaseProblemConfiguration],\n    description: str = \"\",\n)\n</code></pre> <p>A group of problem configurations.</p> <p>Note</p> <p>Problem configurations should be grouped based on common characteristics such as functions smoothness and constraint set geometry.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the group of problem configurations.</p> </li> <li> <code>problems</code>               (<code>Iterable[BaseProblemConfiguration]</code>)           \u2013            <p>The problem configurations of the group.</p> </li> <li> <code>description</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The description of the group of problem configurations.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problems_group.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    problems: Iterable[BaseProblemConfiguration],\n    description: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Args:\n        name: The name of the group of problem configurations.\n        problems: The problem configurations of the group.\n        description: The description of the group of problem configurations.\n    \"\"\"  # noqa: D205, D212, D415\n    self.name = name\n    self.__problems = problems\n    self.description = description\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str = name\n</code></pre> <p>The name of the group of problem configurations.</p>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup.compute_data_profile","title":"compute_data_profile","text":"<pre><code>compute_data_profile(\n    algos_configurations: AlgorithmsConfigurations,\n    histories_paths: Results,\n    show: bool = True,\n    plot_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int = 0,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Generate the data profiles of given algorithms relative to the problems.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>histories_paths</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, show the plot.</p> </li> <li> <code>plot_path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path where to save the plot. If empty, the plot is not saved.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_eval_number</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum evaluations number to be displayed. If 0, this value is inferred from the longest history.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problems_group.py</code> <pre><code>def compute_data_profile(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    histories_paths: Results,\n    show: bool = True,\n    plot_path: str | Path = \"\",\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int = 0,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_settings: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Generate the data profiles of given algorithms relative to the problems.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        histories_paths: The paths to the reference histories for each algorithm.\n        show: If ``True``, show the plot.\n        plot_path: The path where to save the plot.\n            If empty, the plot is not saved.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_eval_number: The maximum evaluations number to be displayed.\n            If 0, this value is inferred from the longest history.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n    \"\"\"\n    data_profile = DataProfile({\n        problem.name: problem.minimization_target_values\n        for problem in self.__problems\n    })\n\n    for configuration_name in algos_configurations.names:\n        for problem in self.__problems:\n            for history_path in histories_paths.get_paths(\n                configuration_name, problem.name\n            ):\n                history = PerformanceHistory.from_file(history_path)\n                if max_eval_number:\n                    history = history.shorten(max_eval_number)\n                history.apply_infeasibility_tolerance(infeasibility_tolerance)\n                data_profile.add_history(\n                    problem.name,\n                    configuration_name,\n                    history.performance_measures,\n                    history.infeasibility_measures,\n                )\n\n    data_profile.plot(\n        show=show,\n        file_path=plot_path,\n        plot_settings=plot_settings,\n        grid_settings=grid_settings,\n        use_abscissa_log_scale=use_abscissa_log_scale,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup.compute_target_values","title":"compute_target_values","text":"<pre><code>compute_target_values(\n    targets_number: int,\n    algorithm_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n) -&gt; None\n</code></pre> <p>Compute target values based on algorithm configurations.</p> <p>Parameters:</p> <ul> <li> <code>targets_number</code>               (<code>int</code>)           \u2013            <p>The number of target values to generate.</p> </li> <li> <code>algorithm_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithm configurations.</p> </li> <li> <code>only_feasible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate only feasible target values.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problems_group.py</code> <pre><code>def compute_target_values(\n    self,\n    targets_number: int,\n    algorithm_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n) -&gt; None:\n    \"\"\"Compute target values based on algorithm configurations.\n\n    Args:\n        targets_number: The number of target values to generate.\n        algorithm_configurations: The algorithm configurations.\n        only_feasible: Whether to generate only feasible target values.\n    \"\"\"\n    for problem in self.__problems:\n        problem.compute_target_values(\n            targets_number, algorithm_configurations, only_feasible\n        )\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/","title":"Report","text":""},{"location":"reference/gemseo_benchmark/report/#gemseo_benchmark.report","title":"report","text":"<p>Generation of a benchmarking report.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/","title":"Axis data","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data","title":"axis_data","text":"<p>Getting data for a plot axis.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AbscissaData","title":"AbscissaData","text":"<pre><code>AbscissaData(\n    axes: Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n)\n</code></pre> <p>               Bases: <code>AxisData</code></p> <p>The data of an abscissa axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints of the underlying problem.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>)           \u2013            <p>Whether to use a logarithmic scale for the axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(\n    self,\n    axes: matplotlib.axes.Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n) -&gt; None:\n    \"\"\"\n    Args:\n        number_of_scalar_constraints: The number of scalar constraints\n            of the underlying problem.\n        use_log_scale: Whether to use a logarithmic scale for the axis.\n    \"\"\"  # noqa: D205, D212\n    super().__init__(axes)\n    self._axes.set_xlabel(self._label)\n    self._axes.tick_params(axis=\"x\", labelrotation=90)\n    self._number_of_scalar_constraints = number_of_scalar_constraints\n    if use_log_scale:\n        self._axes.set_xscale(matplotlib.scale.LogScale.name)\n    else:\n        self._format_linear_ticks()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AbscissaData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AbscissaData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AbscissaData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AbscissaData.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray | RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray | RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>@abstractmethod\ndef get(\n    self, performance_histories: PerformanceHistories\n) -&gt; IntegerArray | RealArray:\n    \"\"\"Return the axis data associated with performance histories.\n\n    Args:\n        performance_histories: The performance histories.\n\n    Returns:\n        The axis data.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AbscissaData.spread","title":"spread  <code>abstractmethod</code>","text":"<pre><code>spread(\n    performance_histories: PerformanceHistories,\n) -&gt; PerformanceHistories\n</code></pre> <p>Spread histories.</p> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>@abstractmethod\ndef spread(\n    self, performance_histories: PerformanceHistories\n) -&gt; PerformanceHistories:\n    \"\"\"Spread histories.\"\"\"\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AxisData","title":"AxisData","text":"<pre><code>AxisData(axes: Axes)\n</code></pre> <p>The data of a plot axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(self, axes: matplotlib.axes.Axes) -&gt; None:\n    \"\"\"\n    Args:\n        axes: The axes of the plot.\n    \"\"\"  # noqa: D205, D212\n    self._axes = axes\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AxisData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AxisData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AxisData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.AxisData.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray | RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray | RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>@abstractmethod\ndef get(\n    self, performance_histories: PerformanceHistories\n) -&gt; IntegerArray | RealArray:\n    \"\"\"Return the axis data associated with performance histories.\n\n    Args:\n        performance_histories: The performance histories.\n\n    Returns:\n        The axis data.\n    \"\"\"\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.ConstraintData","title":"ConstraintData","text":"<pre><code>ConstraintData(axes: Axes)\n</code></pre> <p>               Bases: <code>OrdinateData</code></p> <p>The constraint unsatisfaction data of an ordinate axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(self, axes: matplotlib.axes.Axes) -&gt; None:  # noqa: D107\n    super().__init__(axes)\n    self._format_linear_integer_ticks()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.ConstraintData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.ConstraintData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.ConstraintData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.ConstraintData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray | RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray | RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; IntegerArray | RealArray:\n    return numpy.array([\n        [self._get_ordinate(history_item) for history_item in performance_history]\n        for performance_history in performance_histories\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.DisciplineData","title":"DisciplineData","text":"<pre><code>DisciplineData(\n    axes: Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n)\n</code></pre> <p>               Bases: <code>AbscissaData</code></p> <p>The discipline data of an abscissa axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints of the underlying problem.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>)           \u2013            <p>Whether to use a logarithmic scale for the axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(\n    self,\n    axes: matplotlib.axes.Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n) -&gt; None:\n    \"\"\"\n    Args:\n        number_of_scalar_constraints: The number of scalar constraints\n            of the underlying problem.\n        use_log_scale: Whether to use a logarithmic scale for the axis.\n    \"\"\"  # noqa: D205, D212\n    super().__init__(axes)\n    self._axes.set_xlabel(self._label)\n    self._axes.tick_params(axis=\"x\", labelrotation=90)\n    self._number_of_scalar_constraints = number_of_scalar_constraints\n    if use_log_scale:\n        self._axes.set_xscale(matplotlib.scale.LogScale.name)\n    else:\n        self._format_linear_ticks()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.DisciplineData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.DisciplineData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.DisciplineData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.DisciplineData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(self, performance_histories: PerformanceHistories) -&gt; IntegerArray:  # noqa: D102\n    return numpy.array(performance_histories.get_numbers_of_discipline_executions())\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.DisciplineData.spread","title":"spread","text":"<pre><code>spread(\n    performance_histories: PerformanceHistories,\n) -&gt; PerformanceHistories\n</code></pre> <p>Spread histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The description is missing.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def spread(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; PerformanceHistories:\n    return performance_histories.spread_over_numbers_of_discipline_executions(\n        self._number_of_scalar_constraints\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.InfeasibilityData","title":"InfeasibilityData","text":"<pre><code>InfeasibilityData(axes: Axes)\n</code></pre> <p>               Bases: <code>OrdinateData</code></p> <p>The infeasibility data of an ordinate axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(self, axes: matplotlib.axes.Axes) -&gt; None:  # noqa:D107\n    super().__init__(axes)\n    self._axes.set_ylabel(self._label)\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.InfeasibilityData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.InfeasibilityData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.InfeasibilityData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.InfeasibilityData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray | RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray | RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; IntegerArray | RealArray:\n    return numpy.array([\n        [self._get_ordinate(history_item) for history_item in performance_history]\n        for performance_history in performance_histories\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.IterationData","title":"IterationData","text":"<pre><code>IterationData(\n    axes: Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n)\n</code></pre> <p>               Bases: <code>AbscissaData</code></p> <p>The iteration data of an abscissa axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints of the underlying problem.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>)           \u2013            <p>Whether to use a logarithmic scale for the axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(\n    self,\n    axes: matplotlib.axes.Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n) -&gt; None:\n    \"\"\"\n    Args:\n        number_of_scalar_constraints: The number of scalar constraints\n            of the underlying problem.\n        use_log_scale: Whether to use a logarithmic scale for the axis.\n    \"\"\"  # noqa: D205, D212\n    super().__init__(axes)\n    self._axes.set_xlabel(self._label)\n    self._axes.tick_params(axis=\"x\", labelrotation=90)\n    self._number_of_scalar_constraints = number_of_scalar_constraints\n    if use_log_scale:\n        self._axes.set_xscale(matplotlib.scale.LogScale.name)\n    else:\n        self._format_linear_ticks()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.IterationData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.IterationData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.IterationData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.IterationData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(self, performance_histories: PerformanceHistories) -&gt; IntegerArray:  # noqa: D102\n    return numpy.arange(1, performance_histories.maximum_size + 1)\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.IterationData.spread","title":"spread","text":"<pre><code>spread(\n    performance_histories: PerformanceHistories,\n) -&gt; PerformanceHistories\n</code></pre> <p>Spread histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The description is missing.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def spread(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; PerformanceHistories:\n    return performance_histories.get_equal_size_histories()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.OrdinateData","title":"OrdinateData","text":"<pre><code>OrdinateData(axes: Axes)\n</code></pre> <p>               Bases: <code>AxisData</code></p> <p>The data of an ordinate axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(self, axes: matplotlib.axes.Axes) -&gt; None:  # noqa:D107\n    super().__init__(axes)\n    self._axes.set_ylabel(self._label)\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.OrdinateData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.OrdinateData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.OrdinateData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.OrdinateData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray | RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray | RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; IntegerArray | RealArray:\n    return numpy.array([\n        [self._get_ordinate(history_item) for history_item in performance_history]\n        for performance_history in performance_histories\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.PerformanceData","title":"PerformanceData","text":"<pre><code>PerformanceData(\n    axes: Axes,\n    infeasible_performance_measure: float,\n    label: str,\n)\n</code></pre> <p>               Bases: <code>OrdinateData</code></p> <p>The performance data of an ordinate axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>infeasible_performance_measure</code>               (<code>float</code>)           \u2013            <p>The performance measure for infeasible history items.</p> </li> <li> <code>label</code>               (<code>str</code>)           \u2013            <p>The description is missing.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(\n    self,\n    axes: matplotlib.axes.Axes,\n    infeasible_performance_measure: float,\n    label: str,\n) -&gt; None:\n    \"\"\"\n    Args:\n        infeasible_performance_measure: The performance measure\n            for infeasible history items.\n    \"\"\"  # noqa: D205, D212\n    self.__label = label\n    super().__init__(axes)\n    self.__infeasible_performance_mesasure = infeasible_performance_measure\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.PerformanceData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.PerformanceData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.PerformanceData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.PerformanceData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray | RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray | RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; IntegerArray | RealArray:\n    return numpy.array([\n        [self._get_ordinate(history_item) for history_item in performance_history]\n        for performance_history in performance_histories\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeAbscissaData","title":"TimeAbscissaData","text":"<pre><code>TimeAbscissaData(\n    axes: Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n)\n</code></pre> <p>               Bases: <code>AbscissaData</code></p> <p>The elapsed time data of an abscissa axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints of the underlying problem.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>)           \u2013            <p>Whether to use a logarithmic scale for the axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(\n    self,\n    axes: matplotlib.axes.Axes,\n    number_of_scalar_constraints: int,\n    use_log_scale: bool,\n) -&gt; None:\n    \"\"\"\n    Args:\n        number_of_scalar_constraints: The number of scalar constraints\n            of the underlying problem.\n        use_log_scale: Whether to use a logarithmic scale for the axis.\n    \"\"\"  # noqa: D205, D212\n    super().__init__(axes)\n    self._axes.set_xlabel(self._label)\n    self._axes.tick_params(axis=\"x\", labelrotation=90)\n    self._number_of_scalar_constraints = number_of_scalar_constraints\n    if use_log_scale:\n        self._axes.set_xscale(matplotlib.scale.LogScale.name)\n    else:\n        self._format_linear_ticks()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeAbscissaData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeAbscissaData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeAbscissaData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeAbscissaData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(self, performance_histories: PerformanceHistories) -&gt; RealArray:  # noqa: D102\n    return numpy.array([\n        time.total_seconds() for time in performance_histories.get_elapsed_times()\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeAbscissaData.spread","title":"spread","text":"<pre><code>spread(\n    performance_histories: PerformanceHistories,\n) -&gt; PerformanceHistories\n</code></pre> <p>Spread histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The description is missing.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def spread(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; PerformanceHistories:\n    return performance_histories.spread_over_time(\n        self._number_of_scalar_constraints\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeOrdinateData","title":"TimeOrdinateData","text":"<pre><code>TimeOrdinateData(axes: Axes)\n</code></pre> <p>               Bases: <code>OrdinateData</code></p> <p>The elapsed time data of an ordinate axis.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def __init__(self, axes: matplotlib.axes.Axes) -&gt; None:  # noqa:D107\n    super().__init__(axes)\n    self._format_linear_time_ticks()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeOrdinateData-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeOrdinateData.time_tick_formatter","title":"time_tick_formatter  <code>class-attribute</code>","text":"<pre><code>time_tick_formatter: FuncFormatter = FuncFormatter(\n    lambda x, _: str(timedelta(seconds=x))\n)\n</code></pre> <p>The formatter for time tick labels.</p>"},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeOrdinateData-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/axis_data/#gemseo_benchmark.report.axis_data.TimeOrdinateData.get","title":"get","text":"<pre><code>get(\n    performance_histories: PerformanceHistories,\n) -&gt; IntegerArray | RealArray\n</code></pre> <p>Return the axis data associated with performance histories.</p> <p>Parameters:</p> <ul> <li> <code>performance_histories</code>               (<code>PerformanceHistories</code>)           \u2013            <p>The performance histories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IntegerArray | RealArray</code>           \u2013            <p>The axis data.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/axis_data.py</code> <pre><code>def get(  # noqa: D102\n    self, performance_histories: PerformanceHistories\n) -&gt; IntegerArray | RealArray:\n    return numpy.array([\n        [self._get_ordinate(history_item) for history_item in performance_history]\n        for performance_history in performance_histories\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/conf/","title":"Conf","text":""},{"location":"reference/gemseo_benchmark/report/conf/#gemseo_benchmark.report.conf","title":"conf","text":"<p>Configuration of the benchmarking report.</p>"},{"location":"reference/gemseo_benchmark/report/range_plot/","title":"Range plot","text":""},{"location":"reference/gemseo_benchmark/report/range_plot/#gemseo_benchmark.report.range_plot","title":"range_plot","text":"<p>Plotting the range of performance history data.</p>"},{"location":"reference/gemseo_benchmark/report/range_plot/#gemseo_benchmark.report.range_plot-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/report/range_plot/#gemseo_benchmark.report.range_plot.RangePlot","title":"RangePlot","text":"<pre><code>RangePlot(\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions],\n    grid_kwargs: Mapping[str, str],\n    alpha: float,\n    matplotlib_log_scale: str,\n)\n</code></pre> <p>               Bases: <code>ReportPlot</code></p> <p>A plot of the range of performance history data.</p> <p>Parameters:</p> <ul> <li> <code>plot_settings</code>           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>alpha</code>               (<code>float</code>)           \u2013            <p>The opacity level for overlapping areas. (Refer to the Matplotlib documentation.)</p> </li> <li> <code>matplotlib_log_scale</code>               (<code>str</code>)           \u2013            <p>The Matplotlib value for logarithmic scale.</p> </li> <li> <code>time_formatter</code>           \u2013            <p>The formatter for time tick labels.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/report_plot.py</code> <pre><code>def __init__(\n    self,\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions],\n    grid_kwargs: Mapping[str, str],\n    alpha: float,\n    matplotlib_log_scale: str,\n) -&gt; None:\n    \"\"\"\n    Args:\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        alpha: The opacity level for overlapping areas.\n            (Refer to the Matplotlib documentation.)\n        matplotlib_log_scale: The Matplotlib value for logarithmic scale.\n        time_formatter: The formatter for time tick labels.\n    \"\"\"  # noqa: D205, D212\n    self._alpha = alpha\n    self._grid_kwargs = grid_kwargs\n    self._matplotlib_log_scale = matplotlib_log_scale\n    self._plot_kwargs = plot_kwargs\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/range_plot/#gemseo_benchmark.report.range_plot.RangePlot-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/range_plot/#gemseo_benchmark.report.range_plot.RangePlot.plot","title":"plot","text":"<pre><code>plot(\n    axes: Axes,\n    performance_histories: Mapping[\n        AlgorithmConfiguration, PerformanceHistories\n    ],\n    ordinate_data: OrdinateData,\n    abscissa_data: AbscissaData,\n    infinity_replacement: float,\n    plot_only_median: bool,\n    plot_all_histories: bool,\n    data_is_minimized: bool,\n    nan_replacement: float,\n    use_ordinate_log_scale: bool,\n) -&gt; None\n</code></pre> <p>Make the plot.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>performance_histories</code>               (<code>Mapping[AlgorithmConfiguration, PerformanceHistories]</code>)           \u2013            <p>The performance histories of algorithm configurations.</p> </li> <li> <code>ordinate_data</code>               (<code>OrdinateData</code>)           \u2013            <p>The data of the ordinate axis.</p> </li> <li> <code>abscissa_data</code>               (<code>AbscissaData</code>)           \u2013            <p>The data of the abscissa axis.</p> </li> <li> <code>infinity_replacement</code>               (<code>float</code>)           \u2013            <p>The finite substitute value for infinite ordinates to enable <code>matplotlib.axes.Axes.fill_between</code>.</p> </li> <li> <code>plot_only_median</code>               (<code>bool</code>)           \u2013            <p>Whether to plot only the median and no other centile.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>)           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>data_is_minimized</code>               (<code>bool</code>)           \u2013            <p>Whether the data is minimized (rather than maximized).</p> </li> <li> <code>nan_replacement</code>               (<code>float</code>)           \u2013            <p>The value to replace NaN history entries to enable the computation of centiles.</p> </li> <li> <code>use_ordinate_log_scale</code>               (<code>bool</code>)           \u2013            <p>Whether to use a logarithmic scale for the ordinate axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/range_plot.py</code> <pre><code>def plot(\n    self,\n    axes: matplotlib.axes.Axes,\n    performance_histories: Mapping[AlgorithmConfiguration, PerformanceHistories],\n    ordinate_data: OrdinateData,\n    abscissa_data: AbscissaData,\n    infinity_replacement: float,\n    plot_only_median: bool,\n    plot_all_histories: bool,\n    data_is_minimized: bool,\n    nan_replacement: float,\n    use_ordinate_log_scale: bool,\n) -&gt; None:\n    \"\"\"Make the plot.\n\n    Args:\n        axes: The axes of the plot.\n        performance_histories: The performance histories\n            of algorithm configurations.\n        ordinate_data: The data of the ordinate axis.\n        abscissa_data: The data of the abscissa axis.\n        infinity_replacement: The finite substitute value for infinite ordinates\n            to enable `matplotlib.axes.Axes.fill_between`.\n        plot_only_median: Whether to plot only the median and no other centile.\n        plot_all_histories: Whether to plot all the performance histories.\n        data_is_minimized: Whether the data is minimized (rather than maximized).\n        nan_replacement: The value to replace NaN history entries\n            to enable the computation of centiles.\n        use_ordinate_log_scale: Whether to use a logarithmic scale\n            for the ordinate axis.\n    \"\"\"\n    data_minimum = float(\"inf\")\n    for algorithm_configuration, histories in performance_histories.items():\n        name = algorithm_configuration.name\n        data = ordinate_data.get(abscissa_data.spread(histories))\n        data_minimum = min(data.min(), data_minimum)\n        abscissas = abscissa_data.get(histories)\n        if plot_all_histories:\n            axes.plot(\n                abscissas,\n                data.T,\n                color=self._plot_kwargs[name][\"color\"],\n                linestyle=self._HISTORY_LINESTYLE,\n                drawstyle=self._DRAWSTYLE,\n            )\n\n        if not plot_only_median:\n            self._plot_centiles_range(\n                axes,\n                abscissas,\n                data,\n                (0, 100),\n                {\n                    \"alpha\": self._alpha,\n                    \"color\": self._plot_kwargs[name][\"color\"],\n                },\n                infinity_replacement,\n                data_is_minimized,\n                nan_replacement,\n            )\n\n        self._plot_median(\n            axes,\n            abscissas,\n            data,\n            dict(drawstyle=self._DRAWSTYLE, **self._plot_kwargs[name]),\n            nan_replacement,\n        )\n\n    axes.set_xbound(abscissas.min(), abscissas.max())\n    axes.set_ymargin(0.01)\n    if use_ordinate_log_scale:\n        scale = (\n            matplotlib.scale.SymmetricalLogScale\n            if data_minimum &lt; 0\n            else matplotlib.scale.LogScale\n        )\n        axes.set_yscale(scale.name)\n\n    axes.grid(**self._grid_kwargs)\n    axes.legend()\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/report/","title":"Report","text":""},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report","title":"report","text":"<p>Generation of a benchmarking report.</p>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.DirectoryName","title":"DirectoryName","text":"<p>               Bases: <code>Enum</code></p> <p>The name of a report directory.</p>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.FileName","title":"FileName","text":"<p>               Bases: <code>Enum</code></p> <p>The name of a report file.</p>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.Report","title":"Report","text":"<pre><code>Report(\n    root_directory_path: str | Path,\n    algos_configurations_groups: Iterable[\n        AlgorithmsConfigurations\n    ],\n    problems_groups: Iterable[ProblemsGroup],\n    histories_paths: Results,\n    custom_algos_descriptions: (\n        Mapping[str, str] | None\n    ) = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_settings: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n)\n</code></pre> <p>A benchmarking report.</p> <p>Parameters:</p> <ul> <li> <code>root_directory_path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the root directory of the report.</p> </li> <li> <code>algos_configurations_groups</code>               (<code>Iterable[AlgorithmsConfigurations]</code>)           \u2013            <p>The groups of algorithms configurations.</p> </li> <li> <code>problems_groups</code>               (<code>Iterable[ProblemsGroup]</code>)           \u2013            <p>The groups of reference problems.</p> </li> <li> <code>histories_paths</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm and reference problem.</p> </li> <li> <code>custom_algos_descriptions</code>               (<code>Mapping[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom descriptions of the algorithms, to be printed in the report instead of the default ones coded in GEMSEO.</p> </li> <li> <code>max_eval_number_per_group</code>               (<code>dict[str, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum evaluations numbers to be displayed on the graphs of each group. The keys are the groups names and the values are the maximum evaluations numbers for the graphs of the group. If <code>None</code>, all the evaluations are displayed. If the key of a group is missing, all the evaluations are displayed for the group.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an algorithm has no associated histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/report.py</code> <pre><code>def __init__(\n    self,\n    root_directory_path: str | Path,\n    algos_configurations_groups: Iterable[AlgorithmsConfigurations],\n    problems_groups: Iterable[ProblemsGroup],\n    histories_paths: Results,\n    custom_algos_descriptions: Mapping[str, str] | None = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_settings: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n) -&gt; None:\n    \"\"\"\n    Args:\n        root_directory_path: The path to the root directory of the report.\n        algos_configurations_groups: The groups of algorithms configurations.\n        problems_groups: The groups of reference problems.\n        histories_paths: The paths to the reference histories for each algorithm\n            and reference problem.\n        custom_algos_descriptions: Custom descriptions of the algorithms,\n            to be printed in the report instead of the default ones coded in GEMSEO.\n        max_eval_number_per_group: The maximum evaluations numbers to be displayed\n            on the graphs of each group.\n            The keys are the groups names and the values are the maximum\n            evaluations numbers for the graphs of the group.\n            If ``None``, all the evaluations are displayed.\n            If the key of a group is missing, all the evaluations are displayed\n            for the group.\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n\n    Raises:\n        ValueError: If an algorithm has no associated histories.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__plot_settings = plot_settings\n    self.__root_directory = Path(root_directory_path)\n    self.__algorithms_configurations_groups = algos_configurations_groups\n    self.__problems_groups = problems_groups\n    self.__histories_paths = histories_paths\n    if custom_algos_descriptions is None:\n        custom_algos_descriptions = {}\n\n    self.__custom_algos_descriptions = custom_algos_descriptions\n    algos_diff = set().union(*[\n        group.names for group in algos_configurations_groups\n    ]) - set(histories_paths.algorithms)\n    if algos_diff:\n        msg = (\n            f\"Missing histories for algorithm{'s' if len(algos_diff) &gt; 1 else ''} \"\n            f\"{', '.join([f'{name!r}' for name in sorted(algos_diff)])}.\"\n        )\n        raise ValueError(msg)\n\n    self.__max_eval_numbers = max_eval_number_per_group or {\n        group.name: None for group in problems_groups\n    }\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.Report-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.Report.generate","title":"generate","text":"<pre><code>generate(\n    to_html: bool = True,\n    to_pdf: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    plot_all_histories: bool = False,\n    use_log_scale: bool = False,\n    plot_only_median: bool = False,\n    use_time_log_scale: bool = False,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Generate the benchmarking report.</p> <p>Parameters:</p> <ul> <li> <code>to_html</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate the report in HTML format.</p> </li> <li> <code>to_pdf</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to generate the report in PDF format.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale on the value axis.</p> </li> <li> <code>plot_only_median</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot only the median and no other centile.</p> </li> <li> <code>use_time_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the time axis.</p> </li> <li> <code>use_abscissa_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the abscissa axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/report.py</code> <pre><code>def generate(\n    self,\n    to_html: bool = True,\n    to_pdf: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    plot_all_histories: bool = False,\n    use_log_scale: bool = False,\n    plot_only_median: bool = False,\n    use_time_log_scale: bool = False,\n    use_abscissa_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Generate the benchmarking report.\n\n    Args:\n        to_html: Whether to generate the report in HTML format.\n        to_pdf: Whether to generate the report in PDF format.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        plot_all_histories: Whether to plot all the performance histories.\n        use_log_scale: Whether to use a logarithmic scale on the value axis.\n        plot_only_median: Whether to plot only the median and no other centile.\n        use_time_log_scale: Whether to use a logarithmic scale\n            for the time axis.\n        use_abscissa_log_scale: Whether to use a logarithmic scale\n            for the abscissa axis.\n    \"\"\"\n    self.__create_root_directory()\n    self.__create_algos_file()\n    self.__create_problems_files()\n    self.__create_results_files(\n        infeasibility_tolerance,\n        plot_all_histories,\n        use_log_scale,\n        plot_only_median,\n        use_time_log_scale,\n        use_abscissa_log_scale,\n    )\n    self.__create_index()\n    self.__build_report(to_html, to_pdf)\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/report_plot/","title":"Report plot","text":""},{"location":"reference/gemseo_benchmark/report/report_plot/#gemseo_benchmark.report.report_plot","title":"report_plot","text":"<p>Plotting performance history data.</p>"},{"location":"reference/gemseo_benchmark/report/report_plot/#gemseo_benchmark.report.report_plot-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/report/report_plot/#gemseo_benchmark.report.report_plot.ReportPlot","title":"ReportPlot","text":"<pre><code>ReportPlot(\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions],\n    grid_kwargs: Mapping[str, str],\n    alpha: float,\n    matplotlib_log_scale: str,\n)\n</code></pre> <p>A plot of performance history data.</p> <p>Parameters:</p> <ul> <li> <code>plot_settings</code>           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_settings</code>           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>alpha</code>               (<code>float</code>)           \u2013            <p>The opacity level for overlapping areas. (Refer to the Matplotlib documentation.)</p> </li> <li> <code>matplotlib_log_scale</code>               (<code>str</code>)           \u2013            <p>The Matplotlib value for logarithmic scale.</p> </li> <li> <code>time_formatter</code>           \u2013            <p>The formatter for time tick labels.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/report_plot.py</code> <pre><code>def __init__(\n    self,\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions],\n    grid_kwargs: Mapping[str, str],\n    alpha: float,\n    matplotlib_log_scale: str,\n) -&gt; None:\n    \"\"\"\n    Args:\n        plot_settings: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_settings: The keyword arguments of `matplotlib.pyplot.grid`.\n        alpha: The opacity level for overlapping areas.\n            (Refer to the Matplotlib documentation.)\n        matplotlib_log_scale: The Matplotlib value for logarithmic scale.\n        time_formatter: The formatter for time tick labels.\n    \"\"\"  # noqa: D205, D212\n    self._alpha = alpha\n    self._grid_kwargs = grid_kwargs\n    self._matplotlib_log_scale = matplotlib_log_scale\n    self._plot_kwargs = plot_kwargs\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/report_plot/#gemseo_benchmark.report.report_plot.ReportPlot-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/report_plot/#gemseo_benchmark.report.report_plot.ReportPlot.plot","title":"plot  <code>abstractmethod</code>","text":"<pre><code>plot() -&gt; None\n</code></pre> <p>Make the plot.</p> Source code in <code>src/gemseo_benchmark/report/report_plot.py</code> <pre><code>@abstractmethod\ndef plot(self) -&gt; None:\n    \"\"\"Make the plot.\"\"\"\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/","title":"Results","text":""},{"location":"reference/gemseo_benchmark/results/#gemseo_benchmark.results","title":"results","text":"<p>Management of paths to performance histories.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/","title":"History item","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item","title":"history_item","text":"<p>A performance history item.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem","title":"HistoryItem","text":"<pre><code>HistoryItem(\n    performance_measure: float,\n    infeasibility_measure: float,\n    n_unsatisfied_constraints: int | None = None,\n    elapsed_time: timedelta = timedelta(),\n    number_of_discipline_executions: int = 0,\n)\n</code></pre> <p>A performance history item.</p> <p>Parameters:</p> <ul> <li> <code>performance_measure</code>               (<code>float</code>)           \u2013            <p>The performance measure of the item.</p> </li> <li> <code>infeasibility_measure</code>               (<code>float</code>)           \u2013            <p>The infeasibility measure of the item.</p> </li> <li> <code>n_unsatisfied_constraints</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of unsatisfied constraints of the item. If <code>None</code>, it will be set to 0 if the infeasibility measure is zero, and if the infeasibility measure is positive it will be set to None.</p> </li> <li> <code>elapsed_time</code>               (<code>timedelta</code>, default:                   <code>timedelta()</code> )           \u2013            <p>The elapsed time of the item.</p> </li> <li> <code>number_of_disicpline_executions</code>           \u2013            <p>The number of discipline executions.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>def __init__(\n    self,\n    performance_measure: float,\n    infeasibility_measure: float,\n    n_unsatisfied_constraints: int | None = None,\n    elapsed_time: datetime.timedelta = datetime.timedelta(),\n    number_of_discipline_executions: int = 0,\n) -&gt; None:\n    \"\"\"\n    Args:\n        performance_measure: The performance measure of the item.\n        infeasibility_measure: The infeasibility measure of the item.\n        n_unsatisfied_constraints: The number of unsatisfied constraints of the\n            item.\n            If ``None``, it will be set to 0 if the infeasibility measure is zero,\n            and if the infeasibility measure is positive it will be set to None.\n        elapsed_time: The elapsed time of the item.\n        number_of_disicpline_executions: The number of discipline executions.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__elapsed_time = elapsed_time\n    (\n        self.__infeasibility_measure,\n        self.__n_unsatisfied_constraints,\n    ) = self.__get_infeasibility(infeasibility_measure, n_unsatisfied_constraints)\n    self.__number_of_discipline_executions = number_of_discipline_executions\n    self.__performance_measure = performance_measure\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.elapsed_time","title":"elapsed_time  <code>property</code> <code>writable</code>","text":"<pre><code>elapsed_time: timedelta\n</code></pre> <p>The elapsed time.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.infeasibility_measure","title":"infeasibility_measure  <code>property</code>","text":"<pre><code>infeasibility_measure: float\n</code></pre> <p>The infeasibility measure of the history item.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the infeasibility measure is negative.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.is_feasible","title":"is_feasible  <code>property</code>","text":"<pre><code>is_feasible: bool\n</code></pre> <p>Whether the history item is feasible.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.n_unsatisfied_constraints","title":"n_unsatisfied_constraints  <code>property</code>","text":"<pre><code>n_unsatisfied_constraints: int | None\n</code></pre> <p>The number of unsatisfied constraints.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.number_of_discipline_executions","title":"number_of_discipline_executions  <code>property</code> <code>writable</code>","text":"<pre><code>number_of_discipline_executions: int\n</code></pre> <p>The number of discipline executions.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.performance_measure","title":"performance_measure  <code>property</code>","text":"<pre><code>performance_measure: float\n</code></pre> <p>The performance measure of the history item.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.apply_infeasibility_tolerance","title":"apply_infeasibility_tolerance","text":"<pre><code>apply_infeasibility_tolerance(\n    infeasibility_tolerance: float,\n) -&gt; None\n</code></pre> <p>Apply a tolerance on the infeasibility measure.</p> <p>Mark the history item as feasible if its infeasibility measure is below the tolerance.</p> <p>Parameters:</p> <ul> <li> <code>infeasibility_tolerance</code>               (<code>float</code>)           \u2013            <p>the tolerance on the infeasibility measure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>def apply_infeasibility_tolerance(self, infeasibility_tolerance: float) -&gt; None:\n    \"\"\"Apply a tolerance on the infeasibility measure.\n\n    Mark the history item as feasible if its infeasibility measure is below the\n    tolerance.\n\n    Args:\n        infeasibility_tolerance: the tolerance on the infeasibility measure.\n    \"\"\"\n    if self.__infeasibility_measure &lt;= infeasibility_tolerance:\n        self.__infeasibility_measure = 0.0\n        self.__n_unsatisfied_constraints = 0\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.copy","title":"copy","text":"<pre><code>copy() -&gt; HistoryItem\n</code></pre> <p>Return a deep copy of the history item.</p> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>def copy(self) -&gt; HistoryItem:\n    \"\"\"Return a deep copy of the history item.\"\"\"\n    return self.__class__(\n        self.__performance_measure,\n        self.__infeasibility_measure,\n        self.__n_unsatisfied_constraints,\n        self.__elapsed_time,\n        self.__number_of_discipline_executions,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: HistoryItemDict) -&gt; HistoryItem\n</code></pre> <p>Create a history item from a dictionary.</p> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: HistoryItemDict) -&gt; HistoryItem:\n    \"\"\"Create a history item from a dictionary.\"\"\"\n    return HistoryItem(\n        data[cls.__PERFORMANCE_MEASURE],\n        data[cls.__INFEASIBILITY_MEASURE],\n        data.get(cls.__N_UNSATISFIED_CONSTRAINTS),\n        datetime.timedelta(seconds=data.get(cls.__ELAPSED_TIME, 0)),\n        data.get(cls.__N_DISCIPLINE_EXECUTIONS),\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.switch_performance_measure_sign","title":"switch_performance_measure_sign","text":"<pre><code>switch_performance_measure_sign() -&gt; None\n</code></pre> <p>Switch the sign of the performance measure.</p> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>def switch_performance_measure_sign(self) -&gt; None:\n    \"\"\"Switch the sign of the performance measure.\"\"\"\n    self.__performance_measure = -self.__performance_measure\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; HistoryItemDict\n</code></pre> <p>Return the history item as dictionary.</p> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>def to_dict(self) -&gt; HistoryItemDict:\n    \"\"\"Return the history item as dictionary.\"\"\"\n    data = {\n        self.__PERFORMANCE_MEASURE: self.__performance_measure,\n        self.__INFEASIBILITY_MEASURE: self.__infeasibility_measure,\n    }\n    if self.n_unsatisfied_constraints is not None:\n        # N.B. type int64 is not JSON serializable\n        data[self.__N_UNSATISFIED_CONSTRAINTS] = int(\n            self.__n_unsatisfied_constraints\n        )\n\n    data[self.__ELAPSED_TIME] = self.__elapsed_time.total_seconds()\n    data[self.__N_DISCIPLINE_EXECUTIONS] = self.__number_of_discipline_executions\n    return data\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/","title":"Performance histories","text":""},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories","title":"performance_histories","text":"<p>A class to implement a collection of performance histories.</p>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories","title":"PerformanceHistories","text":"<pre><code>PerformanceHistories(*histories: PerformanceHistory)\n</code></pre> <p>               Bases: <code>MutableSequence</code></p> <p>A collection of performance histories.</p> <p>Parameters:</p> <ul> <li> <code>*histories</code>               (<code>PerformanceHistory</code>, default:                   <code>()</code> )           \u2013            <p>The performance histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def __init__(self, *histories: PerformanceHistory) -&gt; None:\n    \"\"\"\n    Args:\n        *histories: The performance histories.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__histories = list(histories)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.maximum_size","title":"maximum_size  <code>property</code>","text":"<pre><code>maximum_size: int\n</code></pre> <p>The maximum size of a history.</p>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.compute_maximum","title":"compute_maximum","text":"<pre><code>compute_maximum() -&gt; PerformanceHistory\n</code></pre> <p>Return the itemwise maximum history of the collection.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The itemwise maximum history of the collection.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def compute_maximum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the itemwise maximum history of the collection.\n\n    Returns:\n        The itemwise maximum history of the collection.\n    \"\"\"\n    return self.__compute_itemwise_statistic(max)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.compute_median","title":"compute_median","text":"<pre><code>compute_median(\n    compute_low_median: bool = True,\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the itemwise median history of the collection.</p> <p>Parameters:</p> <ul> <li> <code>compute_low_median</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to compute the low median (rather than the high median).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The itemwise median history of the collection.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def compute_median(self, compute_low_median: bool = True) -&gt; PerformanceHistory:\n    \"\"\"Return the itemwise median history of the collection.\n\n    Args:\n        compute_low_median: Whether to compute the low median\n            (rather than the high median).\n\n    Returns:\n        The itemwise median history of the collection.\n    \"\"\"\n    if compute_low_median:\n        return self.__compute_itemwise_statistic(statistics.median_low)\n\n    return self.__compute_itemwise_statistic(statistics.median_high)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.compute_minimum","title":"compute_minimum","text":"<pre><code>compute_minimum() -&gt; PerformanceHistory\n</code></pre> <p>Return the itemwise minimum history of the collection.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The itemwise minimum history of the collection.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def compute_minimum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the itemwise minimum history of the collection.\n\n    Returns:\n        The itemwise minimum history of the collection.\n    \"\"\"\n    return self.__compute_itemwise_statistic(min)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.cumulate_minimum","title":"cumulate_minimum","text":"<pre><code>cumulate_minimum() -&gt; PerformanceHistories\n</code></pre> <p>Return the histories of the minimum.</p> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def cumulate_minimum(self) -&gt; PerformanceHistories:\n    \"\"\"Return the histories of the minimum.\"\"\"\n    return PerformanceHistories(*[\n        history.compute_cumulated_minimum() for history in self\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.get_elapsed_times","title":"get_elapsed_times","text":"<pre><code>get_elapsed_times() -&gt; list[timedelta]\n</code></pre> <p>Return the sorted elapsed times of all the history items.</p> <p>Returns:</p> <ul> <li> <code>list[timedelta]</code>           \u2013            <p>The sorted elapsed times.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def get_elapsed_times(self) -&gt; list[datetime.timedelta]:\n    \"\"\"Return the sorted elapsed times of all the history items.\n\n    Returns:\n        The sorted elapsed times.\n    \"\"\"\n    return sorted([item.elapsed_time for history in self for item in history])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.get_equal_size_histories","title":"get_equal_size_histories","text":"<pre><code>get_equal_size_histories() -&gt; PerformanceHistories\n</code></pre> <p>Return the histories extended to the maximum size.</p> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def get_equal_size_histories(self) -&gt; PerformanceHistories:\n    \"\"\"Return the histories extended to the maximum size.\"\"\"\n    return PerformanceHistories(*[\n        history.extend(self.maximum_size) for history in self\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.get_numbers_of_discipline_executions","title":"get_numbers_of_discipline_executions","text":"<pre><code>get_numbers_of_discipline_executions() -&gt; list[int]\n</code></pre> <p>Return the sorted numbers of discipline executions of all the history items.</p> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>The sorted numbers of discipline executions.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def get_numbers_of_discipline_executions(self) -&gt; list[int]:\n    \"\"\"Return the sorted numbers of discipline executions of all the history items.\n\n    Returns:\n        The sorted numbers of discipline executions.\n    \"\"\"\n    return sorted({\n        item.number_of_discipline_executions for history in self for item in history\n    })\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.insert","title":"insert","text":"<pre><code>insert(index: int, history: PerformanceHistory) -&gt; None\n</code></pre> <p>Insert a performance history in the collection.</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The index where to insert the performance history.</p> </li> <li> <code>history</code>               (<code>PerformanceHistory</code>)           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def insert(self, index: int, history: PerformanceHistory) -&gt; None:\n    \"\"\"Insert a performance history in the collection.\n\n    Args:\n        index: The index where to insert the performance history.\n        history: The performance history.\n    \"\"\"\n    self.__histories.insert(index, history)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_centiles_range","title":"plot_centiles_range  <code>staticmethod</code>","text":"<pre><code>plot_centiles_range(\n    histories: ndarray,\n    axes: Axes,\n    centile_range: tuple[float, float],\n    fill_between_settings: Mapping[str, str],\n    infinity: float | None,\n    performance_measure_is_minimized: bool,\n) -&gt; None\n</code></pre> <p>Plot a range of centiles of histories data.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>ndarray</code>)           \u2013            <p>The histories data.</p> </li> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>centile_range</code>               (<code>tuple[float, float]</code>)           \u2013            <p>The range of centiles to be drawn.</p> </li> <li> <code>fill_between_settings</code>               (<code>Mapping[str, str]</code>)           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.fill_between</code>.</p> </li> <li> <code>infinity</code>               (<code>float | None</code>)           \u2013            <p>The substitute value for infinite ordinates.</p> </li> <li> <code>performance_measure_is_minimized</code>               (<code>bool</code>)           \u2013            <p>Whether the performance measure is minimized (rather than maximized).</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>@staticmethod\ndef plot_centiles_range(\n    histories: numpy.ndarray,\n    axes: Axes,\n    centile_range: tuple[float, float],\n    fill_between_settings: Mapping[str, str],\n    infinity: float | None,\n    performance_measure_is_minimized: bool,\n) -&gt; None:\n    \"\"\"Plot a range of centiles of histories data.\n\n    Args:\n        histories: The histories data.\n        axes: The axes of the plot.\n        centile_range: The range of centiles to be drawn.\n        fill_between_settings: Keyword arguments\n            for `matplotlib.axes.Axes.fill_between`.\n        infinity: The substitute value for infinite ordinates.\n        performance_measure_is_minimized: Whether the performance measure\n            is minimized (rather than maximized).\n    \"\"\"\n    method = \"inverted_cdf\"  # N.B. This method supports infinite values.\n    histories = numpy.nan_to_num(\n        histories,\n        nan=float(\"inf\") if performance_measure_is_minimized else -float(\"inf\"),\n    )\n    lower_centile = numpy.percentile(\n        histories, min(centile_range), 0, method=method\n    )\n    upper_centile = numpy.percentile(\n        histories, max(centile_range), 0, method=method\n    )\n    # Determine the first index with a finite value to plot.\n    centile = lower_centile if performance_measure_is_minimized else upper_centile\n    first_index = next(\n        (i for i, value in enumerate(centile) if numpy.isfinite(value)),\n        len(centile),\n    )\n    axes.plot(  # hack to get same limits/ticks\n        range(1, first_index + 1),\n        numpy.full(\n            first_index,\n            centile[first_index] if first_index &lt; len(centile) else numpy.nan,\n        ),\n        alpha=0,\n    )\n\n    if infinity is not None:\n        if performance_measure_is_minimized:\n            upper_centile = numpy.nan_to_num(upper_centile, posinf=infinity)\n        else:\n            lower_centile = numpy.nan_to_num(lower_centile, neginf=infinity)\n\n    axes.fill_between(\n        range(first_index + 1, histories.shape[1] + 1),\n        lower_centile[first_index:],\n        upper_centile[first_index:],\n        **fill_between_settings,\n    )\n    axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_infeasibility_measure_distribution","title":"plot_infeasibility_measure_distribution","text":"<pre><code>plot_infeasibility_measure_distribution(\n    axes: Axes, plot_all_histories: bool = False\n) -&gt; None\n</code></pre> <p>Plot the distribution of the infeasibility measure.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def plot_infeasibility_measure_distribution(\n    self, axes: Axes, plot_all_histories: bool = False\n) -&gt; None:\n    \"\"\"Plot the distribution of the infeasibility measure.\n\n    Args:\n        axes: The axes of the plot.\n        plot_all_histories: Whether to plot all the performance histories.\n    \"\"\"\n    self.__plot_distribution(\n        numpy.array([history.infeasibility_measures for history in self]),\n        axes,\n        \"Infeasibility measure\",\n        plot_all_histories=plot_all_histories,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_median","title":"plot_median  <code>staticmethod</code>","text":"<pre><code>plot_median(\n    histories: ndarray,\n    axes: Axes,\n    plot_settings: Mapping[str, str | int | float],\n    performance_measure_is_minimized: bool,\n) -&gt; None\n</code></pre> <p>Plot a range of centiles of histories data.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>ndarray</code>)           \u2013            <p>The histories data.</p> </li> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>plot_settings</code>               (<code>Mapping[str, str | int | float]</code>)           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.plot</code>.</p> </li> <li> <code>performance_measure_is_minimized</code>               (<code>bool</code>)           \u2013            <p>Whether the performance measure is minimized (rather than maximized).</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>@staticmethod\ndef plot_median(\n    histories: numpy.ndarray,\n    axes: Axes,\n    plot_settings: Mapping[str, str | int | float],\n    performance_measure_is_minimized: bool,\n) -&gt; None:\n    \"\"\"Plot a range of centiles of histories data.\n\n    Args:\n        histories: The histories data.\n        axes: The axes of the plot.\n        plot_settings: Keyword arguments for `matplotlib.axes.Axes.plot`.\n        performance_measure_is_minimized: Whether the performance measure\n            is minimized (rather than maximized).\n    \"\"\"\n    median = numpy.median(\n        numpy.nan_to_num(\n            histories,\n            nan=float(\"inf\") if performance_measure_is_minimized else -float(\"inf\"),\n        ),\n        0,\n    )\n    # Skip infinite values to support the ``markevery`` option.\n    first_index = next(\n        (index for index, value in enumerate(median) if numpy.isfinite(value)),\n        histories.shape[1],\n    )\n    axes.plot(\n        range(first_index + 1, histories.shape[1] + 1),\n        median[first_index:],\n        **plot_settings,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_number_of_unsatisfied_constraints_distribution","title":"plot_number_of_unsatisfied_constraints_distribution","text":"<pre><code>plot_number_of_unsatisfied_constraints_distribution(\n    axes: Axes, plot_all_histories: bool = False\n) -&gt; None\n</code></pre> <p>Plot the distribution of the number of unsatisfied constraints.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def plot_number_of_unsatisfied_constraints_distribution(\n    self, axes: Axes, plot_all_histories: bool = False\n) -&gt; None:\n    \"\"\"Plot the distribution of the number of unsatisfied constraints.\n\n    Args:\n        axes: The axes of the plot.\n        plot_all_histories: Whether to plot all the performance histories.\n    \"\"\"\n    self.__plot_distribution(\n        numpy.array([\n            [\n                numpy.nan if n is None else n\n                for n in history.n_unsatisfied_constraints\n            ]\n            for history in self\n        ]),\n        axes,\n        \"Number of unsatisfied constraints\",\n        plot_all_histories=plot_all_histories,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_performance_measure_distribution","title":"plot_performance_measure_distribution","text":"<pre><code>plot_performance_measure_distribution(\n    axes: Axes,\n    extremal_feasible_performance: float | None,\n    infeasible_performance_measure: float,\n    plot_all_histories: bool = False,\n    performance_measure_is_minimized: bool = True,\n) -&gt; None\n</code></pre> <p>Plot the distribution of the performance measure.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>extremal_feasible_performance</code>               (<code>float | None</code>)           \u2013            <p>The extremal feasible performance measure.</p> </li> <li> <code>infeasible_performance_measure</code>               (<code>float</code>)           \u2013            <p>The value to replace the performance measure of infeasible history items when computing statistics.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>performance_measure_is_minimized</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether the performance measure is minimized (rather than maximized).</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def plot_performance_measure_distribution(\n    self,\n    axes: Axes,\n    extremal_feasible_performance: float | None,\n    infeasible_performance_measure: float,\n    plot_all_histories: bool = False,\n    performance_measure_is_minimized: bool = True,\n) -&gt; None:\n    \"\"\"Plot the distribution of the performance measure.\n\n    Args:\n        axes: The axes of the plot.\n        extremal_feasible_performance: The extremal feasible performance measure.\n        infeasible_performance_measure: The value to replace the performance measure\n            of infeasible history items when computing statistics.\n        plot_all_histories: Whether to plot all the performance histories.\n        performance_measure_is_minimized: Whether the performance measure\n            is minimized (rather than maximized).\n    \"\"\"\n    self.__plot_distribution(\n        numpy.array([\n            [\n                item.performance_measure\n                if item.is_feasible\n                else infeasible_performance_measure\n                for item in history\n            ]\n            for history in self.get_equal_size_histories()\n        ]),\n        axes,\n        \"Performance measure\",\n        extremal_feasible_performance,\n        plot_all_histories,\n        performance_measure_is_minimized,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.spread_over_numbers_of_discipline_executions","title":"spread_over_numbers_of_discipline_executions","text":"<pre><code>spread_over_numbers_of_discipline_executions(\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistories\n</code></pre> <p>Spread the histories over all the numbers of discipline executions.</p> <p>Parameters:</p> <ul> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints of the underlying problem.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistories</code>           \u2013            <p>The performance histories with has as many items as total numbers of discipline executions.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def spread_over_numbers_of_discipline_executions(\n    self, number_of_scalar_constraints: int\n) -&gt; PerformanceHistories:\n    \"\"\"Spread the histories over all the numbers of discipline executions.\n\n    Args:\n        number_of_scalar_constraints: The number of scalar constraints\n            of the underlying problem.\n\n    Returns:\n        The performance histories with has as many items as total numbers\n        of discipline executions.\n    \"\"\"\n    numbers_of_discipline_executions = self.get_numbers_of_discipline_executions()\n    return self.__class__(*[\n        history.spread_over_numbers_of_discipline_executions(\n            numbers_of_discipline_executions, number_of_scalar_constraints\n        )\n        for history in self\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.spread_over_time","title":"spread_over_time","text":"<pre><code>spread_over_time(\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistories\n</code></pre> <p>Spread the histories over all the elapsed times.</p> <p>Parameters:</p> <ul> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints of the underlying problem.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistories</code>           \u2013            <p>The performance histories with has as many items as total elapsed times.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def spread_over_time(\n    self, number_of_scalar_constraints: int\n) -&gt; PerformanceHistories:\n    \"\"\"Spread the histories over all the elapsed times.\n\n    Args:\n        number_of_scalar_constraints: The number of scalar constraints\n            of the underlying problem.\n\n    Returns:\n        The performance histories with has as many items as total elapsed times.\n    \"\"\"\n    timeline = self.get_elapsed_times()\n    return self.__class__(*[\n        history.spread_over_timeline(timeline, number_of_scalar_constraints)\n        for history in self\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.switch_performance_measure_sign","title":"switch_performance_measure_sign","text":"<pre><code>switch_performance_measure_sign() -&gt; None\n</code></pre> <p>Switch the sign of the performance measure.</p> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def switch_performance_measure_sign(self) -&gt; None:\n    \"\"\"Switch the sign of the performance measure.\"\"\"\n    for history in self:\n        history.switch_performance_measure_sign()\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/","title":"Performance history","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history","title":"performance_history","text":"<p>Class that generates performance measures out of data generated by an algorithm.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory","title":"PerformanceHistory","text":"<pre><code>PerformanceHistory(\n    performance_measures: Sequence[float] = (),\n    infeasibility_measures: Sequence[float] = (),\n    feasibility_statuses: Sequence[bool] = (),\n    n_unsatisfied_constraints: Sequence[int] = (),\n    problem_name: str = \"\",\n    objective_name: str = \"\",\n    constraints_names: Sequence[str] = (),\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: (\n        AlgorithmConfiguration | None\n    ) = None,\n    number_of_variables: int | None = None,\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n)\n</code></pre> <p>               Bases: <code>Sequence</code></p> <p>A history of performance measures generated by an algorithm.</p> <p>A :class:<code>.PerformanceHistory</code> is a sequence of :class:<code>.HistoryItem</code>\\ s.</p> <p>Iterative algorithms that solve, for example, optimization problems or equations, produce histories of data such as the value of the objective to minimize, or the size of the equation residual, at each iteration. The best value obtained up until each iteration can be generated out of this data. Here we call \"performance history\" the history of the best values obtained up until each iteration.</p> <p>Infeasible data can be discarded based upon histories of infeasibility measures or boolean feasibility statuses.</p> <p>Performance histories can be used to generate target values for a problem, or to generate the data profile of an algorithm.</p> <p>Attributes:</p> <ul> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> <li> <code>total_time</code>               (<code>float</code>)           \u2013            <p>The run time of the algorithm.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>performance_measures</code>               (<code>Sequence[float]</code>, default:                   <code>()</code> )           \u2013            <p>The history of performance measures.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float]</code>, default:                   <code>()</code> )           \u2013            <p>The history of infeasibility measures. An infeasibility measure is a non-negative real number representing the gap between the design and the feasible space, a zero value meaning feasibility. If empty and <code>feasibility_statuses</code> is not empty then the infeasibility measures are set to zero in case of feasibility, and set to infinity otherwise. If empty and <code>feasibility_statuses</code> is empty then every infeasibility measure is set to zero.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the (boolean) feasibility statuses. If <code>infeasibility_measures</code> is not empty then <code>feasibility_statuses</code> is disregarded. If empty and 'infeasibility_measures' is empty then every infeasibility measure is set to zero.</p> </li> <li> <code>n_unsatisfied_constraints</code>               (<code>Sequence[int]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the number of unsatisfied constraints. If empty, the entries will be set to 0 for feasible entries and <code>None</code> for infeasible entries.</p> </li> <li> <code>problem_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the problem.</p> </li> <li> <code>objective_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the objective function.</p> </li> <li> <code>constraints_names</code>               (<code>Sequence[str]</code>, default:                   <code>()</code> )           \u2013            <p>The names the scalar constraints. Each name must correspond to a scalar value. If empty, they will not be set.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the initial design of experiments. If <code>None</code>, it will not be set.</p> </li> <li> <code>total_time</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The total time of the optimization, in seconds. If <code>None</code>, it will not be set.</p> </li> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the algorithm which generated the history. If <code>None</code>, it will not be set.</p> </li> <li> <code>number_of_variables</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of optimization variables. If <code>None</code>, it will not be set.</p> </li> <li> <code>elapsed_times</code>               (<code>Sequence[timedelta]</code>, default:                   <code>()</code> )           \u2013            <p>The history of elapsed times. If empty, the elapsed times are set to zero.</p> </li> <li> <code>number_of_discipline_executions</code>               (<code>Sequence[int]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the number of discipline executions. If <code>empty</code>, the number of discipline executions are set to zero.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the lengths of the histories do not match.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def __init__(\n    self,\n    performance_measures: Sequence[float] = (),\n    infeasibility_measures: Sequence[float] = (),\n    feasibility_statuses: Sequence[bool] = (),\n    n_unsatisfied_constraints: Sequence[int] = (),\n    problem_name: str = \"\",\n    objective_name: str = \"\",\n    constraints_names: Sequence[str] = (),\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: AlgorithmConfiguration | None = None,\n    number_of_variables: int | None = None,\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n) -&gt; None:\n    \"\"\"\n    Args:\n        performance_measures: The history of performance measures.\n        infeasibility_measures: The history of infeasibility measures.\n            An infeasibility measure is a non-negative real number representing\n            the gap between the design and the feasible space,\n            a zero value meaning feasibility.\n            If empty and `feasibility_statuses` is not empty\n            then the infeasibility measures are set to zero in case of feasibility,\n            and set to infinity otherwise.\n            If empty and `feasibility_statuses` is empty\n            then every infeasibility measure is set to zero.\n        feasibility_statuses: The history of the (boolean) feasibility statuses.\n            If `infeasibility_measures` is not empty then `feasibility_statuses` is\n            disregarded.\n            If empty and 'infeasibility_measures' is empty\n            then every infeasibility measure is set to zero.\n        n_unsatisfied_constraints: The history of the number of unsatisfied\n            constraints.\n            If empty, the entries will be set to 0 for feasible entries\n            and ``None`` for infeasible entries.\n        problem_name: The name of the problem.\n        objective_name: The name of the objective function.\n        constraints_names: The names the scalar constraints.\n            Each name must correspond to a scalar value.\n            If empty, they will not be set.\n        doe_size: The size of the initial design of experiments.\n            If ``None``, it will not be set.\n        total_time: The total time of the optimization, in seconds.\n            If ``None``, it will not be set.\n        algorithm_configuration: The name of the algorithm which generated the\n            history.\n            If ``None``, it will not be set.\n        number_of_variables: The number of optimization variables.\n            If ``None``, it will not be set.\n        elapsed_times: The history of elapsed times.\n            If empty, the elapsed times are set to zero.\n        number_of_discipline_executions: The history of the number\n            of discipline executions.\n            If `empty`, the number of discipline executions are set to zero.\n\n    Raises:\n        ValueError: If the lengths of the histories do not match.\n    \"\"\"  # noqa: D205, D212, D415\n    self._constraints_names = constraints_names\n    self._objective_name = objective_name\n    self.algorithm_configuration = algorithm_configuration\n    self.doe_size = doe_size\n    self.items = self.__get_history_items(\n        performance_measures,\n        infeasibility_measures,\n        feasibility_statuses,\n        n_unsatisfied_constraints,\n        elapsed_times,\n        number_of_discipline_executions,\n    )\n    self.problem_name = problem_name\n    self._number_of_variables = number_of_variables\n    self.total_time = total_time\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.infeasibility_measures","title":"infeasibility_measures  <code>property</code>","text":"<pre><code>infeasibility_measures: list[float]\n</code></pre> <p>The infeasibility measures.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.items","title":"items  <code>property</code> <code>writable</code>","text":"<pre><code>items: list[HistoryItem]\n</code></pre> <p>The history items.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.n_unsatisfied_constraints","title":"n_unsatisfied_constraints  <code>property</code>","text":"<pre><code>n_unsatisfied_constraints: list[int]\n</code></pre> <p>The numbers of unsatisfied constraints.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.performance_measures","title":"performance_measures  <code>property</code>","text":"<pre><code>performance_measures: list[float]\n</code></pre> <p>The performance measures.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.apply_infeasibility_tolerance","title":"apply_infeasibility_tolerance","text":"<pre><code>apply_infeasibility_tolerance(\n    infeasibility_tolerance: float,\n) -&gt; None\n</code></pre> <p>Apply a tolerance on the infeasibility measures of the history items.</p> <p>Mark the history items with an infeasibility measure below the tolerance as feasible.</p> <p>Parameters:</p> <ul> <li> <code>infeasibility_tolerance</code>               (<code>float</code>)           \u2013            <p>the tolerance on the infeasibility measure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def apply_infeasibility_tolerance(self, infeasibility_tolerance: float) -&gt; None:\n    \"\"\"Apply a tolerance on the infeasibility measures of the history items.\n\n    Mark the history items with an infeasibility measure below the tolerance\n    as feasible.\n\n    Args:\n        infeasibility_tolerance: the tolerance on the infeasibility measure.\n    \"\"\"\n    for item in self.items:\n        item.apply_infeasibility_tolerance(infeasibility_tolerance)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.compute_cumulated_minimum","title":"compute_cumulated_minimum","text":"<pre><code>compute_cumulated_minimum() -&gt; PerformanceHistory\n</code></pre> <p>Return the history of the cumulated minimum.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The history of the cumulated minimum.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def compute_cumulated_minimum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history of the cumulated minimum.\n\n    Returns:\n        The history of the cumulated minimum.\n    \"\"\"\n    minimum_history = copy(self)\n    minimum_history.items = minimum_history.items[:1]\n    for item in self.items[1:]:\n        last_item = minimum_history.items[-1]\n        if item &gt;= last_item:\n            new_item = last_item.copy()\n            # N.B. The copy ensures the new items are independent objects.\n            new_item.elapsed_time = item.elapsed_time\n        else:\n            new_item = item\n\n        minimum_history.items.append(new_item)\n\n    return minimum_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.extend","title":"extend","text":"<pre><code>extend(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Extend the performance history by repeating its last item.</p> <p>If the history is longer than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the extended performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The extended performance history.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the expected size is smaller than the history size.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def extend(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Extend the performance history by repeating its last item.\n\n    If the history is longer than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the extended performance history.\n\n    Returns:\n        The extended performance history.\n\n    Raises:\n        ValueError: If the expected size is smaller than the history size.\n    \"\"\"\n    if size &lt; len(self):\n        msg = (\n            f\"The expected size ({size}) is smaller than \"\n            f\"the history size ({len(self)}).\"\n        )\n        raise ValueError(msg)\n\n    history = copy(self)\n    history.items = list(self)\n    for _ in range(size - len(self)):\n        history.items.append(self[-1].copy())\n        # N.B. The copy ensures the new items are independent objects.\n\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str | Path) -&gt; PerformanceHistory\n</code></pre> <p>Create a new performance history from a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str | Path) -&gt; PerformanceHistory:\n    \"\"\"Create a new performance history from a file.\n\n    Args:\n        path: The path to the file.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    with Path(path).open(\"r\") as file:\n        data = json.load(file)\n\n    history = cls()\n    history.problem_name = data.get(cls.__PROBLEM)\n    history._number_of_variables = data.get(cls.__NUMBER_OF_VARIABLES)\n    history._objective_name = data.get(cls.__OBJECTIVE_NAME)\n    history._constraints_names = data.get(cls.__CONSTRAINTS_NAMES, [])\n    if cls.__ALGORITHM_CONFIGURATION in data:\n        history.algorithm_configuration = AlgorithmConfiguration.from_dict(\n            data[cls.__ALGORITHM_CONFIGURATION]\n        )\n\n    history.doe_size = data.get(cls.__DOE_SIZE)\n    history.total_time = data.get(cls.__EXECUTION_TIME)\n    history.items = [\n        HistoryItem.from_dict(item_data) for item_data in data[cls.__HISTORY_ITEMS]\n    ]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.from_problem","title":"from_problem  <code>classmethod</code>","text":"<pre><code>from_problem(\n    problem: OptimizationProblem,\n    problem_name: str = \"\",\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n) -&gt; PerformanceHistory\n</code></pre> <p>Create a performance history from a solved optimization problem.</p> <p>Parameters:</p> <ul> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> <li> <code>problem_name</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The name of the problem.</p> </li> <li> <code>elapsed_times</code>               (<code>Sequence[timedelta]</code>, default:                   <code>()</code> )           \u2013            <p>The history of elapsed times. If empty, the elapsed times are set to zero.</p> </li> <li> <code>number_of_discipline_executions</code>               (<code>Sequence[int]</code>, default:                   <code>()</code> )           \u2013            <p>The history of the number of discipline executions. If <code>empty</code>, the number of discipline executions are set to zero.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_problem(\n    cls,\n    problem: OptimizationProblem,\n    problem_name: str = \"\",\n    elapsed_times: Sequence[timedelta] = (),\n    number_of_discipline_executions: Sequence[int] = (),\n) -&gt; PerformanceHistory:\n    \"\"\"Create a performance history from a solved optimization problem.\n\n    Args:\n        problem: The optimization problem.\n        problem_name: The name of the problem.\n        elapsed_times: The history of elapsed times.\n            If empty, the elapsed times are set to zero.\n        number_of_discipline_executions: The history of the number\n            of discipline executions.\n            If `empty`, the number of discipline executions are set to zero.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    obj_name = problem.objective.name\n    obj_values = []\n    infeas_measures = []\n    feas_statuses = []\n    n_unsatisfied_constraints = []\n    retained_elapsed_times = []\n    retained_number_of_discipline_executions = []\n    functions_names = {obj_name, *problem.constraints.get_names()}\n    for index, (design_values, output_values) in enumerate(\n        problem.database.items()\n    ):\n        # Only consider points with all functions values\n        if not functions_names &lt;= set(output_values.keys()):\n            continue\n\n        x_vect = design_values.unwrap()\n        obj_values.append(atleast_1d(output_values[obj_name]).real[0])\n        feasibility, measure = problem.history.check_design_point_is_feasible(\n            x_vect\n        )\n        number_of_unsatisfied_constraints = (\n            problem.constraints.get_number_of_unsatisfied_constraints(output_values)\n        )\n        infeas_measures.append(measure)\n        feas_statuses.append(feasibility)\n        n_unsatisfied_constraints.append(number_of_unsatisfied_constraints)\n        if elapsed_times:\n            retained_elapsed_times.append(elapsed_times[index])\n\n        if number_of_discipline_executions:\n            retained_number_of_discipline_executions.append(\n                number_of_discipline_executions[index]\n            )\n\n    return cls(\n        obj_values,\n        infeas_measures,\n        feas_statuses,\n        n_unsatisfied_constraints,\n        problem_name,\n        problem.objective.name,\n        problem.scalar_constraint_names,\n        number_of_variables=problem.design_space.dimension,\n        elapsed_times=retained_elapsed_times,\n        number_of_discipline_executions=retained_number_of_discipline_executions,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.get_plot_data","title":"get_plot_data","text":"<pre><code>get_plot_data(\n    feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]\n</code></pre> <p>Return the data to plot the performance history.</p> <p>Parameters:</p> <ul> <li> <code>feasible</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get only feasible values.</p> </li> <li> <code>minimum_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get the history of the cumulated minimum instead of the history of the performance measure.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[int], list[HistoryItem]]</code>           \u2013            <p>The abscissas and the ordinates of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def get_plot_data(\n    self, feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]:\n    \"\"\"Return the data to plot the performance history.\n\n    Args:\n        feasible: Whether to get only feasible values.\n        minimum_history: Whether to get the history of the cumulated minimum\n            instead of the history of the performance measure.\n\n    Returns:\n        The abscissas and the ordinates of the plot.\n    \"\"\"\n    history = self.compute_cumulated_minimum() if minimum_history else self\n\n    # Find the index of the first feasible history item\n    if feasible:\n        first_feasible_index = len(history)\n        for index, item in enumerate(history):\n            if item.is_feasible:\n                first_feasible_index = index\n                break\n    else:\n        first_feasible_index = 0\n\n    return (\n        list(range(first_feasible_index + 1, len(history) + 1)),\n        history[first_feasible_index:],\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.remove_leading_infeasible","title":"remove_leading_infeasible","text":"<pre><code>remove_leading_infeasible() -&gt; PerformanceHistory\n</code></pre> <p>Return the history starting from the first feasible item.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The truncated performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def remove_leading_infeasible(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history starting from the first feasible item.\n\n    Returns:\n        The truncated performance history.\n    \"\"\"\n    first_feasible = len(self)\n    for index, item in enumerate(self):\n        if item.is_feasible:\n            first_feasible = index\n            break\n\n    truncated_history = copy(self)\n    truncated_history.items = self.items[first_feasible:]\n    return truncated_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.shorten","title":"shorten","text":"<pre><code>shorten(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Shorten the performance history to a given size.</p> <p>If the history is shorter than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the shortened performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The shortened performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def shorten(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Shorten the performance history to a given size.\n\n    If the history is shorter than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the shortened performance history.\n\n    Returns:\n        The shortened performance history.\n    \"\"\"\n    history = copy(self)\n    history.items = self.items[:size]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.spread_over_numbers_of_discipline_executions","title":"spread_over_numbers_of_discipline_executions","text":"<pre><code>spread_over_numbers_of_discipline_executions(\n    numbers_of_discipline_executions: Iterable[int],\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistory\n</code></pre> <p>Spread the history over a scale of number discipline executions.</p> <p>Note</p> <p>The scale is assumed to be sorted in increasing order.</p> <p>Parameters:</p> <ul> <li> <code>numbers_of_discipline_executions</code>               (<code>Iterable[int]</code>)           \u2013            <p>An increasing sequence of numbers of discipline executions.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history with has as many items as elements in the scale.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def spread_over_numbers_of_discipline_executions(\n    self,\n    numbers_of_discipline_executions: Iterable[int],\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistory:\n    \"\"\"Spread the history over a scale of number discipline executions.\n\n    !!! note\n\n        The scale is assumed to be sorted in increasing order.\n\n    Args:\n        numbers_of_discipline_executions: An increasing sequence\n            of numbers of discipline executions.\n        number_of_scalar_constraints: The number of scalar constraints.\n\n    Returns:\n        The performance history with has as many items as elements in the scale.\n    \"\"\"\n    return self.__spread_over_scale(\n        numbers_of_discipline_executions,\n        number_of_scalar_constraints,\n        HistoryItem.number_of_discipline_executions,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.spread_over_timeline","title":"spread_over_timeline","text":"<pre><code>spread_over_timeline(\n    timeline: Iterable[timedelta],\n    number_of_scalar_constraints: int,\n) -&gt; PerformanceHistory\n</code></pre> <p>Spread the history over a timeline of elapsed times.</p> <p>Note</p> <p>The timeline is assumed to be sorted in increasing order.</p> <p>Parameters:</p> <ul> <li> <code>timeline</code>               (<code>Iterable[timedelta]</code>)           \u2013            <p>An increasing sequence of elapsed times.</p> </li> <li> <code>number_of_scalar_constraints</code>               (<code>int</code>)           \u2013            <p>The number of scalar constraints.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history with has as many items as elements in the timeline.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def spread_over_timeline(\n    self, timeline: Iterable[timedelta], number_of_scalar_constraints: int\n) -&gt; PerformanceHistory:\n    \"\"\"Spread the history over a timeline of elapsed times.\n\n    !!! note\n\n        The timeline is assumed to be sorted in increasing order.\n\n    Args:\n        timeline: An increasing sequence of elapsed times.\n        number_of_scalar_constraints: The number of scalar constraints.\n\n    Returns:\n        The performance history with has as many items as elements in the timeline.\n    \"\"\"\n    return self.__spread_over_scale(\n        timeline, number_of_scalar_constraints, HistoryItem.elapsed_time\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.switch_performance_measure_sign","title":"switch_performance_measure_sign","text":"<pre><code>switch_performance_measure_sign() -&gt; None\n</code></pre> <p>Switch the sign of the performance measure.</p> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def switch_performance_measure_sign(self) -&gt; None:\n    \"\"\"Switch the sign of the performance measure.\"\"\"\n    for item in self:\n        item.switch_performance_measure_sign()\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.to_file","title":"to_file","text":"<pre><code>to_file(path: str | Path) -&gt; None\n</code></pre> <p>Save the performance history in a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path where to write the file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def to_file(\n    self,\n    path: str | Path,\n) -&gt; None:\n    \"\"\"Save the performance history in a file.\n\n    Args:\n        path: The path where to write the file.\n    \"\"\"\n    data = {}\n    if self.problem_name:\n        data[self.__PROBLEM] = self.problem_name\n\n    if self._number_of_variables is not None:\n        data[self.__NUMBER_OF_VARIABLES] = self._number_of_variables\n\n    if self._objective_name:\n        data[self.__OBJECTIVE_NAME] = self._objective_name\n\n    if self._constraints_names:\n        data[self.__CONSTRAINTS_NAMES] = self._constraints_names\n\n    if self.algorithm_configuration is not None:\n        data[self.__ALGORITHM_CONFIGURATION] = self.algorithm_configuration.to_dict(\n            True\n        )\n\n    if self.doe_size is not None:\n        data[self.__DOE_SIZE] = self.doe_size\n\n    if self.total_time is not None:\n        data[self.__EXECUTION_TIME] = self.total_time\n\n    data[self.__HISTORY_ITEMS] = [item.to_dict() for item in self.items]\n    with Path(path).open(\"w\") as file:\n        json.dump(data, file, indent=2, separators=(\",\", \": \"))\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/","title":"Results","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results","title":"results","text":"<p>A class to collect the paths to performance histories.</p>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results","title":"Results","text":"<pre><code>Results(path: str | Path = '')\n</code></pre> <p>A collection of paths to performance histories.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>, default:                   <code>''</code> )           \u2013            <p>The path to the JSON file from which to load the paths. If <code>None</code>, the collection is initially empty.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def __init__(self, path: str | Path = \"\") -&gt; None:\n    \"\"\"\n    Args:\n        path: The path to the JSON file from which to load the paths.\n            If ``None``, the collection is initially empty.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__dict = {}\n    if path:\n        self.from_file(path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.algorithms","title":"algorithms  <code>property</code>","text":"<pre><code>algorithms: list[str]\n</code></pre> <p>Return the names of the algorithms configurations.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>The names of the algorithms configurations.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.add_path","title":"add_path","text":"<pre><code>add_path(\n    algorithm_configuration_name: str,\n    problem_name: str,\n    path: str | Path,\n) -&gt; None\n</code></pre> <p>Add a path to a performance history.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration associated with the history.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem associated with the history.</p> </li> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the history.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the path to the history does not exist.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def add_path(\n    self, algorithm_configuration_name: str, problem_name: str, path: str | Path\n) -&gt; None:\n    \"\"\"Add a path to a performance history.\n\n    Args:\n        algorithm_configuration_name: The name of the algorithm configuration\n            associated with the history.\n        problem_name: The name of the problem associated with the history.\n        path: The path to the history.\n\n    Raises:\n        FileNotFoundError: If the path to the history does not exist.\n    \"\"\"\n    try:\n        absolute_path = Path(path).resolve(strict=True)\n    except FileNotFoundError:\n        msg = f\"The path to the history does not exist: {path}.\"\n        raise FileNotFoundError(msg) from None\n    if algorithm_configuration_name not in self.__dict:\n        self.__dict[algorithm_configuration_name] = {}\n\n    if problem_name not in self.__dict[algorithm_configuration_name]:\n        self.__dict[algorithm_configuration_name][problem_name] = []\n\n    self.__dict[algorithm_configuration_name][problem_name].append(absolute_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.contains","title":"contains","text":"<pre><code>contains(\n    algo_name: str, problem_name: str, path: Path\n) -&gt; bool\n</code></pre> <p>Check whether a result is stored.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the performance history</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>Whether the result is stored.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def contains(self, algo_name: str, problem_name: str, path: Path) -&gt; bool:\n    \"\"\"Check whether a result is stored.\n\n    Args:\n        algo_name: The name of the algorithm configuration.\n        problem_name: The name of the problem.\n        path: The path to the performance history\n\n    Returns:\n        Whether the result is stored.\n    \"\"\"\n    return (\n        algo_name in self.__dict\n        and problem_name in self.__dict[algo_name]\n        and path in self.__dict[algo_name][problem_name]\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.from_file","title":"from_file","text":"<pre><code>from_file(path: str | Path) -&gt; None\n</code></pre> <p>Load paths to performance histories from a JSON file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the JSON file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def from_file(self, path: str | Path) -&gt; None:\n    \"\"\"Load paths to performance histories from a JSON file.\n\n    Args:\n        path: The path to the JSON file.\n    \"\"\"\n    if not Path(path).is_file():\n        msg = f\"The path to the JSON file does not exist: {path}.\"\n        raise FileNotFoundError(msg)\n\n    with Path(path).open(\"r\") as file:\n        histories = json.load(file)\n    for algo_name, problems in histories.items():\n        for problem_name, paths in problems.items():\n            for path in paths:\n                self.add_path(algo_name, problem_name, path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.get_paths","title":"get_paths","text":"<pre><code>get_paths(algo_name: str, problem_name: str) -&gt; list[Path]\n</code></pre> <p>Return the paths associated with an algorithm and a problem.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Path]</code>           \u2013            <p>The paths to the performance histories.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm name is unknown, or if the problem name is unknown.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def get_paths(self, algo_name: str, problem_name: str) -&gt; list[Path]:\n    \"\"\"Return the paths associated with an algorithm and a problem.\n\n    Args:\n        algo_name: The name of the algorithm.\n        problem_name: The name of the problem.\n\n    Returns:\n        The paths to the performance histories.\n\n    Raises:\n        ValueError: If the algorithm name is unknown,\n            or if the problem name is unknown.\n    \"\"\"\n    if algo_name not in self.__dict:\n        msg = f\"Unknown algorithm name: {algo_name}.\"\n        raise ValueError(msg)\n\n    if problem_name not in self.__dict[algo_name]:\n        msg = f\"Unknown problem name: {problem_name}.\"\n        raise ValueError(msg)\n\n    return self.__dict[algo_name][problem_name]\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.get_problems","title":"get_problems","text":"<pre><code>get_problems(algo_name: str) -&gt; list[str]\n</code></pre> <p>Return the names of the problems for a given algorithm configuration.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>The names of the problems.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm configuration name is unknown.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def get_problems(self, algo_name: str) -&gt; list[str]:\n    \"\"\"Return the names of the problems for a given algorithm configuration.\n\n    Args:\n        algo_name: The name of the algorithm configuration.\n\n    Returns:\n        The names of the problems.\n\n    Raises:\n        ValueError: If the algorithm configuration name is unknown.\n    \"\"\"\n    if algo_name not in self.__dict:\n        msg = f\"Unknown algorithm name: {algo_name}.\"\n        raise ValueError(msg)\n\n    return list(self.__dict[algo_name])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.remove_paths","title":"remove_paths","text":"<pre><code>remove_paths(\n    algorithm_configuration_name: str, problem_name: str\n) -&gt; None\n</code></pre> <p>Remove the paths associated with an algorithm/problem pair.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def remove_paths(\n    self, algorithm_configuration_name: str, problem_name: str\n) -&gt; None:\n    \"\"\"Remove the paths associated with an algorithm/problem pair.\n\n    Args:\n        algorithm_configuration_name: The name of the algorithm configuration.\n        problem_name: The name of the problem.\n    \"\"\"\n    if (\n        algorithm_configuration_name in self.__dict\n        and problem_name in self.__dict[algorithm_configuration_name]\n    ):\n        del self.__dict[algorithm_configuration_name][problem_name]\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.to_file","title":"to_file","text":"<pre><code>to_file(\n    path: str | Path, indent: int | None = None\n) -&gt; None\n</code></pre> <p>Save the histories paths to a JSON file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path where to save the JSON file.</p> </li> <li> <code>indent</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The indent level of the JSON serialization.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def to_file(self, path: str | Path, indent: int | None = None) -&gt; None:\n    \"\"\"Save the histories paths to a JSON file.\n\n    Args:\n        path: The path where to save the JSON file.\n        indent: The indent level of the JSON serialization.\n    \"\"\"\n    # Convert the paths to strings to be JSON serializable\n    serializable = {}\n    for algo_name, problems in self.__dict.items():\n        serializable[algo_name] = {}\n        for problem_name, paths in problems.items():\n            serializable[algo_name][problem_name] = [str(path) for path in paths]\n    with Path(path).open(\"w\") as file:\n        json.dump(serializable, file, indent=indent)\n</code></pre>"},{"location":"user_guide/","title":"User guide","text":""},{"location":"user_guide/#user-guide","title":"User guide","text":"<p>The gemseo_benchmark package provides functionalities to benchmark optimization algorithms, that is, to measure and compare their performances.</p> <p>A typical use of this package consists of the following steps:</p> <ol> <li>define the     algorithms configurations to be compared,</li> <li>define the     problem configurations     that will serve as landmarks for the analysis,</li> <li>execute a benchmarking scenario to produce<ol> <li>the results of the     algorithms configurations on the problem configurations,</li> <li>a benchmarking report     in HTML or PDF format illustrated with     data profiles.</li> </ol> </li> </ol> <p>Note</p> <p>Other algorithms will be supported in the future (ex: root-finding algorithms).</p> <p>The following sections present the sub-packages of gemseo_benchmark.</p>"},{"location":"user_guide/#algorithms-configurations","title":"Algorithms configurations","text":"<p>The gemseo_benchmark.algorithms sub-package is responsible for the definition of the algorithms configurations to be investigated in a benchmarking study.</p> <p>An AlgorithmConfiguration contains:</p> <ul> <li>the name of an algorithm,</li> <li>optionally, a name for the configuration (it will be generated     automatically if unspecified),</li> <li>the values passed as its options (default values are used for the     unspecified options).</li> </ul> <p>For example, we may consider the L-BFGS-B algorithm with its <code>maxcor</code> option (the maximum number of corrections of the Hessian approximation) set to 2.</p> <pre><code>lbfgsb_2_corrections = AlgorithmConfiguration(\n   \"L-BFGS-B\",\n   \"L-BFGS-B with 2 Hessian corrections\",\n   maxcor=2,\n)\n</code></pre> <p>Additionally, we may consider the same algorithm with a different option value, say 20.</p> <pre><code>lbfgsb_20_corrections = AlgorithmConfiguration(\n   \"L-BFGS-B\",\n   \"L-BFGS-B with 20 Hessian corrections\",\n   maxcor=20,\n)\n</code></pre> <p>Of course it is also possible to consider an algorithm with all its options set to their default values.</p> <pre><code>slsqp_default = AlgorithmConfiguration(\"SLSQP\")\n</code></pre> <p>The class AlgorithmsConfigurations is useful to gather algorithms configurations in groups so that they be treated together in a benchmarking report.</p> <pre><code>lbfgsb_configurations = AlgorithmsConfigurations(\n    lbfgsb_2_corrections,\n    lbfgsb_20_corrections,\n    name=\"L-BFGS-B configurations\",\n)\n</code></pre>"},{"location":"user_guide/#problem-configurations","title":"Problem configurations","text":"<p>The gemseo_benchmark.problems sub-package handles the problem configurations, on which the performances of the algorithms configurations is to be measured.</p> <p>A [OptimizationBenchmarkingProblem][gemseo_benchmark.problems.optimization_benchmarking_problem.OptimizationBenchmarkingProblem] contains the mathematical definition of the problem, as an OptimizationProblem, and requires three other features.</p> <ol> <li>The starting points, from which the algorithms configurations should     be launched on the problem. Indeed, an algorithm may be quite     dependent on the starting point. Therefore, in the context of a     benchmarking study, it is advised to consider several starting     points.<ol> <li>One can pass the starting points directly,</li> <li>or configure their generation as a design of experiments (DOE).</li> </ol> </li> <li>The best objective value known for the problem.</li> <li>The target values,     necessary to compute     data profiles:     typically, a scale of objective functions values ranging from a     relatively easily achievable value to the best value known.     Similarly to starting points, the target values can be either passed     directly, or their generation can be configured.</li> </ol> <p>For example, we define below problem configurations based on Rastrigin and Rosenbrock respectively, where</p> <ul> <li> <p>5 starting points are computed by Latin hypercube sampling (LHS),</p> <pre><code>doe_settings = {\"doe_size\": 5, \"doe_algo_name\": \"lhs\"}\n</code></pre> </li> <li> <p>and the target values are passed directly as an exponential scale     towards the minimum (zero).</p> <pre><code>target_values = TargetValues([10**-4, 10**-5, 10**-6, 0.0])\n</code></pre> </li> </ul> <p>(The class TargetValues will be presented further down.)</p> <pre><code>rastrigin = OptimizationBenchmarkingProblem(\n    \"Rastrigin\",\n    Rastrigin,\n    target_values=target_values,\n    **doe_settings,\n    optimum=0.0,\n)\n\nrosenbrock = OptimizationBenchmarkingProblem(\n    \"Rosenbrock\",\n    Rosenbrock,\n    target_values=target_values,\n    **doe_settings,\n    optimum=0.0,\n)\n</code></pre> <p>Note that the second argument of [OptimizationBenchmarkingProblem][gemseo_benchmark.problems.optimization_benchmarking_problem.OptimizationBenchmarkingProblem] must be callable. For example, a five-variables problem configuration based on Rosenbrock may be defined as follows.</p> <pre><code>rosenbrock_5d = OptimizationBenchmarkingProblem(\n    \"Rosenbrock 5D\",\n    lambda: Rosenbrock(5),\n    target_values=target_values,\n    **doe_settings,\n    optimum=0.0,\n)\n</code></pre> <p>The class ProblemsGroup is useful to gather reference problems in groups so that they be treated together in a benchmarking report.</p> <pre><code>problems_2D = ProblemsGroup(\n    \"2-variabbles functions\",\n    [rastrigin, rosenbrock],\n    description=\"Unconstrained functions depending on 2 variables.\",\n)\n</code></pre>"},{"location":"user_guide/#results","title":"Results","text":"<p>The gemseo_benchmark.results sub-package manages the results produced by the algorithms configurations on the problem configurations.</p> <p>The history of the data produced by an algorithm configuration on a problem configuration is stored in a PerformanceHistory. More precisely:</p> <ul> <li>A value of interest in the benchmarking of algorithms is defined and     named performance value.     The most telling performance value is the     value of the objective function for an optimization problem, or the     value of a residual for a nonlinear equation.</li> <li>Each performance value is stored in a     HistoryItem, along with an     infeasibility measure (especially for problems subject to     constraints).</li> <li>A PerformanceHistory is a     sequence of HistoryItems.     The index of the sequence is understood as the 0-based number of     functions evaluations.</li> </ul> <p>A PerformanceHistory may be saved to a file in JavaScript Object Notation (JSON).</p> <p>The class Results gathers the paths to each PerformanceHistory in a benchmarking study. In practice, Results are generated by a benchmarking scenario, thanks to Benchmarker.execute.</p>"},{"location":"user_guide/#benchmarker","title":"Benchmarker","text":"<p>The gemseo_benchmark.benchmarker sub-package is responsible for the generation of the results.</p> <p>The class Benchmarker is responsible for two tasks:</p> <ol> <li>executing (possibly in parallel) the algorithms configurations on     the reference problems,</li> <li>saving the performance histories to files, and storing their paths     in Results.</li> </ol>"},{"location":"user_guide/#data-profiles","title":"Datas profiles","text":"<p>The gemseo_benchmark.data_profiles sub-package handles the computation of data profiles.</p> <p>A data profile is a graph that represents the extent to which an algorithm solves a problem (or a group of problems) for a given number of functions evaluations. To clarify this definition we need to introduce target values.</p>"},{"location":"user_guide/#target-values","title":"Target values","text":"<p>The difficulty of a problem configuration is represented by a scale of performance values, called target values, ranging from a relatively easily achievable value to the best value known. The most telling example of target value is the optimal value of the objective function. Target values can be thought as milestones on the trajectory towards the best value known.</p> <pre><code>target_values = TargetValues([10**-4, 10**-5, 10**-6, 0.0])\n</code></pre> <p>Since a sequence of target values is in fact a sequence of HistoryItems, the class TargetValues is a subclass of PerformanceHistory.</p>"},{"location":"user_guide/#targets-generator","title":"Targets generator","text":"<p>The target values of a problem can be handpicked but they can also be automatically computed with a generator of target values.</p> <p>A TargetsGenerator relies on algorithms chosen as references.</p> <ol> <li>The problem is solved with the reference algorithms from each     starting point.</li> <li>Instances of PerformanceHistory     representing the history of the best performance value (which is     decreasing) are computed, e.g.     \\(\\{\\min_{0\\leq i \\leq k} f(x_i)\\}_{0 \\leq k \\leq K}\\) where \\(f\\) is     the objective function and \\(x_k\\) are the values of the design     variables at iteration \\(k\\).</li> <li>A notion of median history is computed from these histories.</li> <li>Performance values are picked at uniform intervals in the median     history: these are the target values.</li> </ol>"},{"location":"user_guide/#data-profile","title":"Data profile","text":"<p>The data profile of an algorithm relative to a problem configuration (or a group of problem configurations) is the graph representing the ratio of target values reached by the algorithm relative to the number functions evaluations performed by the algorithm.</p> <p></p>"},{"location":"user_guide/#report","title":"Report","text":"<p>The gemseo_benchmark.report sub-package manages the automatic generation of a benchmarking report in PDF or HTML format describing:</p> <ul> <li>the algorithms configurations,</li> <li>the problem configurations,</li> <li>the results generated by the algorithms on the problems, especially     in the form of data profiles.</li> </ul>"},{"location":"user_guide/#scenario","title":"Scenario","text":"<p>The class Scenario is the highest-level class of the package: it lets the user execute the algorithms configurations on the problems and generate a benchmarking report by calling a single method.</p> <pre><code>scenario_dir = Path(\"scenario\")\nscenario_dir.mkdir()\nscenario = Scenario([lbfgsb_configurations], scenario_dir)\nresults = scenario.execute([problems_2D])\n</code></pre>"}]}