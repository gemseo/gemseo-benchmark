{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#gemseo-benchmark","title":"gemseo-benchmark","text":""},{"location":"#overview","title":"Overview","text":"<p>A GEMSEO-based package to benchmark optimization algorithms.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the latest version with <code>pip install gemseo-benchmark</code>.</p> <p>See pip for more information.</p>"},{"location":"#bugs-and-questions","title":"Bugs and questions","text":"<p>Please use the gitlab issue tracker to submit bugs or questions.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See the contributing section of GEMSEO.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Benoit Pauwels</li> <li>Antoine Dechaume</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes of this project will be documented here.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#develop","title":"Develop","text":""},{"location":"changelog/#added","title":"Added","text":""},{"location":"changelog/#data-profiles","title":"Data profiles","text":"<ul> <li>The color and marker for each algorithm configuration, as well as the grid options,   can now be customized thanks to the new arguments <code>colors</code>, <code>markers</code>   and <code>grid_kwargs</code>   (see <code>DataProfile.plot</code>, <code>Problem.compute_data_profile</code>,   and <code>ProblemsGroup.compute_data_profile</code>).</li> <li>The scale of the axis showing the number of function evaluations   can now be made logarithmic.</li> </ul>"},{"location":"changelog/#report","title":"Report","text":"<ul> <li>The color and marker for each algorithm configuration can now be customized   at the initialization of <code>Report</code> thanks to the new arguments<code>colors</code>   and <code>markers</code>.</li> <li>The user can now request that <code>Scenario.execute</code> or <code>Report.generate</code>   plot only the median of the performance measure rather than its whole range   thanks to the new boolean argument <code>plot_only_median</code>.</li> <li>On the page dedicated to the benchmarking problems,   the infeasibility measure of infeasibe target values is now displayed.</li> <li>Graphs have been added to the pages dedicated to each problem:   they show a focus on the performance measure near the target values,   the execution time,   the infeasibility measure,   and the number of unsatisfied constraints.</li> <li>Pages dedicated to the results of each algorithm configuration on each problem   have been added.   They feature graphs representing the performance measure, the infeasibility measure,   and the number of unsatisfied constraints.</li> <li>The scale of the axis showing the number of function evaluations   can now be made logarithmic.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":""},{"location":"changelog/#report_1","title":"Report","text":"<ul> <li>The results on each problem are now displayed on separate pages   rather than on the page of the problems group.</li> <li>Setting the optimum of a problem is no longer mandatory.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":""},{"location":"changelog/#results","title":"Results","text":"<ul> <li>Removing leading infeasible items from an infeasible performance history   now returns an empty performance history.</li> <li>Path options are now properly supported.</li> </ul>"},{"location":"changelog/#scenario","title":"Scenario","text":"<ul> <li>An algorithm configuration can now belong to several groups   of algorithm configurations.</li> </ul>"},{"location":"changelog/#version-300-november-2024","title":"Version 3.0.0 (November 2024)","text":""},{"location":"changelog/#added_1","title":"Added","text":""},{"location":"changelog/#benchmarker","title":"Benchmarker","text":"<ul> <li>The option <code>log_gemseo_to_file</code> has been added to <code>Benchmarker.execute</code>   and <code>Scenario.execute</code> to save the GEMSEO log of each algorithm execution   to a file in the same directory as its performance history file.</li> </ul>"},{"location":"changelog/#data-profiles_1","title":"Data profiles","text":"<ul> <li>Target values can be plotted on existing axes as horizontal lines with   <code>TargetValues.plot_on_axes</code>.</li> </ul>"},{"location":"changelog/#results_1","title":"Results","text":"<ul> <li>The distribution of a collection of performance histories can be plotted in terms of   performance measure (<code>PerformanceHistories.plot_performance_measure_distribution``),   infeasibility measure (</code>PerformanceHistories.plot_infeasibility_measure_distribution<code>)   and number of unsatisfied constraints   (```PerformanceHistories.plot_number_of_unsatisfied_constraints_distribution</code>).</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":""},{"location":"changelog/#results_2","title":"Results","text":"<ul> <li>Methods   <code>PerformanceHistory.compute_cumulated_minimum</code>,   <code>PerformanceHistory.extend</code>,   <code>PerformanceHistory.remove_leading_infeasible</code>,   and <code>PerformanceHistory.shorten</code>   preserve the attributes other than <code>PerformanceHistory.items</code>.</li> </ul>"},{"location":"changelog/#version-200-december-2023","title":"Version 2.0.0 (December 2023)","text":""},{"location":"changelog/#changed_2","title":"Changed","text":""},{"location":"changelog/#benchmarker_1","title":"Benchmarker","text":"<ul> <li>The option to automatically save the logs of pSeven has been removed   from classes <code>Scenario</code> and <code>Benchmarker</code>.   However, the user can still save these logs   by passing an instance-specific option to <code>AlgorithmConfiguration</code>   (refer to the \"Added\" section of the present changelog).   For example:   <code>instance_algorithm_options   ={\"log_path\": lambda problem, index: f\"my/log/files/{problem.name}.{index}.log\"}</code>.   N.B. the user is now responsible for the creation of the parent directories.</li> <li>Class <code>Worker</code> no longer sets <code>PerformanceHistory.doe_size</code>   to the length of the value of the pSeven option <code>\"sample_x\"</code>.   Note that this does not affect the behavior of <code>gemseo-benchmark</code>:   <code>PerformanceHistory.doe_size</code> is only used as convenience   when loading/saving a <code>PerformanceHistory</code> using a file.   In particular, the behavior of <code>Report</code> is not changed.   The user can still set the value of <code>PerformanceHistory.doe_size</code>   by themselves since it is a public attribute.</li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Support for Python 3.11.</li> </ul>"},{"location":"changelog/#algorithms","title":"Algorithms","text":"<ul> <li>Algorithm options specific to problem instances (e.g. paths for output files)   can be passed to <code>AlgorithmConfiguration</code> in the new argument <code>instance_algorithm_options</code>.</li> </ul>"},{"location":"changelog/#benchmarker_2","title":"Benchmarker","text":"<ul> <li>One can get the path to a performance history file with <code>Benchmarker.get_history_path</code>.</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Support for Python 3.8.</li> </ul>"},{"location":"changelog/#version-110-september-2023","title":"Version 1.1.0 (September 2023)","text":""},{"location":"changelog/#added_3","title":"Added","text":""},{"location":"changelog/#results_3","title":"Results","text":"<ul> <li>The names of functions and the number of variables are stored in the     performance history files.</li> </ul>"},{"location":"changelog/#report_2","title":"Report","text":"<ul> <li>The optimization histories can be displayed on a logarithmic scale.</li> </ul>"},{"location":"changelog/#scenario_1","title":"Scenario","text":"<ul> <li>The options <code>custom_algos_descriptions</code> and     <code>max_eval_number_per_group</code> of <code>Report</code>{.interpreted-text     role=\"class\"} can be passed through <code>Scenario</code>{.interpreted-text     role=\"class\"}.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":""},{"location":"changelog/#report_3","title":"Report","text":"<ul> <li>The sections of the PDF report are correctly numbered.</li> <li>The graphs of the PDF report are anchored to their expected     locations.</li> </ul>"},{"location":"changelog/#version-100-june-2023","title":"Version 1.0.0 (June 2023)","text":"<p>First version.</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>The developers thank all the open source libraries making <code>gemseo-benchmark</code> possible.</p>"},{"location":"credits/#exec-1--external-dependencies","title":"External Dependencies","text":"<p><code>gemseo-benchmark</code> depends on software with compatible licenses that are listed below.</p> Project License <code>Python</code> Python Software License <code>gemseo</code> GNU Lesser General Public License v3 <code>matplotlib</code> Python Software Foundation License <code>numpy</code> BSD License <code>sphinx</code> BSD License"},{"location":"credits/#exec-1--external-applications","title":"External applications","text":"<p>Some external applications are used by <code>gemseo-benchmark</code>, but not linked with the application, for testing, documentation generation, training or example purposes.</p> Project License <code>black</code> MIT License <code>commitizen</code> ? <code>covdefaults</code> MIT License <code>griffe-inherited-docstrings</code> ISC <code>insert-license</code> MIT <code>markdown-exec</code> ISC <code>mike</code> BSD-3-Clause <code>mkdocs-bibtex</code> BSD-3-Clause-LBNL <code>mkdocs-gallery</code> BSD 3-Clause <code>mkdocs-gen-files</code> MIT License <code>mkdocs-include-markdown-plugin</code> Apache Software License <code>mkdocs-literate-nav</code> MIT License <code>mkdocs-material</code> MIT License <code>mkdocs-section-index</code> MIT License <code>mkdocstrings</code> ISC <code>pre-commit</code> MIT License <code>pygrep-hooks</code> MIT <code>pytest</code> MIT License <code>pytest-cov</code> MIT License <code>pytest-xdist</code> MIT License <code>ruff</code> MIT License"},{"location":"licenses/","title":"Licenses","text":""},{"location":"licenses/#licenses","title":"Licenses","text":""},{"location":"licenses/#gnu-lgpl-v30","title":"GNU LGPL v3.0","text":"<p>The <code>gemseo-benchmark</code> source code is distributed under the GNU LGPL v3.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU Lesser General Public\nLicense version 3 as published by the Free Software Foundation.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with this program; if not, write to the Free Software Foundation,\nInc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n</code></pre></p>"},{"location":"licenses/#bsd-0-clause","title":"BSD 0-Clause","text":"<p>The <code>gemseo-benchmark</code> examples are distributed under the BSD 0-Clause <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under a BSD 0-Clause License.\n\nPermission to use, copy, modify, and/or distribute this software\nfor any purpose with or without fee is hereby granted.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL\nWARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\nTHE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,\nOR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING\nFROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,\nNEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION\nWITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n</code></pre></p>"},{"location":"licenses/#cc-by-sa-40","title":"CC BY-SA 4.0","text":"<p>The <code>gemseo-benchmark</code> documentation is distributed under the CC BY-SA 4.0 license. <pre><code>Copyright 2021 IRT Saint Exup\u00e9ry, https://www.irt-saintexupery.com\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0\nInternational License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-sa/4.0/ or send a letter to Creative\nCommons, PO Box 1866, Mountain View, CA 94042, USA.\n</code></pre></p>"},{"location":"generated/examples/all/","title":"all","text":""},{"location":"generated/examples/all/#examples","title":"Examples","text":"<p> Generate target values </p> <p> Compute data profiles </p> <p> Generate a benchmarking report </p> <p> Download all examples in Python source code: all_python.zip</p> <p> Download all examples in Jupyter notebooks: all_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/all/mg_execution_times/","title":"Computation times","text":"<p>00:20.065 total execution time for generated_examples_all files:</p> <p>+-----------------------------------------------------------------------------------------+-----------+--------+ | plot_report (docs/examples/all/plot_report.py)                      | 00:13.896 | 0.0 MB | +-----------------------------------------------------------------------------------------+-----------+--------+ | plot_target_values (docs/examples/all/plot_target_values.py) | 00:04.251 | 0.0 MB | +-----------------------------------------------------------------------------------------+-----------+--------+ | plot_data_profiles (docs/examples/all/plot_data_profiles.py) | 00:01.918 | 0.0 MB | +-----------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/examples/all/plot_data_profiles/","title":"Compute data profiles","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/all/plot_data_profiles/#compute-data-profiles","title":"Compute data profiles","text":"<p>In this example, we compute the data profiles of three algorithms configurations based on two reference problems.</p>"},{"location":"generated/examples/all/plot_data_profiles/#imports","title":"Imports","text":"<p>We start by making the necessary imports.</p> <pre><code>from __future__ import annotations\n\nimport shutil\nimport tempfile\nfrom pathlib import Path\n\nfrom gemseo import configure\nfrom gemseo.problems.optimization.rastrigin import Rastrigin\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n\nfrom gemseo_benchmark.algorithms.algorithm_configuration import AlgorithmConfiguration\nfrom gemseo_benchmark.algorithms.algorithms_configurations import (\n    AlgorithmsConfigurations,\n)\nfrom gemseo_benchmark.data_profiles.target_values import TargetValues\nfrom gemseo_benchmark.problems.problem import Problem\nfrom gemseo_benchmark.problems.problems_group import ProblemsGroup\nfrom gemseo_benchmark.scenario import Scenario\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#set-the-algorithms-configurations","title":"Set the algorithms configurations","text":"<p>Let us define the algorithms configurations for which we want to compute data profiles.</p> <p>For example, let us choose a configuration of the L-BFGS-B algorithm with a number of Hessian corrections limited to 2. (This option is called <code>maxcor</code>.)</p> <pre><code>lbfgsb_2_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 2 Hessian corrections\",\n    maxcor=2,\n)\n</code></pre> <p>Note:     The customized name <code>\"L-BFGS-B with 2 Hessian corrections\"</code>     will serve as label in the plot of the data profiles.</p> <p>To investigate the influence of the <code>maxcor</code> option, let us consider a different configuration of L-BFGS-B with up to 20 Hessian corrections.</p> <pre><code>lbfgsb_20_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 20 Hessian corrections\",\n    maxcor=20,\n)\n</code></pre> <p>Additionally, let us choose the SLSQP algorithm, with all its options set to their default values, to compare it against L-BFGS-B.</p> <pre><code>slsqp_default = AlgorithmConfiguration(\"SLSQP\")\n</code></pre> <p>Finally, we gather our selection of algorithms configurations in a group.</p> <pre><code>algorithms_configurations = AlgorithmsConfigurations(\n    lbfgsb_2_corrections,\n    lbfgsb_20_corrections,\n    slsqp_default,\n    name=\"Derivative-based algorithms\",\n)\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#set-the-reference-problems","title":"Set the reference problems","text":"<p>Let us choose two problems already implemented in GEMSEO as references to measure the performances of our selection of algorithms configurations: Rastrigin and Rosenbrock.</p> <p>We define target values as an exponential scale of values decreasing towards zero, the minimum value of both Rastrigin's and Rosenbrock's functions.</p> <pre><code>optimum = 0.0\ntarget_values = TargetValues([10**-i for i in range(4, 7)] + [optimum])\n</code></pre> <p>Note:     It could be preferable to customize a different scale of target values     for each problem, although we keep it simple here.</p> <p>We now have all the elements to define the benchmarking problems.</p> <pre><code>rastrigin = Problem(\n    \"Rastrigin\",\n    Rastrigin,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\nrosenbrock = Problem(\n    \"Rosenbrock\",\n    Rosenbrock,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\n</code></pre> <p>Here we configure a design of experiments (DOE) to generate five starting points by optimized Latin hypercube sampling (LHS).</p> <p>Finally, we gather our reference problems in a group.</p> <pre><code>problems = ProblemsGroup(\"Reference problems\", [rastrigin, rosenbrock])\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#generate-the-benchmarking-results","title":"Generate the benchmarking results","text":"<p>Now that the algorithms configurations and the reference problems are properly set, we can measure the performances of the former on the latter.</p> <p>We set up a Scenario with our group of algorithms configurations and a path to a directory where to save the performance histories.</p> <pre><code>scenario_dir = Path(tempfile.mkdtemp())\nscenario = Scenario([algorithms_configurations], scenario_dir)\n</code></pre> <p>Here we choose to deactivate the functions counters, progress bars and bounds check of GEMSEO to accelerate the script.</p> <pre><code>configure(\n    enable_function_statistics=False,\n    enable_progress_bar=False,\n    check_desvars_bounds=False,\n)\n</code></pre> <p>Let us execute the benchmarking scenario on our group of reference problems.</p> <p>Note:     Here we skip the generation of the report     as we only intend to compute the data profiles.</p> <pre><code>results = scenario.execute([problems], skip_report=True)\n</code></pre> <p>Out:</p> <pre><code>    INFO - 06:49:43: Run the solvers on the benchmarking problems\n    INFO - 06:49:43: Solving instance 1 of problem Rosenbrock with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 2 of problem Rosenbrock with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 3 of problem Rosenbrock with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 4 of problem Rosenbrock with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 5 of problem Rosenbrock with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 1 of problem Rastrigin with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 2 of problem Rastrigin with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 3 of problem Rastrigin with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 4 of problem Rastrigin with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 5 of problem Rastrigin with algorithm configuration L-BFGS-B with 2 Hessian corrections.\n    INFO - 06:49:43: Solving instance 1 of problem Rosenbrock with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 2 of problem Rosenbrock with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 3 of problem Rosenbrock with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 4 of problem Rosenbrock with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 5 of problem Rosenbrock with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 1 of problem Rastrigin with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 2 of problem Rastrigin with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 3 of problem Rastrigin with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 4 of problem Rastrigin with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 5 of problem Rastrigin with algorithm configuration L-BFGS-B with 20 Hessian corrections.\n    INFO - 06:49:43: Solving instance 1 of problem Rosenbrock with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 2 of problem Rosenbrock with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 3 of problem Rosenbrock with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 4 of problem Rosenbrock with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 5 of problem Rosenbrock with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 1 of problem Rastrigin with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 2 of problem Rastrigin with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 3 of problem Rastrigin with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 4 of problem Rastrigin with algorithm configuration SLSQP.\n    INFO - 06:49:43: Solving instance 5 of problem Rastrigin with algorithm configuration SLSQP.\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     |  1.41018015165714  |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | 0.2322874136456892 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 1.7415659577864148e-20\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value       | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 1.000000000055461 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 1.000000000098947 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | -0.4963499257250799 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     |  -1.401247936127537 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 1.5944751850991858e-19\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 0.9999999996096349 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 0.9999999992108659 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | 0.3172353388212383 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     |  1.23361600844826  |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 1.1905388687642983e-17\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 0.9999999968546973 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 0.9999999938512558 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | -1.780309692207495 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | 0.5009516806698295 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 3.0189671472405215e-16\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value       | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 1.000000017141322 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 1.000000033998536 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     |  1.19873656304939  |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | -1.054443857960793 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 2.112611695366286e-18\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 0.9999999989866946 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 0.9999999978691863 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.07050900758285703 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.01161437068228446 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value         | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | -9.18709552877317e-15 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 1.636191182541324e-14 |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | -0.02481749628625399 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | -0.07006239680637684 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: (5.329070518200751e-15+0j)\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value         | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | 5.237772771060634e-09 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 1.536721166983668e-09 |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.01586176694106192 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.06168080042241303 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: (3.552713678800501e-15+0j)\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | -4.663586766762151e-09 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | -1.34813063490391e-09  |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | -0.08901548461037478 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.02504758403349147  |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value         | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | 5.687117443642364e-14 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 8.851253063824061e-14 |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.05993682815246951  |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | -0.05272219289803963 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | 1.934841176165492e-13  |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | -5.358213872597162e-13 |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     |  1.41018015165714  |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | 0.2322874136456892 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 1.4199666384279484e-18\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 0.9999999988651864 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 0.9999999976940184 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | -0.4963499257250799 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     |  -1.401247936127537 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 1.7669415507599113e-18\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 0.9999999991579474 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 0.999999998213041  |      2      | float |\n    INFO - 06:49:43:          +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | 0.3172353388212383 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     |  1.23361600844826  |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 5.176346997863e-17\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value       | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 1.000000006044003 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 1.000000011697705 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | -1.780309692207495 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | 0.5009516806698295 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 2.335932799942564e-19\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value       | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 1.000000000435729 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     | 1.000000000850545 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     |  1.19873656304939  |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | -1.054443857960793 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 3.4133613871566586e-20\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |       Value       | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     | 1.000000000184744 |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     |  1.00000000036931 |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.07050900758285703 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.01161437068228446 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | -7.531047591102791e-10 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 1.308648966968917e-09  |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | -0.02481749628625399 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | -0.07006239680637684 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | -4.447812118613115e-10 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 3.258402159200813e-10  |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.01586176694106192 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.06168080042241303 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | 2.562481615786538e-10  |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | -1.689021422723869e-10 |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | -0.08901548461037478 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.02504758403349147  |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: (5.329070518200751e-15+0j)\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value         | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | 3.315589469643498e-09 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 4.988126994787123e-09 |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+-----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.05993682815246951  |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | -0.05272219289803963 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 0\n    INFO - 06:49:43:       Message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: (1.7763568394002505e-15+0j)\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | -9.535801454019222e-10 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 2.602111054406109e-09  |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     |  1.41018015165714  |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | 0.2322874136456892 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 8\n    INFO - 06:49:43:       Message: Positive directional derivative for linesearch\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0.0\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | -0.4963499257250799 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     |  -1.401247936127537 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 8\n    INFO - 06:49:43:       Message: Positive directional derivative for linesearch\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0.0\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | 0.3172353388212383 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     |  1.23361600844826  |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 8\n    INFO - 06:49:43:       Message: Positive directional derivative for linesearch\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0.0\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     | -1.780309692207495 |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | 0.5009516806698295 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 8\n    INFO - 06:49:43:       Message: Positive directional derivative for linesearch\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0.0\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize rosen(x) = sum( 100*(x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2 )\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |       Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |      -2     |  1.19873656304939  |      2      | float |\n    INFO - 06:49:43:       | x[1] |      -2     | -1.054443857960793 |      2      | float |\n    INFO - 06:49:43:       +------+-------------+--------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: 8\n    INFO - 06:49:43:       Message: Positive directional derivative for linesearch\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0.0\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound | Value | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          | x[1] |      -2     |   1   |      2      | float |\n    INFO - 06:49:43:          +------+-------------+-------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.07050900758285703 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.01161437068228446 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: None\n    INFO - 06:49:43:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | -1.199765828352461e-09 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 1.464691992691947e-09  |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | -0.02481749628625399 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | -0.07006239680637684 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: None\n    INFO - 06:49:43:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | -5.326570573505052e-10 |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | 4.589869873061758e-10  |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value        | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | 0.01586176694106192 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.06168080042241303 |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+---------------------+-------------+-------+\n    INFO - 06:49:43: Optimization result:\n    INFO - 06:49:43:    Optimizer info:\n    INFO - 06:49:43:       Status: None\n    INFO - 06:49:43:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 06:49:43:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:43:    Solution:\n    INFO - 06:49:43:       Objective: 0j\n    INFO - 06:49:43:       Design space:\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43:          | x[0] |     -0.1    | 5.618675108509663e-10  |     0.1     | float |\n    INFO - 06:49:43:          | x[1] |     -0.1    | -4.760171284923231e-10 |     0.1     | float |\n    INFO - 06:49:43:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:43: Optimization problem:\n    INFO - 06:49:43:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:43:    with respect to x\n    INFO - 06:49:43:    over the design space:\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:43:       | x[0] |     -0.1    | -0.08901548461037478 |     0.1     | float |\n    INFO - 06:49:43:       | x[1] |     -0.1    | 0.02504758403349147  |     0.1     | float |\n    INFO - 06:49:43:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:44: Optimization result:\n    INFO - 06:49:44:    Optimizer info:\n    INFO - 06:49:44:       Status: None\n    INFO - 06:49:44:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 06:49:44:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:44:    Solution:\n    INFO - 06:49:44:       Objective: 0j\n    INFO - 06:49:44:       Design space:\n    INFO - 06:49:44:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:44:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:44:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:44:          | x[0] |     -0.1    | -7.992217998520346e-14 |     0.1     | float |\n    INFO - 06:49:44:          | x[1] |     -0.1    | 4.503342143635791e-14  |     0.1     | float |\n    INFO - 06:49:44:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:44: Optimization problem:\n    INFO - 06:49:44:    minimize Rastrigin(x) = 20 + sum(x[i]**2 - 10*cos(2pi*x[i]))\n    INFO - 06:49:44:    with respect to x\n    INFO - 06:49:44:    over the design space:\n    INFO - 06:49:44:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:44:       | Name | Lower bound |        Value         | Upper bound | Type  |\n    INFO - 06:49:44:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:44:       | x[0] |     -0.1    | 0.05993682815246951  |     0.1     | float |\n    INFO - 06:49:44:       | x[1] |     -0.1    | -0.05272219289803963 |     0.1     | float |\n    INFO - 06:49:44:       +------+-------------+----------------------+-------------+-------+\n    INFO - 06:49:44: Optimization result:\n    INFO - 06:49:44:    Optimizer info:\n    INFO - 06:49:44:       Status: None\n    INFO - 06:49:44:       Message: Maximum number of iterations reached. GEMSEO stopped the driver.\n    INFO - 06:49:44:       Number of calls to the objective function by the optimizer: 0\n    INFO - 06:49:44:    Solution:\n    INFO - 06:49:44:       Objective: 0j\n    INFO - 06:49:44:       Design space:\n    INFO - 06:49:44:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:44:          | Name | Lower bound |         Value          | Upper bound | Type  |\n    INFO - 06:49:44:          +------+-------------+------------------------+-------------+-------+\n    INFO - 06:49:44:          | x[0] |     -0.1    | -4.658005925417541e-11 |     0.1     | float |\n    INFO - 06:49:44:          | x[1] |     -0.1    | -4.913201789857879e-11 |     0.1     | float |\n    INFO - 06:49:44:          +------+-------------+------------------------+-------------+-------+\n</code></pre>"},{"location":"generated/examples/all/plot_data_profiles/#compute-the-datas-profiles","title":"Compute the datas profiles","text":"<p>Now that the performances histories are generated for the reference problems, the data profiles of the algorithms configurations can be computed.</p> <pre><code>problems.compute_data_profile(algorithms_configurations, results, show=True)\n</code></pre> <p></p> <p>Here we remove the performances histories as we do not wish to keep them.</p> <pre><code>shutil.rmtree(scenario_dir)\n</code></pre> <p>Total running time of the script: ( 0 minutes  1.918 seconds)</p> <p> Download Python source code: plot_data_profiles.py</p> <p> Download Jupyter notebook: plot_data_profiles.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/all/plot_report/","title":"Generate a benchmarking report","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/all/plot_report/#generate-a-benchmarking-report","title":"Generate a benchmarking report","text":"<p>In this example, we generate a benchmarking report based on the performances of three algorithms configurations on three reference problems.</p>"},{"location":"generated/examples/all/plot_report/#imports","title":"Imports","text":"<p>We start by making the necessary imports.</p> <pre><code>from __future__ import annotations\n\nimport shutil\nimport tempfile\nfrom pathlib import Path\n\nfrom gemseo import configure\nfrom gemseo.problems.optimization.rastrigin import Rastrigin\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock\n\nfrom gemseo_benchmark.algorithms.algorithm_configuration import AlgorithmConfiguration\nfrom gemseo_benchmark.algorithms.algorithms_configurations import (\n    AlgorithmsConfigurations,\n)\nfrom gemseo_benchmark.data_profiles.target_values import TargetValues\nfrom gemseo_benchmark.problems.problem import Problem\nfrom gemseo_benchmark.problems.problems_group import ProblemsGroup\nfrom gemseo_benchmark.scenario import Scenario\n</code></pre>"},{"location":"generated/examples/all/plot_report/#set-the-algorithms-configurations","title":"Set the algorithms configurations","text":"<p>Let us define the algorithms configurations that we want to benchmark.</p> <p>For example, let us choose a configuration of the L-BFGS-B algorithm with a number of Hessian corrections limited to 2. (this option is called <code>maxcor</code>.)</p> <pre><code>lbfgsb_2_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 2 Hessian corrections\",\n    maxcor=2,\n)\n</code></pre> <p>Note:     The customized name \"L-BFGS-B with 2 Hessian corrections\"     will serve to refer to this algorithm configuration in the report.</p> <p>To investigate the influence of the <code>maxcor</code> option, let us consider a different configuration of L-BFGS-B with up to 20 Hessian corrections.</p> <pre><code>lbfgsb_20_corrections = AlgorithmConfiguration(\n    \"L-BFGS-B\",\n    \"L-BFGS-B with 20 Hessian corrections\",\n    maxcor=20,\n)\n</code></pre> <p>Let us put these two configurations of L-BFGS-B in a same group of algorithms configurations so that a section of the report will be dedicated to them.</p> <pre><code>lbfgsb_configurations = AlgorithmsConfigurations(\n    lbfgsb_2_corrections,\n    lbfgsb_20_corrections,\n    name=\"L-BFGS-B configurations\",\n)\n</code></pre> <p>Additionally, let us choose the SLSQP algorithm, with all its options set to their default values, to compare it against L-BFGS-B. Let us put it in a group of its own.</p> <pre><code>slsqp_default = AlgorithmConfiguration(\"SLSQP\")\nslsqp_configurations = AlgorithmsConfigurations(slsqp_default, name=\"SLSQP\")\n</code></pre>"},{"location":"generated/examples/all/plot_report/#set-the-reference-problems","title":"Set the reference problems","text":"<p>Let us choose two problems already implemented in GEMSEO as references to measure the performances of our selection of algorithms configurations: Rastrigin and Rosenbrock.</p> <p>We define target values as an exponential scale of values decreasing towards zero, the minimum value of both Rastrigin's and Rosenbrock's functions.</p> <pre><code>optimum = 0.0\ntarget_values = TargetValues([10**-i for i in range(4, 7)] + [optimum])\n</code></pre> <p>N.B. it could be preferable to customize a different scale of target values for each problem, although we keep it simple here.</p> <p>We now have all the elements to define the benchmarking problems.</p> <pre><code>rastrigin_2d = Problem(\n    \"Rastrigin\",\n    Rastrigin,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\nrosenbrock_2d = Problem(\n    \"Rosenbrock\",\n    Rosenbrock,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n    target_values=target_values,\n)\n</code></pre> <p>Here we configure a design of experiments (DOE) to generate five starting points by optimized Latin hypercube sampling (LHS).</p> <p>Let us gather these two two-variables problems in a group so that they will be treated together.</p> <pre><code>problems_2d = ProblemsGroup(\"2D problems\", [rastrigin_2d, rosenbrock_2d])\n</code></pre> <p>We add a five-variables problem, also based on Rosenbrock's function, to compare the algorithms configurations in higher dimension. Let us put it in a group of its own.</p> <pre><code>rosenbrock_5d = Problem(\n    \"Rosenbrock 5D\",\n    lambda: Rosenbrock(5),\n    target_values=target_values,\n    optimum=optimum,\n    doe_size=5,\n    doe_algo_name=\"OT_OPT_LHS\",\n)\nproblems_5d = ProblemsGroup(\"5D problems\", [rosenbrock_5d])\n</code></pre>"},{"location":"generated/examples/all/plot_report/#generate-the-benchmarking-results","title":"Generate the benchmarking results","text":"<p>Now that the algorithms configurations and the reference problems are properly set, we can measure the performances of the former on the latter.</p> <p>We set up a Scenario with our two groups of algorithms configurations and a path to a directory where to save the performance histories and the report.</p> <pre><code>scenario_dir = Path(tempfile.mkdtemp())\nscenario = Scenario([lbfgsb_configurations, slsqp_configurations], scenario_dir)\n</code></pre> <p>Here we choose to deactivate the functions counters, progress bars and bounds check of GEMSEO to accelerate the script.</p> <pre><code>configure(\n    enable_function_statistics=False,\n    enable_progress_bar=False,\n    check_desvars_bounds=False,\n)\n</code></pre> <p>Let us execute the benchmarking scenario on our two groups of reference problems.</p> <pre><code>scenario.execute([problems_2d, problems_5d])\n</code></pre> <p>Out:</p> <pre><code>/builds/gemseo/dev/gemseo-benchmark/src/gemseo_benchmark/report/_figures.py:596: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  artists = axes.boxplot(data, whis=(0, 100), patch_artist=True, labels=labels)\n/builds/gemseo/dev/gemseo-benchmark/src/gemseo_benchmark/report/_figures.py:596: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  artists = axes.boxplot(data, whis=(0, 100), patch_artist=True, labels=labels)\n/builds/gemseo/dev/gemseo-benchmark/src/gemseo_benchmark/report/_figures.py:596: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  artists = axes.boxplot(data, whis=(0, 100), patch_artist=True, labels=labels)\n/builds/gemseo/dev/gemseo-benchmark/src/gemseo_benchmark/report/_figures.py:596: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  artists = axes.boxplot(data, whis=(0, 100), patch_artist=True, labels=labels)\n/builds/gemseo/dev/gemseo-benchmark/src/gemseo_benchmark/report/_figures.py:596: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  artists = axes.boxplot(data, whis=(0, 100), patch_artist=True, labels=labels)\n/builds/gemseo/dev/gemseo-benchmark/src/gemseo_benchmark/report/_figures.py:596: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  artists = axes.boxplot(data, whis=(0, 100), patch_artist=True, labels=labels)\n\n&lt;gemseo_benchmark.results.results.Results object at 0x7841f14f7280&gt;\n</code></pre> <p>The root the HTML report is the following.</p> <pre><code>str((scenario_dir / \"report\" / \"_build\" / \"html\" / \"index.html\").absolute())\n</code></pre> <p>Out:</p> <pre><code>'/tmp/tmppw9m76jm/report/_build/html/index.html'\n</code></pre> <p>Here we remove the data as we do not intend to keep it.</p> <pre><code>shutil.rmtree(scenario_dir)\n</code></pre> <p>Total running time of the script: ( 0 minutes  13.896 seconds)</p> <p> Download Python source code: plot_report.py</p> <p> Download Jupyter notebook: plot_report.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/examples/all/plot_target_values/","title":"Generate target values","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/examples/all/plot_target_values/#generate-target-values","title":"Generate target values","text":"<p>In this example, we generate target values for a problem based on the performances of an algorithm configuration.</p>"},{"location":"generated/examples/all/plot_target_values/#imports","title":"Imports","text":"<p>We start by making the necessary imports.</p> <pre><code>from __future__ import annotations\n\nfrom gemseo import compute_doe\nfrom gemseo import configure\nfrom gemseo.problems.optimization.power_2 import Power2\n\nfrom gemseo_benchmark.algorithms.algorithm_configuration import AlgorithmConfiguration\nfrom gemseo_benchmark.algorithms.algorithms_configurations import (\n    AlgorithmsConfigurations,\n)\nfrom gemseo_benchmark.problems.problem import Problem\n</code></pre> <p>Let us consider the problem Power2 already implemented in GEMSEO.</p> <pre><code>problem = Problem(\n    name=\"Power2\",\n    optimization_problem_creator=Power2,\n    optimum=Power2.get_solution()[1],\n)\n</code></pre> <p>We define ten starting points by optimized Latin hypercube sampling (LHS).</p> <pre><code>design_space = problem.creator().design_space\nproblem.start_points = compute_doe(design_space, algo_name=\"OT_OPT_LHS\", n_samples=10)\n</code></pre> <p>Out:</p> <pre><code>&lt;frozen importlib._bootstrap&gt;:228: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n&lt;frozen importlib._bootstrap&gt;:228: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n&lt;frozen importlib._bootstrap&gt;:228: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n</code></pre> <p>Let use the optimizer COBYLA to generate performance histories on the problem.</p> <pre><code>algorithms_configurations = AlgorithmsConfigurations(\n    AlgorithmConfiguration(\n        \"NLOPT_COBYLA\",\n        max_iter=65,\n        eq_tolerance=1e-4,\n        ineq_tolerance=0.0,\n    )\n)\n</code></pre> <p>Here we choose to deactivate the functions counters, progress bars and bounds check of GEMSEO to accelerate the script.</p> <pre><code>configure(\n    enable_function_statistics=False,\n    enable_progress_bar=False,\n    check_desvars_bounds=False,\n)\n</code></pre> <p>Let us compute five target values for the problem. This automatic procedure has two stages:</p> <ol> <li>execution of the specified algorithms once for each of the starting points,</li> <li>automatic selection of target values based on the algorithms histories.</li> </ol> <p>These targets represent the milestones of the problem resolution.</p> <pre><code>problem.compute_targets(5, algorithms_configurations, best_target_tolerance=1e-5)\n</code></pre> <p>Out:</p> <pre><code>[(2.19302808026987, 0.0), (2.19302808026987, 0.0), (2.1922071109988592, 0.0), (2.1922071109988592, 0.0), (2.1920987389246234, 0.0)]\n</code></pre> <p>We can plot the algorithms histories used as reference for the computation of the target values, with the objective value on the vertical axis and the number of functions evaluations on the horizontal axis.</p> <pre><code>problem.targets_generator.plot_histories(problem.optimum, show=True)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 1 Axes&gt;\n</code></pre> <p>Finally, we can plot the target values: the objective value of each of the five targets is represented on the vertical axis with a marker indicating whether the target is feasible or not.</p> <pre><code>problem.target_values.plot()\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;Figure size 640x480 with 1 Axes&gt;\n</code></pre> <p>Total running time of the script: ( 0 minutes  4.251 seconds)</p> <p> Download Python source code: plot_target_values.py</p> <p> Download Jupyter notebook: plot_target_values.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>gemseo_benchmark<ul> <li>algorithms<ul> <li>algorithm_configuration</li> <li>algorithms_configurations</li> </ul> </li> <li>benchmarker<ul> <li>benchmarker</li> <li>worker</li> </ul> </li> <li>data_profiles<ul> <li>data_profile</li> <li>target_values</li> <li>targets_generator</li> </ul> </li> <li>problems<ul> <li>problem</li> <li>problems_group</li> </ul> </li> <li>report<ul> <li>conf</li> <li>report</li> </ul> </li> <li>results<ul> <li>history_item</li> <li>performance_histories</li> <li>performance_history</li> <li>results</li> </ul> </li> <li>scenario</li> </ul> </li> </ul>"},{"location":"reference/gemseo_benchmark/","title":"API documentation","text":""},{"location":"reference/gemseo_benchmark/#gemseo_benchmark","title":"gemseo_benchmark","text":"<p>Benchmarking of algorithms.</p>"},{"location":"reference/gemseo_benchmark/#gemseo_benchmark-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/#gemseo_benchmark.get_markers_cycle","title":"get_markers_cycle","text":"<pre><code>get_markers_cycle() -&gt; Iterator\n</code></pre> <p>Return the markers cycle for the plots.</p> <p>Returns:</p> <ul> <li> <code>Iterator</code>           \u2013            <p>The markers cycle.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/__init__.py</code> <pre><code>def get_markers_cycle() -&gt; Iterator:\n    \"\"\"Return the markers cycle for the plots.\n\n    Returns:\n        The markers cycle.\n    \"\"\"\n    return itertools.cycle(MARKERS)\n</code></pre>"},{"location":"reference/gemseo_benchmark/#gemseo_benchmark.join_substrings","title":"join_substrings","text":"<pre><code>join_substrings(string: str) -&gt; str\n</code></pre> <p>Join sub-strings with underscores.</p> <p>Parameters:</p> <ul> <li> <code>string</code>               (<code>str</code>)           \u2013            <p>The string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The joined sub-strings.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/__init__.py</code> <pre><code>def join_substrings(string: str) -&gt; str:\n    \"\"\"Join sub-strings with underscores.\n\n    Args:\n        string: The string.\n\n    Returns:\n        The joined sub-strings.\n    \"\"\"\n    return re.sub(r\"\\s+\", \"_\", string)\n</code></pre>"},{"location":"reference/gemseo_benchmark/scenario/","title":"Scenario","text":""},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario","title":"scenario","text":"<p>A class to implement a benchmarking scenario (solving and reporting).</p>"},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario.Scenario","title":"Scenario","text":"<pre><code>Scenario(\n    algorithms_configurations_groups: Iterable[\n        AlgorithmsConfigurations\n    ],\n    outputs_path: str | Path,\n)\n</code></pre> <p>A benchmarking scenario, including running of solvers and reporting.</p> <p>Parameters:</p> <ul> <li> <code>algorithms_configurations_groups</code>               (<code>Iterable[AlgorithmsConfigurations]</code>)           \u2013            <p>The groups of algorithms configurations to be benchmarked.</p> </li> <li> <code>outputs_path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the directory where to save the output files (histories and report).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the path to outputs directory does not exist.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/scenario.py</code> <pre><code>def __init__(\n    self,\n    algorithms_configurations_groups: Iterable[AlgorithmsConfigurations],\n    outputs_path: str | Path,\n) -&gt; None:\n    \"\"\"\n    Args:\n        algorithms_configurations_groups: The groups of algorithms configurations\n            to be benchmarked.\n        outputs_path: The path to the directory where to save the output files\n            (histories and report).\n\n    Raises:\n        ValueError: If the path to outputs directory does not exist.\n    \"\"\"  # noqa: D205, D212, D415\n    if not Path(outputs_path).is_dir():\n        msg = f\"The path to the outputs directory does not exist: {outputs_path}.\"\n        raise NotADirectoryError(msg)\n\n    self._algorithms_configurations_groups = algorithms_configurations_groups\n    self._outputs_path = Path(outputs_path).resolve()\n    self._histories_path = self._get_dir_path(self.__HISTORIES_DIRNAME)\n    self._results_path = self._outputs_path / self.__RESULTS_FILENAME\n</code></pre>"},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario.Scenario-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/scenario/#gemseo_benchmark.scenario.Scenario.execute","title":"execute","text":"<pre><code>execute(\n    problems_groups: Iterable[ProblemsGroup],\n    overwrite_histories: bool = False,\n    skip_solvers: bool = False,\n    skip_report: bool = False,\n    generate_html_report: bool = True,\n    generate_pdf_report: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    save_databases: bool = False,\n    number_of_processes: int = 1,\n    use_threading: bool = False,\n    custom_algos_descriptions: (\n        Mapping[str, str] | None\n    ) = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_all_histories: bool = True,\n    use_log_scale: bool = False,\n    log_gemseo_to_file: bool = False,\n    directory_path: Path | None = None,\n    plot_only_median: bool = False,\n    use_evaluation_log_scale: bool = False,\n) -&gt; Results\n</code></pre> <p>Execute the benchmarking scenario.</p> <p>Parameters:</p> <ul> <li> <code>problems_groups</code>               (<code>Iterable[ProblemsGroup]</code>)           \u2013            <p>The groups of benchmarking problems.</p> </li> <li> <code>overwrite_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite the performance histories.</p> </li> <li> <code>skip_solvers</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to skip the running of solvers.</p> </li> <li> <code>skip_report</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to skip the generation of the report.</p> </li> <li> <code>generate_html_report</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate the report in HTML format.</p> </li> <li> <code>generate_pdf_report</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to generate the report in PDF format.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>save_databases</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the databases of the optimizations.</p> </li> <li> <code>number_of_processes</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The maximum number of simultaneous threads or processes used to parallelize the execution.</p> </li> <li> <code>use_threading</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use threads instead of processes to parallelize the execution.</p> </li> <li> <code>custom_algos_descriptions</code>               (<code>Mapping[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom descriptions of the algorithms, to be printed in the report instead of the default ones coded in GEMSEO.</p> </li> <li> <code>max_eval_number_per_group</code>               (<code>dict[str, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum evaluations numbers to be displayed on the graphs of each group. The keys are the groups names and the values are the maximum evaluations numbers for the graphs of the group. If <code>None</code>, all the evaluations are displayed. If the key of a group is missing, all the evaluations are displayed for the group.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale on the value axis.</p> </li> <li> <code>log_gemseo_to_file</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the GEMSEO log to a file next to the performance history file.</p> </li> <li> <code>directory_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the directory where the report will be generated.</p> </li> <li> <code>plot_only_median</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot only the median and no other centile.</p> </li> <li> <code>use_evaluation_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the number of function evaluations axis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Results</code>           \u2013            <p>The performance histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/scenario.py</code> <pre><code>def execute(\n    self,\n    problems_groups: Iterable[ProblemsGroup],\n    overwrite_histories: bool = False,\n    skip_solvers: bool = False,\n    skip_report: bool = False,\n    generate_html_report: bool = True,\n    generate_pdf_report: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    save_databases: bool = False,\n    number_of_processes: int = 1,\n    use_threading: bool = False,\n    custom_algos_descriptions: Mapping[str, str] | None = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_all_histories: bool = True,\n    use_log_scale: bool = False,\n    log_gemseo_to_file: bool = False,\n    directory_path: Path | None = None,\n    plot_only_median: bool = False,\n    use_evaluation_log_scale: bool = False,\n) -&gt; Results:\n    \"\"\"Execute the benchmarking scenario.\n\n    Args:\n        problems_groups: The groups of benchmarking problems.\n        overwrite_histories: Whether to overwrite the performance histories.\n        skip_solvers: Whether to skip the running of solvers.\n        skip_report: Whether to skip the generation of the report.\n        generate_html_report: Whether to generate the report in HTML format.\n        generate_pdf_report: Whether to generate the report in PDF format.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        save_databases: Whether to save the databases of the optimizations.\n        number_of_processes: The maximum number of simultaneous threads or\n            processes used to parallelize the execution.\n        use_threading: Whether to use threads instead of processes\n            to parallelize the execution.\n        custom_algos_descriptions: Custom descriptions of the algorithms,\n            to be printed in the report instead of the default ones coded in GEMSEO.\n        max_eval_number_per_group: The maximum evaluations numbers to be displayed\n            on the graphs of each group.\n            The keys are the groups names and the values are the maximum\n            evaluations numbers for the graphs of the group.\n            If ``None``, all the evaluations are displayed.\n            If the key of a group is missing, all the evaluations are displayed\n            for the group.\n        plot_all_histories: Whether to plot all the performance histories.\n        use_log_scale: Whether to use a logarithmic scale on the value axis.\n        log_gemseo_to_file: Whether to save the GEMSEO log to a file\n            next to the performance history file.\n        directory_path: The path to the directory where the report\n            will be generated.\n        plot_only_median: Whether to plot only the median and no other centile.\n        use_evaluation_log_scale: Whether to use a logarithmic scale\n            for the number of function evaluations axis.\n\n    Returns:\n        The performance histories.\n    \"\"\"\n    if not skip_solvers:\n        LOGGER.info(\"Run the solvers on the benchmarking problems\")\n        self._run_solvers(\n            problems_groups,\n            overwrite_histories,\n            save_databases,\n            number_of_processes,\n            use_threading,\n            log_gemseo_to_file,\n        )\n\n    if not skip_report:\n        LOGGER.info(\"Generate the benchmarking report\")\n        self.__generate_report(\n            problems_groups,\n            generate_html_report,\n            generate_pdf_report,\n            infeasibility_tolerance,\n            custom_algos_descriptions,\n            max_eval_number_per_group,\n            plot_all_histories,\n            use_log_scale,\n            directory_path,\n            plot_only_median,\n            use_evaluation_log_scale,\n        )\n\n    return Results(self._results_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/","title":"Algorithms","text":""},{"location":"reference/gemseo_benchmark/algorithms/#gemseo_benchmark.algorithms","title":"algorithms","text":"<p>The configurations of the algorithms from their options.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/","title":"Algorithm configuration","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration","title":"algorithm_configuration","text":"<p>Configuration of an algorithm defined by the values of its options.</p> <p>An algorithm depends on the values of its options. A value set defines a configuration of the algorithm.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration","title":"AlgorithmConfiguration","text":"<pre><code>AlgorithmConfiguration(\n    algorithm_name: str,\n    configuration_name: str | None = None,\n    instance_algorithm_options: InstanceAlgorithmOptions = READ_ONLY_EMPTY_DICT,\n    **algorithm_options: Any\n)\n</code></pre> <p>The configuration of an algorithm.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> <li> <code>configuration_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the configuration of the algorithm. If <code>None</code>, a name will be generated based on the algorithm name and its options, based on the pattern <code>\"algorithm_name[option_name=option_value, ...]\"</code>.</p> </li> <li> <code>instance_algorithm_options</code>               (<code>InstanceAlgorithmOptions</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The options of the algorithm specific to instances of a problem. They shall be passed as a mapping that links the name of an algorithm option to a callable that takes the 0-based index of the instance as argument and returns the value of the option.</p> </li> <li> <code>**algorithm_options</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>The options of the algorithm.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>def __init__(\n    self,\n    algorithm_name: str,\n    configuration_name: str | None = None,\n    instance_algorithm_options: InstanceAlgorithmOptions = READ_ONLY_EMPTY_DICT,\n    **algorithm_options: Any,\n) -&gt; None:\n    \"\"\"\n    Args:\n        algorithm_name: The name of the algorithm.\n        configuration_name: The name of the configuration of the algorithm.\n            If ``None``, a name will be generated based on the algorithm name and\n            its options, based on the pattern\n            ``\"algorithm_name[option_name=option_value, ...]\"``.\n        instance_algorithm_options: The options of the algorithm specific to\n            instances of a problem.\n            They shall be passed as a mapping\n            that links the name of an algorithm option\n            to a callable that takes the 0-based index of the instance as argument\n            and returns the value of the option.\n        **algorithm_options: The options of the algorithm.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__algorithm_name = algorithm_name\n    self.__algorithm_options = algorithm_options\n    self.__configuration_name = configuration_name or self.__get_configuration_name(\n        algorithm_name, **algorithm_options\n    )\n    self.__instance_algorithm_options = instance_algorithm_options\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.algorithm_name","title":"algorithm_name  <code>property</code>","text":"<pre><code>algorithm_name: str\n</code></pre> <p>The name of the algorithm.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.algorithm_options","title":"algorithm_options  <code>property</code>","text":"<pre><code>algorithm_options: dict[str, Any]\n</code></pre> <p>The options of the algorithm.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.instance_algorithm_options","title":"instance_algorithm_options  <code>property</code>","text":"<pre><code>instance_algorithm_options: InstanceAlgorithmOptions\n</code></pre> <p>The instance-specific options of the algorithm.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the algorithm configuration.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.copy","title":"copy","text":"<pre><code>copy() -&gt; AlgorithmConfiguration\n</code></pre> <p>Return a copy of the algorithm configuration.</p> <p>Returns:</p> <ul> <li> <code>AlgorithmConfiguration</code>           \u2013            <p>A copy of the algorithm configuration.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>def copy(self) -&gt; AlgorithmConfiguration:\n    \"\"\"Return a copy of the algorithm configuration.\n\n    Returns:\n        A copy of the algorithm configuration.\n    \"\"\"\n    return AlgorithmConfiguration(\n        self.algorithm_name,\n        self.name,\n        self.instance_algorithm_options,\n        **self.algorithm_options,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    algorithm_configuration: dict[\n        str, str | dict[str, Any]\n    ],\n) -&gt; AlgorithmConfiguration\n</code></pre> <p>Load an algorithm configuration from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>dict[str, str | dict[str, Any]]</code>)           \u2013            <p>The algorithm configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AlgorithmConfiguration</code>           \u2013            <p>The algorithm configuration.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls, algorithm_configuration: dict[str, str | dict[str, Any]]\n) -&gt; AlgorithmConfiguration:\n    \"\"\"Load an algorithm configuration from a dictionary.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n\n    Returns:\n        The algorithm configuration.\n    \"\"\"\n    return AlgorithmConfiguration(\n        algorithm_configuration[cls.__ALGORITHM_NAME],\n        algorithm_configuration[cls.__CONFIGURATION_NAME],\n        algorithm_configuration.get(cls.__INSTANCE_ALGORITHM_OPTIONS, {}),\n        **algorithm_configuration[cls.__ALGORITHM_OPTIONS],\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithm_configuration/#gemseo_benchmark.algorithms.algorithm_configuration.AlgorithmConfiguration.to_dict","title":"to_dict","text":"<pre><code>to_dict(\n    skip_instance_algorithm_options: bool = False,\n) -&gt; dict[str, str | dict[str, Any]]\n</code></pre> <p>Return the algorithm configuration as a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>skip_instance_algorithm_options</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to skip the algorithm options specific to problem instances.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str | dict[str, Any]]</code>           \u2013            <p>The algorithm configuration as a dictionary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithm_configuration.py</code> <pre><code>def to_dict(\n    self, skip_instance_algorithm_options: bool = False\n) -&gt; dict[str, str | dict[str, Any]]:\n    \"\"\"Return the algorithm configuration as a dictionary.\n\n    Args:\n        skip_instance_algorithm_options: Whether to skip the algorithm options\n            specific to problem instances.\n\n    Returns:\n        The algorithm configuration as a dictionary.\n    \"\"\"\n    dictionary = {\n        self.__CONFIGURATION_NAME: self.__configuration_name,\n        self.__ALGORITHM_NAME: self.__algorithm_name,\n        self.__ALGORITHM_OPTIONS: self.__make_json_serializable(\n            self.__algorithm_options\n        ),\n    }\n    if not skip_instance_algorithm_options:\n        dictionary[self.__INSTANCE_ALGORITHM_OPTIONS] = (\n            self.__instance_algorithm_options\n        )\n\n    return dictionary\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/","title":"Algorithms configurations","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations","title":"algorithms_configurations","text":"<p>A collection of algorithms configurations.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations","title":"AlgorithmsConfigurations","text":"<pre><code>AlgorithmsConfigurations(\n    *algorithms_configurations: AlgorithmConfiguration,\n    name: str = \"\"\n)\n</code></pre> <p>               Bases: <code>MutableSet[AlgorithmConfiguration]</code></p> <p>A collection of algorithms configurations.</p> <p>Parameters:</p> <ul> <li> <code>*algorithms_configurations</code>               (<code>AlgorithmConfiguration</code>, default:                   <code>()</code> )           \u2013            <p>The algorithms configurations.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithms_configurations.py</code> <pre><code>def __init__(\n    self, *algorithms_configurations: AlgorithmConfiguration, name: str = \"\"\n) -&gt; None:\n    \"\"\"\n    Args:\n        *algorithms_configurations: The algorithms configurations.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__algorithms = []\n    self.__configurations = []\n    self.__name = name\n    self.__names = []\n    for configuration in algorithms_configurations:\n        self.add(configuration)\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.algorithms","title":"algorithms  <code>property</code>","text":"<pre><code>algorithms: list[str]\n</code></pre> <p>The names of the algorithms in alphabetical order.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.configurations","title":"configurations  <code>property</code>","text":"<pre><code>configurations: list[AlgorithmConfiguration]\n</code></pre> <p>The algorithms configurations.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the collection of algorithms configurations.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the collection of algorithms configurations has no name.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>The names of the algorithms configurations in alphabetical order.</p>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.add","title":"add","text":"<pre><code>add(\n    algorithm_configuration: AlgorithmConfiguration,\n) -&gt; None\n</code></pre> <p>Add an algorithm configuration to the collection.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the collection already contains an algorithm configuration with the same name.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithms_configurations.py</code> <pre><code>def add(self, algorithm_configuration: AlgorithmConfiguration) -&gt; None:\n    \"\"\"Add an algorithm configuration to the collection.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n\n    Raises:\n        ValueError: If the collection already contains an algorithm configuration\n            with the same name.\n    \"\"\"\n    if algorithm_configuration in self:\n        msg = (\n            \"The collection already contains an algorithm configuration named \"\n            f\"{algorithm_configuration.name}.\"\n        )\n        raise ValueError(msg)\n\n    index = bisect.bisect(self.__names, algorithm_configuration.name)\n    self.__configurations.insert(index, algorithm_configuration)\n    bisect.insort(self.__names, algorithm_configuration.name)\n    bisect.insort(self.__algorithms, algorithm_configuration.algorithm_name)\n</code></pre>"},{"location":"reference/gemseo_benchmark/algorithms/algorithms_configurations/#gemseo_benchmark.algorithms.algorithms_configurations.AlgorithmsConfigurations.discard","title":"discard","text":"<pre><code>discard(\n    algorithm_configuration: AlgorithmConfiguration,\n) -&gt; None\n</code></pre> <p>Remove an algorithm configuration.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration to remove.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/algorithms/algorithms_configurations.py</code> <pre><code>def discard(self, algorithm_configuration: AlgorithmConfiguration) -&gt; None:\n    \"\"\"Remove an algorithm configuration.\n\n    Args:\n        algorithm_configuration: The algorithm configuration to remove.\n    \"\"\"\n    self.__configurations.remove(algorithm_configuration)\n    self.__names.remove(algorithm_configuration.name)\n    if algorithm_configuration.algorithm_name not in [\n        algo_config.algorithm_name for algo_config in self\n    ]:\n        self.__algorithms.remove(algorithm_configuration.algorithm_name)\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/","title":"Benchmarker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/#gemseo_benchmark.benchmarker","title":"benchmarker","text":"<p>A benchmarker to run algorithms on problems.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/","title":"Benchmarker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker","title":"benchmarker","text":"<p>A benchmarker of optimization algorithms on reference problems.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker","title":"Benchmarker","text":"<pre><code>Benchmarker(\n    histories_path: Path,\n    results_path: Path | None = None,\n    databases_path: Path | None = None,\n)\n</code></pre> <p>A benchmarker of optimization algorithms on reference problems.</p> <p>Parameters:</p> <ul> <li> <code>histories_path</code>               (<code>Path</code>)           \u2013            <p>The path to the directory where to save the performance histories.</p> </li> <li> <code>results_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the file for saving the performance histories paths. If exists, the file is updated with the new performance histories paths.</p> </li> <li> <code>databases_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the destination directory for the databases. If <code>None</code>, the databases will not be saved.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/benchmarker.py</code> <pre><code>def __init__(\n    self,\n    histories_path: Path,\n    results_path: Path | None = None,\n    databases_path: Path | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        histories_path: The path to the directory where to save the performance\n            histories.\n        results_path: The path to the file for saving the performance histories\n            paths.\n            If exists, the file is updated with the new performance histories paths.\n        databases_path: The path to the destination directory for the databases.\n            If ``None``, the databases will not be saved.\n    \"\"\"  # noqa: D205, D212, D415\n    self._databases_path = databases_path\n    self.__histories_path = histories_path\n    self.__optimizers_factory = OptimizationLibraryFactory()\n    self.__is_algorithm_available = self.__optimizers_factory.is_available\n    self.__results_path = results_path\n    if results_path is not None and results_path.is_file():\n        self._results = Results(results_path)\n    else:\n        self._results = Results()\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker.execute","title":"execute","text":"<pre><code>execute(\n    problems: Iterable[Problem],\n    algorithms: AlgorithmsConfigurations,\n    overwrite_histories: bool = False,\n    number_of_processes: int = 1,\n    use_threading: bool = False,\n    log_gemseo_to_file: bool = False,\n) -&gt; Results\n</code></pre> <p>Run optimization algorithms on reference problems.</p> <p>Parameters:</p> <ul> <li> <code>problems</code>               (<code>Iterable[Problem]</code>)           \u2013            <p>The benchmarking problems.</p> </li> <li> <code>algorithms</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>overwrite_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite the existing performance histories.</p> </li> <li> <code>number_of_processes</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The maximum simultaneous number of threads or processes used to parallelize the execution.</p> </li> <li> <code>use_threading</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use threads instead of processes to parallelize the execution.</p> </li> <li> <code>log_gemseo_to_file</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the GEMSEO log to a file next to the performance history file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Results</code>           \u2013            <p>The results of the optimization.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm is not available.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/benchmarker.py</code> <pre><code>def execute(\n    self,\n    problems: Iterable[Problem],\n    algorithms: AlgorithmsConfigurations,\n    overwrite_histories: bool = False,\n    number_of_processes: int = 1,\n    use_threading: bool = False,\n    log_gemseo_to_file: bool = False,\n) -&gt; Results:\n    \"\"\"Run optimization algorithms on reference problems.\n\n    Args:\n        problems: The benchmarking problems.\n        algorithms: The algorithms configurations.\n        overwrite_histories: Whether to overwrite the existing performance\n            histories.\n        number_of_processes: The maximum simultaneous number of threads or\n            processes used to parallelize the execution.\n        use_threading: Whether to use threads instead of processes\n            to parallelize the execution.\n        log_gemseo_to_file: Whether to save the GEMSEO log to a file\n            next to the performance history file.\n\n    Returns:\n        The results of the optimization.\n\n    Raises:\n        ValueError: If the algorithm is not available.\n    \"\"\"\n    # Prepare the inputs of the benchmarking workers\n    inputs = []\n    for algorithm_configuration in [config.copy() for config in algorithms]:\n        algorithm_name = algorithm_configuration.algorithm_name\n        if not self.__is_algorithm_available(algorithm_name):\n            msg = f\"The algorithm is not available: {algorithm_name}.\"\n            raise ValueError(msg)\n\n        self.__disable_stopping_criteria(algorithm_configuration)\n        for problem in problems:\n            for problem_instance_index, problem_instance in enumerate(problem):\n                if self.__skip_instance(\n                    algorithm_configuration,\n                    problem,\n                    problem_instance_index,\n                    overwrite_histories,\n                ):\n                    continue\n\n                if log_gemseo_to_file:\n                    log_path = self.get_history_path(\n                        algorithm_configuration,\n                        problem.name,\n                        problem_instance_index,\n                    ).with_suffix(\".log\")\n                else:\n                    log_path = None\n\n                inputs.append((\n                    self.__set_instance_algorithm_options(\n                        algorithm_configuration,\n                        problem,\n                        problem_instance_index,\n                    ),\n                    problem,\n                    problem_instance,\n                    problem_instance_index,\n                    log_path,\n                ))\n\n    if inputs:\n        worker = Worker(self._HISTORY_CLASS)\n        if number_of_processes == 1:\n            for worker_inputs in inputs:\n                self.__worker_callback(0, worker(worker_inputs))\n        else:\n            CallableParallelExecution(\n                [worker],\n                number_of_processes,\n                use_threading,\n            ).execute(inputs, self.__worker_callback)\n\n    return self._results\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker.Benchmarker.get_history_path","title":"get_history_path","text":"<pre><code>get_history_path(\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_name: str,\n    index: int,\n    make_parents: bool = False,\n) -&gt; Path\n</code></pre> <p>Return a path for a history file.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration</code>)           \u2013            <p>The algorithm configuration.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The index of the problem instance.</p> </li> <li> <code>make_parents</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to make the parent directories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>           \u2013            <p>The path for the history file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/benchmarker.py</code> <pre><code>def get_history_path(\n    self,\n    algorithm_configuration: AlgorithmConfiguration,\n    problem_name: str,\n    index: int,\n    make_parents: bool = False,\n) -&gt; Path:\n    \"\"\"Return a path for a history file.\n\n    Args:\n        algorithm_configuration: The algorithm configuration.\n        problem_name: The name of the problem.\n        index: The index of the problem instance.\n        make_parents: Whether to make the parent directories.\n\n    Returns:\n        The path for the history file.\n    \"\"\"\n    return self._get_path(\n        self.__histories_path,\n        algorithm_configuration,\n        problem_name,\n        index,\n        \"json\",\n        make_parents=make_parents,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/benchmarker/benchmarker/#gemseo_benchmark.benchmarker.benchmarker-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/benchmarker/worker/","title":"Worker","text":""},{"location":"reference/gemseo_benchmark/benchmarker/worker/#gemseo_benchmark.benchmarker.worker","title":"worker","text":"<p>A class to implement a benchmarking worker.</p>"},{"location":"reference/gemseo_benchmark/benchmarker/worker/#gemseo_benchmark.benchmarker.worker-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/benchmarker/worker/#gemseo_benchmark.benchmarker.worker.Worker","title":"Worker","text":"<pre><code>Worker(\n    history_class: type[\n        PerformanceHistory\n    ] = PerformanceHistory,\n)\n</code></pre> <p>A benchmarking worker.</p> <p>Parameters:</p> <ul> <li> <code>history_class</code>               (<code>type[PerformanceHistory]</code>, default:                   <code>PerformanceHistory</code> )           \u2013            <p>The class of performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/benchmarker/worker.py</code> <pre><code>def __init__(\n    self, history_class: type[PerformanceHistory] = PerformanceHistory\n) -&gt; None:\n    \"\"\"\n    Args:\n        history_class: The class of performance history.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__history_class = history_class\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/","title":"Data profiles","text":""},{"location":"reference/gemseo_benchmark/data_profiles/#gemseo_benchmark.data_profiles","title":"data_profiles","text":"<p>Computation of data profiles for algorithms comparison.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/","title":"Data profile","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile","title":"data_profile","text":"<p>Class to compute data profiles for algorithms comparison.</p> <p>A data profile is a graphical tool to compare iterative algorithms, e.g. optimization algorithms or root-finding algorithms, on reference problems.</p> <p>Each of the reference problems must be assigned targets, i.e. values of the objective function or values of the residual norm, ranging from a first acceptable value to the best known value for the problem.</p> <p>The algorithms will be compared based on the number of targets they reach, cumulated over all the reference problems, relative to the number of problems functions evaluations they make.</p> <p>The data profile is the empirical cumulated distribution function of the number of functions evaluations made by an algorithm to reach a problem target.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile","title":"DataProfile","text":"<pre><code>DataProfile(target_values: Mapping[str, TargetValues])\n</code></pre> <p>Data profile that compares iterative algorithms on reference problems.</p> <p>A data profile is an empirical cumulative distribution function of the number of problems functions evaluations required by an algorithm to reach a reference problem target.</p> <p>Parameters:</p> <ul> <li> <code>target_values</code>               (<code>Mapping[str, TargetValues]</code>)           \u2013            <p>The target values of each of the reference problems.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def __init__(self, target_values: Mapping[str, TargetValues]) -&gt; None:\n    \"\"\"\n    Args:\n        target_values: The target values of each of the reference problems.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__targets_number = 0\n    self.target_values = target_values\n    self.__values_histories = {}\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.target_values","title":"target_values  <code>property</code> <code>writable</code>","text":"<pre><code>target_values: dict[str, TargetValues]\n</code></pre> <p>The target values of each reference problem.</p> <p>Target values are a scale of objective function values, ranging from an easily achievable one to the best known value. A data profile is computed by counting the number of targets reached by an algorithm at each iteration.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>if the target values are not passed as a dictionary.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the reference problems have different numbers of target values.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.add_history","title":"add_history","text":"<pre><code>add_history(\n    problem_name: str,\n    algorithm_configuration_name: str,\n    objective_values: Sequence[float],\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n) -&gt; None\n</code></pre> <p>Add a history of performance values.</p> <p>Parameters:</p> <ul> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> <li> <code>algorithm_configuration_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration.</p> </li> <li> <code>objective_values</code>               (<code>Sequence[float]</code>)           \u2013            <p>A history of objective values. N.B. the value at index <code>i</code> is assumed to have been obtained with <code>i+1</code> evaluations.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of infeasibility measures. If <code>None</code> then measures are set to zero in case of feasibility and set to infinity otherwise.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of (boolean) feasibility statuses. If <code>None</code> then feasibility is always assumed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the problem name is not the name of a reference problem.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def add_history(\n    self,\n    problem_name: str,\n    algorithm_configuration_name: str,\n    objective_values: Sequence[float],\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n) -&gt; None:\n    \"\"\"Add a history of performance values.\n\n    Args:\n        problem_name: The name of the problem.\n        algorithm_configuration_name: The name of the algorithm configuration.\n        objective_values: A history of objective values.\n            N.B. the value at index ``i`` is assumed to have been obtained with\n            ``i+1`` evaluations.\n        infeasibility_measures: A history of infeasibility measures.\n            If ``None`` then measures are set to zero in case of feasibility and set\n            to infinity otherwise.\n        feasibility_statuses: A history of (boolean) feasibility statuses.\n            If ``None`` then feasibility is always assumed.\n\n    Raises:\n        ValueError: If the problem name is not the name of a reference problem.\n    \"\"\"\n    if problem_name not in self.__target_values:\n        msg = f\"{problem_name!r} is not the name of a reference problem\"\n        raise ValueError(msg)\n    if algorithm_configuration_name not in self.__values_histories:\n        self.__values_histories[algorithm_configuration_name] = {\n            pb_name: [] for pb_name in self.__target_values\n        }\n    history = PerformanceHistory(\n        objective_values, infeasibility_measures, feasibility_statuses\n    )\n    self.__values_histories[algorithm_configuration_name][problem_name].append(\n        history\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.compute_data_profiles","title":"compute_data_profiles","text":"<pre><code>compute_data_profiles(\n    *algo_names: str,\n) -&gt; dict[str, list[Number]]\n</code></pre> <p>Compute the data profiles of the required algorithms.</p> <p>For each algorithm, compute the cumulative distribution function of the number of evaluations required by the algorithm to reach a reference target.</p> <p>Parameters:</p> <ul> <li> <code>algo_names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>The names of the algorithms. If <code>None</code> then all the algorithms are considered.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list[Number]]</code>           \u2013            <p>The data profiles.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def compute_data_profiles(self, *algo_names: str) -&gt; dict[str, list[Number]]:\n    \"\"\"Compute the data profiles of the required algorithms.\n\n    For each algorithm, compute the cumulative distribution function of the number\n    of evaluations required by the algorithm to reach a reference target.\n\n    Args:\n        algo_names: The names of the algorithms.\n            If ``None`` then all the algorithms are considered.\n\n    Returns:\n        The data profiles.\n    \"\"\"\n    data_profiles = {}\n    if not algo_names:\n        algo_names = self.__values_histories.keys()\n\n    for name in algo_names:\n        total_hits_history = self.__compute_hits_history(name)\n        problems_number = len(self.__target_values)\n        repeat_number = self.__get_repeat_number(name)\n        targets_total = self.__targets_number * problems_number * repeat_number\n        ratios = total_hits_history / targets_total\n        data_profiles[name] = ratios.tolist()\n    return data_profiles\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/data_profile/#gemseo_benchmark.data_profiles.data_profile.DataProfile.plot","title":"plot","text":"<pre><code>plot(\n    algo_names: Iterable[str] | None = None,\n    show: bool = True,\n    file_path: str | Path | None = None,\n    markevery: MarkeveryType | None = None,\n    plot_kwargs: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_kwargs: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Plot the data profiles of the required algorithms.</p> <p>Parameters:</p> <ul> <li> <code>algo_names</code>               (<code>Iterable[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>The names of the algorithms. If <code>None</code> then all the algorithms are considered.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, show the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path where to save the plot. If <code>None</code>, the plot is not saved.</p> </li> <li> <code>markevery</code>               (<code>MarkeveryType | None</code>, default:                   <code>None</code> )           \u2013            <p>The sampling parameter for the markers of the plot. Refer to the Matplotlib documentation.</p> </li> <li> <code>plot_kwargs</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_kwargs</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_evaluation_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the number of function evaluations axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/data_profile.py</code> <pre><code>def plot(\n    self,\n    algo_names: Iterable[str] | None = None,\n    show: bool = True,\n    file_path: str | Path | None = None,\n    markevery: MarkeveryType | None = None,\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_kwargs: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Plot the data profiles of the required algorithms.\n\n    Args:\n        algo_names: The names of the algorithms.\n            If ``None`` then all the algorithms are considered.\n        show: If True, show the plot.\n        file_path: The path where to save the plot.\n            If ``None``, the plot is not saved.\n        markevery: The sampling parameter for the markers of the plot.\n            Refer to the Matplotlib documentation.\n        plot_kwargs: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_kwargs: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_evaluation_log_scale: Whether to use a logarithmic scale\n            for the number of function evaluations axis.\n    \"\"\"\n    if algo_names is None:\n        algo_names = ()\n\n    data_profiles = self.compute_data_profiles(*algo_names)\n    plot_kwargs_copy = plot_kwargs.copy()\n    for kwargs in plot_kwargs_copy.values():\n        if \"markevery\" not in kwargs:\n            kwargs[\"markevery\"] = markevery\n\n    figure = self._plot_data_profiles(\n        data_profiles, plot_kwargs_copy, grid_kwargs, use_evaluation_log_scale\n    )\n    save_show_figure(figure, show, file_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/","title":"Target values","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values","title":"target_values","text":"<p>Computation of target values out of algorithms performance histories.</p> <p>Consider a problem to be solved by an iterative algorithm, e.g. an optimization problem or a root-finding problem. Targets are values, i.e. values of the objective function or values of the residual norm, ranging from a first acceptable value to the best known value for the problem. Targets are used to estimate the efficiency (relative to the number of problem functions evaluations) of an algorithm to solve a problem (or several) and computes its data profile (see :mod:<code>.data_profiles.data_profile</code>).</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues","title":"TargetValues","text":"<pre><code>TargetValues(\n    objective_values: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    n_unsatisfied_constraints: Sequence[int] | None = None,\n    problem_name: str | None = None,\n    objective_name: str | None = None,\n    constraints_names: Sequence[str] | None = None,\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: (\n        AlgorithmConfiguration | None\n    ) = None,\n    number_of_variables: int | None = None,\n)\n</code></pre> <p>               Bases: <code>PerformanceHistory</code></p> <p>Target values of a problem.</p> <p>Parameters:</p> <ul> <li> <code>objective_values</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of the quantity to be minimized. If <code>None</code>, will be considered empty.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of infeasibility measures. An infeasibility measure is a non-negative real number representing the gap between the design and the feasible space, a zero value meaning feasibility. If <code>None</code> and <code>feasibility_statuses</code> is not None then the infeasibility measures are set to zero in case of feasibility, and set to infinity otherwise. If <code>None</code> and <code>feasibility_statuses</code> is None then every infeasibility measure is set to zero.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of the (boolean) feasibility statuses. If <code>infeasibility_measures</code> is not None then <code>feasibility_statuses</code> is disregarded. If <code>None</code> and 'infeasibility_measures' is None then every infeasibility measure is set to zero.</p> </li> <li> <code>n_unsatisfied_constraints</code>               (<code>Sequence[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of the number of unsatisfied constraints. If <code>None</code>, the entries will be set to 0 for feasible entries and None for infeasible entries.</p> </li> <li> <code>problem_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the problem. If <code>None</code>, it will not be set.</p> </li> <li> <code>objective_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the objective function. If <code>None</code>, it will not be set.</p> </li> <li> <code>constraints_names</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>The names the scalar constraints. Each name must correspond to a scalar value. If <code>None</code>, it will not be set.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the initial design of experiments. If <code>None</code>, it will not be set.</p> </li> <li> <code>total_time</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The total time of the optimization, in seconds. If <code>None</code>, it will not be set.</p> </li> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the algorithm which generated the history. If <code>None</code>, it will not be set.</p> </li> <li> <code>number_of_variables</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of optimization variables. If <code>None</code>, it will not be set.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the lengths of the histories do not match.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def __init__(\n    self,\n    objective_values: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    n_unsatisfied_constraints: Sequence[int] | None = None,\n    problem_name: str | None = None,\n    objective_name: str | None = None,\n    constraints_names: Sequence[str] | None = None,\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: AlgorithmConfiguration | None = None,\n    number_of_variables: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        objective_values: The history of the quantity to be minimized.\n            If ``None``, will be considered empty.\n        infeasibility_measures: The history of infeasibility measures.\n            An infeasibility measure is a non-negative real number representing\n            the gap between the design and the feasible space,\n            a zero value meaning feasibility.\n            If ``None`` and `feasibility_statuses` is not None\n            then the infeasibility measures are set to zero in case of feasibility,\n            and set to infinity otherwise.\n            If ``None`` and `feasibility_statuses` is None\n            then every infeasibility measure is set to zero.\n        feasibility_statuses: The history of the (boolean) feasibility statuses.\n            If `infeasibility_measures` is not None then `feasibility_statuses` is\n            disregarded.\n            If ``None`` and 'infeasibility_measures' is None\n            then every infeasibility measure is set to zero.\n        n_unsatisfied_constraints: The history of the number of unsatisfied\n            constraints.\n            If ``None``, the entries will be set to 0 for feasible entries\n            and None for infeasible entries.\n        problem_name: The name of the problem.\n            If ``None``, it will not be set.\n        objective_name: The name of the objective function.\n            If ``None``, it will not be set.\n        constraints_names: The names the scalar constraints.\n            Each name must correspond to a scalar value.\n            If ``None``, it will not be set.\n        doe_size: The size of the initial design of experiments.\n            If ``None``, it will not be set.\n        total_time: The total time of the optimization, in seconds.\n            If ``None``, it will not be set.\n        algorithm_configuration: The name of the algorithm which generated the\n            history.\n            If ``None``, it will not be set.\n        number_of_variables: The number of optimization variables.\n            If ``None``, it will not be set.\n\n    Raises:\n        ValueError: If the lengths of the histories do not match.\n    \"\"\"  # noqa: D205, D212, D415\n    if constraints_names is None:\n        self._constraints_names = []\n    else:\n        self._constraints_names = constraints_names\n\n    self._objective_name = objective_name\n    self.algorithm_configuration = algorithm_configuration\n    self.doe_size = doe_size\n    self.items = self.__get_history_items(\n        objective_values,\n        infeasibility_measures,\n        feasibility_statuses,\n        n_unsatisfied_constraints,\n    )\n    self.problem_name = problem_name\n    self._number_of_variables = number_of_variables\n    self.total_time = total_time\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.infeasibility_measures","title":"infeasibility_measures  <code>property</code>","text":"<pre><code>infeasibility_measures: list[float]\n</code></pre> <p>The infeasibility measures.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.items","title":"items  <code>property</code> <code>writable</code>","text":"<pre><code>items: list[HistoryItem]\n</code></pre> <p>The history items.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If an item is set with a type different from HistoryItem.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.n_unsatisfied_constraints","title":"n_unsatisfied_constraints  <code>property</code>","text":"<pre><code>n_unsatisfied_constraints: list[int]\n</code></pre> <p>The numbers of unsatisfied constraints.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.objective_values","title":"objective_values  <code>property</code>","text":"<pre><code>objective_values: list[float]\n</code></pre> <p>The objective values.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.apply_infeasibility_tolerance","title":"apply_infeasibility_tolerance","text":"<pre><code>apply_infeasibility_tolerance(\n    infeasibility_tolerance: float,\n) -&gt; None\n</code></pre> <p>Apply a tolerance on the infeasibility measures of the history items.</p> <p>Mark the history items with an infeasibility measure below the tolerance as feasible.</p> <p>Parameters:</p> <ul> <li> <code>infeasibility_tolerance</code>               (<code>float</code>)           \u2013            <p>the tolerance on the infeasibility measure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def apply_infeasibility_tolerance(self, infeasibility_tolerance: float) -&gt; None:\n    \"\"\"Apply a tolerance on the infeasibility measures of the history items.\n\n    Mark the history items with an infeasibility measure below the tolerance\n    as feasible.\n\n    Args:\n        infeasibility_tolerance: the tolerance on the infeasibility measure.\n    \"\"\"\n    for item in self.items:\n        item.apply_infeasibility_tolerance(infeasibility_tolerance)\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.compute_cumulated_minimum","title":"compute_cumulated_minimum","text":"<pre><code>compute_cumulated_minimum() -&gt; PerformanceHistory\n</code></pre> <p>Return the history of the cumulated minimum.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The history of the cumulated minimum.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def compute_cumulated_minimum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history of the cumulated minimum.\n\n    Returns:\n        The history of the cumulated minimum.\n    \"\"\"\n    minimum_history = copy(self)\n    minimum_history.items = [\n        reduce(min, self.__items[: i + 1]) for i in range(len(self))\n    ]\n    return minimum_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.compute_maximum_history","title":"compute_maximum_history  <code>staticmethod</code>","text":"<pre><code>compute_maximum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the maximum of several performance histories.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>Iterable[PerformanceHistory]</code>)           \u2013            <p>The performance histories</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The maximum history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@staticmethod\ndef compute_maximum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory:\n    \"\"\"Return the maximum of several performance histories.\n\n    Args:\n        histories: The performance histories\n\n    Returns:\n        The maximum history.\n    \"\"\"\n    from gemseo_benchmark.results.performance_histories import PerformanceHistories\n\n    return PerformanceHistories(*histories).compute_maximum()\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.compute_median_history","title":"compute_median_history  <code>staticmethod</code>","text":"<pre><code>compute_median_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the median of several performance histories.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>Iterable[PerformanceHistory]</code>)           \u2013            <p>The performance histories</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The median history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@staticmethod\ndef compute_median_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory:\n    \"\"\"Return the median of several performance histories.\n\n    Args:\n        histories: The performance histories\n\n    Returns:\n        The median history.\n    \"\"\"\n    from gemseo_benchmark.results.performance_histories import PerformanceHistories\n\n    return PerformanceHistories(*histories).compute_median()\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.compute_minimum_history","title":"compute_minimum_history  <code>staticmethod</code>","text":"<pre><code>compute_minimum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the minimum of several performance histories.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>Iterable[PerformanceHistory]</code>)           \u2013            <p>The performance histories</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The minimum history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@staticmethod\ndef compute_minimum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory:\n    \"\"\"Return the minimum of several performance histories.\n\n    Args:\n        histories: The performance histories\n\n    Returns:\n        The minimum history.\n    \"\"\"\n    from gemseo_benchmark.results.performance_histories import PerformanceHistories\n\n    return PerformanceHistories(*histories).compute_minimum()\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.compute_target_hits_history","title":"compute_target_hits_history","text":"<pre><code>compute_target_hits_history(\n    values_history: PerformanceHistory,\n) -&gt; list[int]\n</code></pre> <p>Compute the history of the number of target hits for a performance history.</p> <p>Parameters:</p> <ul> <li> <code>values_history</code>               (<code>PerformanceHistory</code>)           \u2013            <p>The history of values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>The history of the number of target hits.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/target_values.py</code> <pre><code>def compute_target_hits_history(\n    self, values_history: PerformanceHistory\n) -&gt; list[int]:\n    \"\"\"Compute the history of the number of target hits for a performance history.\n\n    Args:\n        values_history: The history of values.\n\n    Returns:\n        The history of the number of target hits.\n    \"\"\"\n    minimum_history = values_history.compute_cumulated_minimum()\n    return [\n        [minimum &lt;= target for target in self].count(True)\n        for minimum in minimum_history\n    ]\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.extend","title":"extend","text":"<pre><code>extend(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Extend the performance history by repeating its last item.</p> <p>If the history is longer than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the extended performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The extended performance history.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the expected size is smaller than the history size.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def extend(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Extend the performance history by repeating its last item.\n\n    If the history is longer than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the extended performance history.\n\n    Returns:\n        The extended performance history.\n\n    Raises:\n        ValueError: If the expected size is smaller than the history size.\n    \"\"\"\n    if size &lt; len(self):\n        msg = (\n            f\"The expected size ({size}) is smaller than \"\n            f\"the history size ({len(self)}).\"\n        )\n        raise ValueError(msg)\n\n    history = copy(self)\n    history.items = list(chain(self, repeat(self[-1], (size - len(self)))))\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str | Path) -&gt; PerformanceHistory\n</code></pre> <p>Create a new performance history from a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str | Path) -&gt; PerformanceHistory:\n    \"\"\"Create a new performance history from a file.\n\n    Args:\n        path: The path to the file.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    with Path(path).open(\"r\") as file:\n        data = json.load(file)\n\n    # Cover deprecated performance history files\n    if isinstance(data, list):\n        history = cls()\n        history.items = [\n            HistoryItem(\n                item_data[PerformanceHistory.__PERFORMANCE],\n                item_data[PerformanceHistory.__INFEASIBILITY],\n                item_data.get(PerformanceHistory.__N_UNSATISFIED_CONSTRAINTS),\n            )\n            for item_data in data\n        ]\n        return history\n\n    history = cls()\n    history.problem_name = data.get(cls.__PROBLEM)\n    history._number_of_variables = data.get(cls.__NUMBER_OF_VARIABLES)\n    history._objective_name = data[cls.__OBJECTIVE_NAME]\n    history._constraints_names = data.get(cls.__CONSTRAINTS_NAMES, [])\n    if cls.__ALGORITHM_CONFIGURATION in data:\n        history.algorithm_configuration = AlgorithmConfiguration.from_dict(\n            data[cls.__ALGORITHM_CONFIGURATION]\n        )\n\n    history.doe_size = data.get(cls.__DOE_SIZE)\n    history.total_time = data.get(cls.__EXECUTION_TIME)\n    history.items = [\n        HistoryItem(\n            item_data[PerformanceHistory.__PERFORMANCE],\n            item_data[PerformanceHistory.__INFEASIBILITY],\n            item_data.get(PerformanceHistory.__N_UNSATISFIED_CONSTRAINTS),\n        )\n        for item_data in data[cls.__HISTORY_ITEMS]\n    ]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.from_problem","title":"from_problem  <code>classmethod</code>","text":"<pre><code>from_problem(\n    problem: OptimizationProblem,\n    problem_name: str | None = None,\n) -&gt; PerformanceHistory\n</code></pre> <p>Create a performance history from a solved optimization problem.</p> <p>Parameters:</p> <ul> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> <li> <code>problem_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the problem. If <code>None</code>, the name of the problem is not set.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_problem(\n    cls,\n    problem: OptimizationProblem,\n    problem_name: str | None = None,\n) -&gt; PerformanceHistory:\n    \"\"\"Create a performance history from a solved optimization problem.\n\n    Args:\n        problem: The optimization problem.\n        problem_name: The name of the problem.\n            If ``None``, the name of the problem is not set.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    obj_name = problem.objective.name\n    obj_values = []\n    infeas_measures = []\n    feas_statuses = []\n    n_unsatisfied_constraints = []\n    functions_names = {obj_name, *problem.constraints.get_names()}\n    for design_values, output_values in problem.database.items():\n        # Only consider points with all functions values\n        if not functions_names &lt;= set(output_values.keys()):\n            continue\n\n        x_vect = design_values.unwrap()\n        obj_values.append(atleast_1d(output_values[obj_name]).real[0])\n        feasibility, measure = problem.history.check_design_point_is_feasible(\n            x_vect\n        )\n        number_of_unsatisfied_constraints = (\n            problem.constraints.get_number_of_unsatisfied_constraints(output_values)\n        )\n        infeas_measures.append(measure)\n        feas_statuses.append(feasibility)\n        n_unsatisfied_constraints.append(number_of_unsatisfied_constraints)\n\n    return cls(\n        obj_values,\n        infeas_measures,\n        feas_statuses,\n        n_unsatisfied_constraints,\n        problem_name,\n        problem.objective.name,\n        problem.scalar_constraint_names,\n        number_of_variables=problem.design_space.dimension,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.get_plot_data","title":"get_plot_data","text":"<pre><code>get_plot_data(\n    feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]\n</code></pre> <p>Return the data to plot the performance history.</p> <p>Parameters:</p> <ul> <li> <code>feasible</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get only feasible values.</p> </li> <li> <code>minimum_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get the history of the cumulated minimum instead of the history of the objective value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[int], list[HistoryItem]]</code>           \u2013            <p>The abscissas and the ordinates of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def get_plot_data(\n    self, feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]:\n    \"\"\"Return the data to plot the performance history.\n\n    Args:\n        feasible: Whether to get only feasible values.\n        minimum_history: Whether to get the history of the cumulated minimum\n            instead of the history of the objective value.\n\n    Returns:\n        The abscissas and the ordinates of the plot.\n    \"\"\"\n    history = self.compute_cumulated_minimum() if minimum_history else self\n\n    # Find the index of the first feasible history item\n    if feasible:\n        first_feasible_index = len(history)\n        for index, item in enumerate(history):\n            if item.is_feasible:\n                first_feasible_index = index\n                break\n    else:\n        first_feasible_index = 0\n\n    return (\n        list(range(first_feasible_index + 1, len(history) + 1)),\n        history[first_feasible_index:],\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.plot","title":"plot","text":"<pre><code>plot(\n    show: bool = True, file_path: str | Path | None = None\n) -&gt; Figure\n</code></pre> <p>Plot the target values.</p> <p>Parameters:</p> <ul> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to show the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path where to save the plot. If <code>None</code>, the plot is not saved.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>A figure showing the target values.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/target_values.py</code> <pre><code>def plot(self, show: bool = True, file_path: str | Path | None = None) -&gt; Figure:\n    \"\"\"Plot the target values.\n\n    Args:\n        show: Whether to show the plot.\n        file_path: The path where to save the plot.\n            If ``None``, the plot is not saved.\n\n    Returns:\n        A figure showing the target values.\n    \"\"\"\n    targets_number = len(self)\n    fig = plt.figure()\n    axes = fig.add_subplot(1, 1, 1)\n    axes.set_title(\"Target values\")\n    plt.xlabel(\"Target index\")\n    plt.xlim([0, targets_number + 1])\n    plt.xticks(linspace(1, targets_number, dtype=int))\n    plt.ylabel(\"Target value\")\n    indexes, history_items = self.get_plot_data()\n\n    # Plot the feasible target values\n    objective_values = [item.objective_value for item in history_items]\n    is_feasible = array([item.is_feasible for item in history_items])\n    if is_feasible.any():\n        axes.plot(\n            array(indexes)[is_feasible],\n            array(objective_values)[is_feasible],\n            color=\"black\",\n            marker=\"o\",\n            linestyle=\"\",\n            label=\"feasible\",\n        )\n\n    # Plot the infeasible target values\n    is_infeasible = logical_not(is_feasible)\n    if is_infeasible.any():\n        axes.plot(\n            array(indexes)[is_infeasible],\n            array(objective_values)[is_infeasible],\n            color=\"red\",\n            marker=\"x\",\n            linestyle=\"\",\n            label=\"infeasible\",\n        )\n\n    plt.legend()\n\n    save_show_figure(fig, show, file_path)\n\n    return fig\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.plot_on_axes","title":"plot_on_axes","text":"<pre><code>plot_on_axes(\n    axes: Axes,\n    axhline_kwargs: Mapping[\n        str, str | int\n    ] = MappingProxyType(\n        {\"color\": \"red\", \"linestyle\": \":\"}\n    ),\n    yticklabels_format: str = \".4g\",\n    set_ylabel_kwargs: Mapping[\n        str, str | int\n    ] = MappingProxyType({\"rotation\": 270, \"labelpad\": 12}),\n) -&gt; None\n</code></pre> <p>Plot target values as horizontal lines.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>axhline_kwargs</code>               (<code>Mapping[str, str | int]</code>, default:                   <code>MappingProxyType({'color': 'red', 'linestyle': ':'})</code> )           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.axhline</code>.</p> </li> <li> <code>yticklabels_format</code>               (<code>str</code>, default:                   <code>'.4g'</code> )           \u2013            <p>The string format for the target values labels.</p> </li> <li> <code>set_ylabel_kwargs</code>               (<code>Mapping[str, str | int]</code>, default:                   <code>MappingProxyType({'rotation': 270, 'labelpad': 12})</code> )           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.set_ylabel</code>.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/target_values.py</code> <pre><code>def plot_on_axes(\n    self,\n    axes: matplotlib.axes.Axes,\n    axhline_kwargs: Mapping[str, str | int] = MappingProxyType({\n        \"color\": \"red\",\n        \"linestyle\": \":\",\n    }),\n    yticklabels_format: str = \".4g\",\n    set_ylabel_kwargs: Mapping[str, str | int] = MappingProxyType({\n        \"rotation\": 270,\n        \"labelpad\": 12,\n    }),\n) -&gt; None:\n    \"\"\"Plot target values as horizontal lines.\n\n    Args:\n        axes: The axes of the plot.\n        axhline_kwargs: Keyword arguments\n            for ``matplotlib.axes.Axes.axhline``.\n        yticklabels_format: The string format for the target values labels.\n        set_ylabel_kwargs: Keyword arguments\n            for ``matplotlib.axes.Axes.set_ylabel``.\n    \"\"\"\n    twin_axes = axes.twinx()\n    values = [target.objective_value for target in self if target.is_feasible]\n    for value in values:\n        axes.axhline(value, **axhline_kwargs)\n\n    twin_axes.set_yticks(values)\n    twin_axes.set_yticklabels([\n        f\"{{value:{yticklabels_format}}}\".format(value=value) for value in values\n    ])\n    twin_axes.set_ylabel(\"Target values\", **set_ylabel_kwargs)\n    twin_axes.set_ylim(axes.get_ylim())\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.remove_leading_infeasible","title":"remove_leading_infeasible","text":"<pre><code>remove_leading_infeasible() -&gt; PerformanceHistory\n</code></pre> <p>Return the history starting from the first feasible item.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The truncated performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def remove_leading_infeasible(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history starting from the first feasible item.\n\n    Returns:\n        The truncated performance history.\n    \"\"\"\n    first_feasible = len(self)\n    for index, item in enumerate(self):\n        if item.is_feasible:\n            first_feasible = index\n            break\n\n    truncated_history = copy(self)\n    if first_feasible is not None:\n        truncated_history.items = self.items[first_feasible:]\n\n    return truncated_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.shorten","title":"shorten","text":"<pre><code>shorten(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Shorten the performance history to a given size.</p> <p>If the history is shorter than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the shortened performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The shortened performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def shorten(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Shorten the performance history to a given size.\n\n    If the history is shorter than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the shortened performance history.\n\n    Returns:\n        The shortened performance history.\n    \"\"\"\n    history = copy(self)\n    history.items = self.items[:size]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/target_values/#gemseo_benchmark.data_profiles.target_values.TargetValues.to_file","title":"to_file","text":"<pre><code>to_file(path: str | Path) -&gt; None\n</code></pre> <p>Save the performance history in a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path where to write the file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def to_file(\n    self,\n    path: str | Path,\n) -&gt; None:\n    \"\"\"Save the performance history in a file.\n\n    Args:\n        path: The path where to write the file.\n    \"\"\"\n    items_data = []\n    # Add each history item in dictionary format\n    for item in self.items:\n        data_item = {\n            PerformanceHistory.__PERFORMANCE: item.objective_value,\n            PerformanceHistory.__INFEASIBILITY: item.infeasibility_measure,\n        }\n        if item.n_unsatisfied_constraints is not None:\n            # N.B. type int64 is not JSON serializable\n            data_item[PerformanceHistory.__N_UNSATISFIED_CONSTRAINTS] = int(\n                item.n_unsatisfied_constraints\n            )\n\n        items_data.append(data_item)\n\n    data = {}\n    if self.problem_name is not None:\n        data[self.__PROBLEM] = self.problem_name\n\n    if self._number_of_variables is not None:\n        data[self.__NUMBER_OF_VARIABLES] = self._number_of_variables\n\n    data[self.__OBJECTIVE_NAME] = self._objective_name\n    if self._constraints_names:\n        data[self.__CONSTRAINTS_NAMES] = self._constraints_names\n\n    if self.algorithm_configuration is not None:\n        data[self.__ALGORITHM_CONFIGURATION] = self.algorithm_configuration.to_dict(\n            True\n        )\n\n    if self.doe_size is not None:\n        data[self.__DOE_SIZE] = self.doe_size\n\n    if self.total_time is not None:\n        data[self.__EXECUTION_TIME] = self.total_time\n\n    data[self.__HISTORY_ITEMS] = items_data\n    with Path(path).open(\"w\") as file:\n        json.dump(data, file, indent=2, separators=(\",\", \": \"))\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/","title":"Targets generator","text":""},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator","title":"targets_generator","text":"<p>Generation of targets for a problem to be solved by an iterative algorithm.</p> <p>The targets are generated out of algorithms histories considered to be of reference: the median of the reference histories is computed and a uniformly distributed subset (of the required size) of this median history is extracted.</p>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator","title":"TargetsGenerator","text":"<pre><code>TargetsGenerator()\n</code></pre> <p>Compute the target values for an objective to minimize.</p> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def __init__(self) -&gt; None:  # noqa: D107\n    self.__histories = PerformanceHistories()\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator.add_history","title":"add_history","text":"<pre><code>add_history(\n    objective_values: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    history: PerformanceHistory | None = None,\n) -&gt; None\n</code></pre> <p>Add a history of objective values.</p> <p>Parameters:</p> <ul> <li> <code>objective_values</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of objective values. If <code>None</code>, a performance history must be passed. N.B. the value at index i is assumed to have been obtained with i+1 evaluations.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of infeasibility measures. If <code>None</code> then measures are set to zero in case of feasibility and set to infinity otherwise.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>A history of (boolean) feasibility statuses. If <code>None</code> then feasibility is always assumed.</p> </li> <li> <code>history</code>               (<code>PerformanceHistory | None</code>, default:                   <code>None</code> )           \u2013            <p>A performance history. If <code>None</code>, objective values must be passed.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither a performance history nor objective values are passed, or if both are passed.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def add_history(\n    self,\n    objective_values: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    history: PerformanceHistory | None = None,\n) -&gt; None:\n    \"\"\"Add a history of objective values.\n\n    Args:\n        objective_values: A history of objective values.\n            If ``None``, a performance history must be passed.\n            N.B. the value at index i is assumed to have been obtained with i+1\n            evaluations.\n        infeasibility_measures: A history of infeasibility measures.\n            If ``None`` then measures are set to zero in case of feasibility and set\n            to infinity otherwise.\n        feasibility_statuses: A history of (boolean) feasibility statuses.\n            If ``None`` then feasibility is always assumed.\n        history: A performance history.\n            If ``None``, objective values must be passed.\n\n    Raises:\n        ValueError: If neither a performance history nor objective values are\n            passed, or if both are passed.\n    \"\"\"\n    if history is not None:\n        if objective_values is not None:\n            msg = \"Both a performance history and objective values were passed.\"\n            raise ValueError(msg)\n    elif objective_values is None:\n        msg = \"Either a performance history or objective values must be passed.\"\n        raise ValueError(msg)\n    else:\n        history = PerformanceHistory(\n            objective_values, infeasibility_measures, feasibility_statuses\n        )\n    self.__histories.append(history)\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator.compute_target_values","title":"compute_target_values","text":"<pre><code>compute_target_values(\n    targets_number: int,\n    budget_min: int = 1,\n    feasible: bool = True,\n    show: bool = False,\n    file_path: str | Path | None = None,\n    best_target_objective: float | None = None,\n    best_target_tolerance: float = 0.0,\n) -&gt; TargetValues\n</code></pre> <p>Compute the target values for a function from the histories of its values.</p> <p>Parameters:</p> <ul> <li> <code>targets_number</code>               (<code>int</code>)           \u2013            <p>The number of targets to compute.</p> </li> <li> <code>budget_min</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of functions evaluations to be used to define the first target. If argument <code>feasible</code> is set to <code>True</code>, this argument will be disregarded and the evaluation budget defining the easiest target will be the budget of the first item in the histories reaching the best target value.</p> </li> <li> <code>feasible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate only feasible targets.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The file path to save the plot. If <code>None</code>, the plot is not saved.</p> </li> <li> <code>best_target_objective</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The objective value of the best target value. If <code>None</code>, it will be inferred from the performance histories.</p> </li> <li> <code>best_target_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The relative tolerance for comparison with the best target value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TargetValues</code>           \u2013            <p>The target values of the function.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If feasibility is required but the best target value is not feasible.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def compute_target_values(\n    self,\n    targets_number: int,\n    budget_min: int = 1,\n    feasible: bool = True,\n    show: bool = False,\n    file_path: str | Path | None = None,\n    best_target_objective: float | None = None,\n    best_target_tolerance: float = 0.0,\n) -&gt; TargetValues:\n    \"\"\"Compute the target values for a function from the histories of its values.\n\n    Args:\n        targets_number: The number of targets to compute.\n        budget_min: The number of functions evaluations to be used to define the\n            first target.\n            If argument ``feasible`` is set to ``True``, this argument will be\n            disregarded and the evaluation budget defining the easiest target\n            will be the budget of the first item in the histories reaching the\n            best target value.\n        feasible: Whether to generate only feasible targets.\n        show: Whether to show the plot.\n        file_path: The file path to save the plot.\n            If ``None``, the plot is not saved.\n        best_target_objective: The objective value of the best target value.\n            If ``None``, it will be inferred from the performance histories.\n        best_target_tolerance: The relative tolerance for comparison with the\n            best target value.\n\n    Returns:\n        The target values of the function.\n\n    Raises:\n        RuntimeError: If feasibility is required but the best target value is not\n            feasible.\n    \"\"\"\n    # Get the performance histories of reference\n    reference_histories, best_target = self.__get_reference_histories(\n        self.__histories, best_target_objective, best_target_tolerance, feasible\n    )\n\n    # Compute the median of the cumulated minimum histories\n    median_history = PerformanceHistories(*reference_histories).compute_median()\n    if feasible:\n        median_history = median_history.remove_leading_infeasible()\n\n    # Truncate the values that stagnate near the best target\n    for index, item in enumerate(median_history):\n        if item &lt;= best_target:\n            median_history = median_history[: index + 1]\n            break\n\n    # Compute a budget scale\n    budget_scale = self.__compute_budget_scale(\n        budget_min, len(median_history), targets_number\n    )\n\n    # Compute the target values\n    target_values = TargetValues()\n    target_values.items = [median_history[item - 1] for item in budget_scale]\n\n    # Plot the target values\n    if show or file_path is not None:\n        target_values.plot(show, file_path)\n\n    return target_values\n</code></pre>"},{"location":"reference/gemseo_benchmark/data_profiles/targets_generator/#gemseo_benchmark.data_profiles.targets_generator.TargetsGenerator.plot_histories","title":"plot_histories","text":"<pre><code>plot_histories(\n    best_target_value: float | None = None,\n    show: bool = False,\n    file_path: str | Path | None = None,\n) -&gt; Figure\n</code></pre> <p>Plot the histories used as a basis to compute the target values.</p> <p>Parameters:</p> <ul> <li> <code>best_target_value</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The best target value to be represented with a horizontal line. If <code>None</code>, no best target value will be plotted.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show the figure.</p> </li> <li> <code>file_path</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path where to save the figure. If <code>None</code>, the figure will not be saved.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>           \u2013            <p>The histories figure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/data_profiles/targets_generator.py</code> <pre><code>def plot_histories(\n    self,\n    best_target_value: float | None = None,\n    show: bool = False,\n    file_path: str | Path | None = None,\n) -&gt; Figure:\n    \"\"\"Plot the histories used as a basis to compute the target values.\n\n    Args:\n        best_target_value: The best target value\n            to be represented with a horizontal line.\n            If ``None``, no best target value will be plotted.\n        show: Whether to show the figure.\n        file_path: The path where to save the figure.\n            If ``None``, the figure will not be saved.\n\n    Returns:\n        The histories figure.\n    \"\"\"\n    # Set up the figure\n    figure = plt.figure()\n    axes = figure.add_subplot(1, 1, 1)\n    axes.set_title(\"Reference performance histories\")\n    plt.xlabel(\"Number or evaluations\")\n    plt.ylabel(\"Performance value\")\n\n    # Plot the best target value\n    if best_target_value is not None:\n        plt.axhline(y=best_target_value, color=\"r\", linestyle=\"-\")\n\n    # Plot the histories of the cumulated minima\n    maximum_budget = max(len(history) for history in self.__histories)\n    minimum_budget = maximum_budget\n    for history in self.__histories:\n        budgets, items = history.get_plot_data(feasible=True, minimum_history=True)\n        # Update the minimum budget\n        if budgets:  # empty if there is no feasible points\n            minimum_budget = min(budgets[0], minimum_budget)\n\n        axes.plot(\n            budgets,\n            [item.objective_value for item in items],\n            marker=\"o\",\n            linestyle=\":\",\n        )\n\n    plt.xlim(left=minimum_budget - 1, right=maximum_budget + 1)\n    axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n    save_show_figure(figure, show, file_path)\n    return figure\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/","title":"Problems","text":""},{"location":"reference/gemseo_benchmark/problems/#gemseo_benchmark.problems","title":"problems","text":"<p>Reference problems for benchmarking.</p>"},{"location":"reference/gemseo_benchmark/problems/problem/","title":"Problem","text":""},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem","title":"problem","text":"<p>Reference problem for benchmarking.</p> <p>A benchmarking problem is a problem class to be solved by iterative algorithms for comparison purposes. A benchmarking problem is characterized by its functions (e.g. objective and constraints for an optimization problem), its starting points (each defining an instance of the problem) and its targets (refer to :mod:<code>.data_profiles.target_values</code>).</p>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem","title":"Problem","text":"<pre><code>Problem(\n    name: str,\n    optimization_problem_creator: Callable[\n        [], OptimizationProblem\n    ],\n    start_points: InputStartPoints | None = None,\n    target_values: TargetValues | None = None,\n    doe_algo_name: str | None = None,\n    doe_size: int | None = None,\n    doe_options: (\n        Mapping[str, DriverLibraryOptionType] | None\n    ) = None,\n    description: str | None = None,\n    target_values_algorithms_configurations: (\n        AlgorithmsConfigurations | None\n    ) = None,\n    target_values_number: int | None = None,\n    optimum: float | None = None,\n)\n</code></pre> <p>An optimization benchmarking problem.</p> <p>An optimization benchmarking problem is characterized by - its functions (objective and constraints, including bounds), - its starting points, - its target values.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the benchmarking problem.</p> </li> <li> <code>optimization_problem_creator</code>               (<code>Callable[[], OptimizationProblem]</code>)           \u2013            <p>A callable that returns an instance of the optimization problem.</p> </li> <li> <code>start_points</code>               (<code>Iterable[ndarray]</code>)           \u2013            <p>The starting points of the benchmarking problem.</p> </li> <li> <code>optimum</code>               (<code>float</code>)           \u2013            <p>The best feasible objective value of the problem. Set to None if unknown.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the benchmarking problem.</p> </li> <li> <code>optimization_problem_creator</code>               (<code>Callable[[], OptimizationProblem]</code>)           \u2013            <p>A callable object that returns an instance of the problem.</p> </li> <li> <code>start_points</code>               (<code>InputStartPoints | None</code>, default:                   <code>None</code> )           \u2013            <p>The starting points of the benchmarking problem. If <code>None</code>: if <code>doe_algo_name</code>, <code>doe_size</code>, and <code>doe_options</code> are not <code>None</code> then the starting points will be generated as a DOE; otherwise the current value of the optimization problem will set as the single starting point.</p> </li> <li> <code>target_values</code>               (<code>TargetValues | None</code>, default:                   <code>None</code> )           \u2013            <p>The target values of the benchmarking problem. If <code>None</code>, the target values will have to be generated later with the <code>generate_targets</code> method.</p> </li> <li> <code>doe_algo_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the DOE algorithm. If <code>None</code>, the current point of the problem design space is set as the only starting point.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of starting points. If <code>None</code>, this number is set as the problem dimension or 10 if bigger.</p> </li> <li> <code>doe_options</code>               (<code>Mapping[str, DriverLibraryOptionType] | None</code>, default:                   <code>None</code> )           \u2013            <p>The options of the DOE algorithm. If <code>None</code>, no option other than the DOE size is passed to the algorithm.</p> </li> <li> <code>description</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The description of the problem (to appear in a report). If <code>None</code>, the problem will not have a description.</p> </li> <li> <code>target_values_algorithms_configurations</code>               (<code>AlgorithmsConfigurations | None</code>, default:                   <code>None</code> )           \u2013            <p>The configurations of the optimization algorithms for the computation of target values. If <code>None</code>, the target values will not be computed.</p> </li> <li> <code>target_values_number</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of target values to compute. If <code>None</code>, the target values will not be computed. N.B. the number of target values shall be the same for all the benchmarking problems of a same group.</p> </li> <li> <code>optimum</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The best feasible objective value of the problem. If <code>None</code>, it will not be set. If not <code>None</code>, it will be set as the hardest target value.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the return type of the creator is not :class:<code>gemseo.algos.opt_problem.OptimizationProblem</code>, or if a starting point is not of type ndarray.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither starting points nor DOE configurations are passed, or if a starting point is of inappropriate shape.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    optimization_problem_creator: Callable[[], OptimizationProblem],\n    start_points: InputStartPoints | None = None,\n    target_values: TargetValues | None = None,\n    doe_algo_name: str | None = None,\n    doe_size: int | None = None,\n    doe_options: Mapping[str, DriverLibraryOptionType] | None = None,\n    description: str | None = None,\n    target_values_algorithms_configurations: AlgorithmsConfigurations | None = None,\n    target_values_number: int | None = None,\n    optimum: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        name: The name of the benchmarking problem.\n        optimization_problem_creator: A callable object that returns an instance\n            of the problem.\n        start_points: The starting points of the benchmarking problem.\n            If ``None``:\n            if ``doe_algo_name``, ``doe_size``, and ``doe_options`` are not ``None``\n            then the starting points will be generated as a DOE;\n            otherwise the current value of the optimization problem\n            will set as the single starting point.\n        target_values: The target values of the benchmarking problem.\n            If ``None``, the target values will have to be generated later with the\n            `generate_targets` method.\n        doe_algo_name: The name of the DOE algorithm.\n            If ``None``, the current point of the problem design space is set as the\n            only starting point.\n        doe_size: The number of starting points.\n            If ``None``, this number is set as the problem dimension or 10 if\n            bigger.\n        doe_options: The options of the DOE algorithm.\n            If ``None``, no option other than the DOE size is passed to the\n            algorithm.\n        description: The description of the problem (to appear in a report).\n            If ``None``, the problem will not have a description.\n        target_values_algorithms_configurations: The configurations of the\n            optimization algorithms for the computation of target values.\n            If ``None``, the target values will not be computed.\n        target_values_number: The number of target values to compute.\n            If ``None``, the target values will not be computed.\n            N.B. the number of target values shall be the same for all the\n            benchmarking problems of a same group.\n        optimum: The best feasible objective value of the problem.\n            If ``None``, it will not be set.\n            If not ``None``, it will be set as the hardest target value.\n\n    Raises:\n        TypeError: If the return type of the creator is not\n            :class:`gemseo.algos.opt_problem.OptimizationProblem`,\n            or if a starting point is not of type ndarray.\n        ValueError: If neither starting points nor DOE configurations are passed,\n            or if a starting point is of inappropriate shape.\n    \"\"\"  # noqa: D205, D212, D415\n    self.name = name\n    self.__description = description\n    self.creator = optimization_problem_creator\n    self.optimum = optimum\n    self.__targets_generator = None\n\n    # Set the dimension\n    problem = optimization_problem_creator()\n    if not isinstance(problem, OptimizationProblem):\n        msg = \"optimization_problem_creator must return an OptimizationProblem.\"\n        raise TypeError(msg)\n    self._problem = problem\n\n    # Set the starting points\n    self.__start_points = []\n    if start_points is not None:\n        self.start_points = start_points\n    elif doe_algo_name is not None:\n        self.start_points = self.__get_start_points(\n            doe_algo_name, doe_size, doe_options\n        )\n    elif problem.design_space.has_current_value:\n        self.start_points = atleast_2d(\n            self._problem.design_space.get_current_value()\n        )\n\n    # Set the target values:\n    self.__target_values = None\n    if target_values is not None:\n        self.target_values = target_values\n    elif (\n        target_values_algorithms_configurations is not None\n        and target_values_number is not None\n    ):\n        self.target_values = self.compute_targets(\n            target_values_number,\n            target_values_algorithms_configurations,\n        )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.constraints_names","title":"constraints_names  <code>property</code>","text":"<pre><code>constraints_names: list[str]\n</code></pre> <p>The names of the scalar constraints.</p>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.description","title":"description  <code>property</code>","text":"<pre><code>description: str\n</code></pre> <p>The description of the problem.</p>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.dimension","title":"dimension  <code>property</code>","text":"<pre><code>dimension: int\n</code></pre> <p>The dimension of the problem.</p>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.objective_name","title":"objective_name  <code>property</code>","text":"<pre><code>objective_name: str\n</code></pre> <p>The name of the objective function.</p>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.start_points","title":"start_points  <code>property</code> <code>writable</code>","text":"<pre><code>start_points: list[ndarray]\n</code></pre> <p>The starting points of the problem.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the problem has no starting point, or if the starting points are passed as a NumPy array with an invalid shape.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.target_values","title":"target_values  <code>property</code> <code>writable</code>","text":"<pre><code>target_values: TargetValues\n</code></pre> <p>The target values of the benchmarking problem.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the benchmarking problem has no target value.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.targets_generator","title":"targets_generator  <code>property</code>","text":"<pre><code>targets_generator: TargetsGenerator\n</code></pre> <p>The generator for target values.</p>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.compute_data_profile","title":"compute_data_profile","text":"<pre><code>compute_data_profile(\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path | None = None,\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int | None = None,\n    plot_kwargs: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_kwargs: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Generate the data profiles of given algorithms.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>results</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display the plot.</p> </li> <li> <code>file_path</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path where to save the plot. If <code>None</code>, the plot is not saved.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_eval_number</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum evaluations number to be displayed. If <code>None</code>, this value is inferred from the longest history.</p> </li> <li> <code>plot_kwargs</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_kwargs</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_evaluation_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the number of function evaluations axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>def compute_data_profile(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: str | Path | None = None,\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int | None = None,\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_kwargs: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Generate the data profiles of given algorithms.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        results: The paths to the reference histories for each algorithm.\n        show: Whether to display the plot.\n        file_path: The path where to save the plot.\n            If ``None``, the plot is not saved.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_eval_number: The maximum evaluations number to be displayed.\n            If ``None``, this value is inferred from the longest history.\n        plot_kwargs: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_kwargs: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_evaluation_log_scale: Whether to use a logarithmic scale\n            for the number of function evaluations axis.\n    \"\"\"\n    # Initialize the data profile\n    data_profile = DataProfile({self.name: self.target_values})\n\n    # Generate the performance histories\n    for configuration_name in algos_configurations.names:\n        for history_path in results.get_paths(configuration_name, self.name):\n            history = PerformanceHistory.from_file(history_path)\n            if max_eval_number is not None:\n                history = history.shorten(max_eval_number)\n\n            history.apply_infeasibility_tolerance(infeasibility_tolerance)\n            data_profile.add_history(\n                self.name,\n                configuration_name,\n                history.objective_values,\n                history.infeasibility_measures,\n            )\n\n    # Plot and/or save the data profile\n    data_profile.plot(\n        show=show,\n        file_path=file_path,\n        plot_kwargs=plot_kwargs,\n        grid_kwargs=grid_kwargs,\n        use_evaluation_log_scale=use_evaluation_log_scale,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.compute_performance","title":"compute_performance  <code>staticmethod</code>","text":"<pre><code>compute_performance(\n    problem: OptimizationProblem,\n) -&gt; tuple[list[float], list[float], list[bool]]\n</code></pre> <p>Extract the performance history from a solved optimization problem.</p> <p>Parameters:</p> <ul> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[float], list[float], list[bool]]</code>           \u2013            <p>The history of objective values, the history of infeasibility measures, the history of feasibility statuses.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>@staticmethod\ndef compute_performance(\n    problem: OptimizationProblem,\n) -&gt; tuple[list[float], list[float], list[bool]]:\n    \"\"\"Extract the performance history from a solved optimization problem.\n\n    Args:\n        problem: The optimization problem.\n\n    Returns:\n        The history of objective values,\n        the history of infeasibility measures,\n        the history of feasibility statuses.\n    \"\"\"\n    obj_name = problem.objective.name\n    obj_values = []\n    infeas_measures = []\n    feas_statuses = []\n    for key, values in problem.database.items():\n        obj_values.append(values[obj_name])\n        feasibility, measure = problem.history.check_design_point_is_feasible(key)\n        infeas_measures.append(measure)\n        feas_statuses.append(feasibility)\n    return obj_values, infeas_measures, feas_statuses\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.compute_targets","title":"compute_targets","text":"<pre><code>compute_targets(\n    targets_number: int,\n    ref_algo_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n    budget_min: int = 1,\n    show: bool = False,\n    file_path: str | None = None,\n    best_target_tolerance: float = 0.0,\n    disable_stopping: bool = True,\n) -&gt; TargetValues\n</code></pre> <p>Generate targets based on reference algorithms.</p> <p>Parameters:</p> <ul> <li> <code>targets_number</code>               (<code>int</code>)           \u2013            <p>The number of targets to generate.</p> </li> <li> <code>ref_algo_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The configurations of the reference algorithms.</p> </li> <li> <code>only_feasible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate only feasible targets.</p> </li> <li> <code>budget_min</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The evaluation budget to be used to define the easiest target.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, show the plot.</p> </li> <li> <code>file_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The path where to save the plot. If <code>None</code>, the plot is not saved.</p> </li> <li> <code>best_target_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The relative tolerance for comparisons with the best target value.</p> </li> <li> <code>disable_stopping</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to disable the stopping criteria.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TargetValues</code>           \u2013            <p>The generated targets.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>def compute_targets(\n    self,\n    targets_number: int,\n    ref_algo_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n    budget_min: int = 1,\n    show: bool = False,\n    file_path: str | None = None,\n    best_target_tolerance: float = 0.0,\n    disable_stopping: bool = True,\n) -&gt; TargetValues:\n    \"\"\"Generate targets based on reference algorithms.\n\n    Args:\n        targets_number: The number of targets to generate.\n        ref_algo_configurations: The configurations of the reference algorithms.\n        only_feasible: Whether to generate only feasible targets.\n        budget_min: The evaluation budget to be used to define the easiest target.\n        show: If True, show the plot.\n        file_path: The path where to save the plot.\n            If ``None``, the plot is not saved.\n        best_target_tolerance: The relative tolerance for comparisons with the\n            best target value.\n        disable_stopping: Whether to disable the stopping criteria.\n\n    Returns:\n        The generated targets.\n    \"\"\"\n    self.__targets_generator = TargetsGenerator()\n\n    # Generate reference performance histories\n    for configuration in ref_algo_configurations:\n        # Disable the stopping criteria\n        options = dict(configuration.algorithm_options)\n        if disable_stopping:\n            options[\"xtol_rel\"] = 0.0\n            options[\"xtol_abs\"] = 0.0\n            options[\"ftol_rel\"] = 0.0\n            options[\"ftol_abs\"] = 0.0\n\n        for instance in self:\n            execute_algo(\n                instance,\n                algo_type=\"opt\",\n                algo_name=configuration.algorithm_name,\n                **options,\n            )\n            history = PerformanceHistory.from_problem(instance)\n            self.__targets_generator.add_history(history=history)\n\n    # Compute the target values\n    target_values = self.__targets_generator.compute_target_values(\n        targets_number,\n        budget_min,\n        only_feasible,\n        show,\n        file_path,\n        self.optimum,\n        best_target_tolerance,\n    )\n    self.__target_values = target_values\n\n    return target_values\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.is_algorithm_suited","title":"is_algorithm_suited","text":"<pre><code>is_algorithm_suited(name: str) -&gt; bool\n</code></pre> <p>Check whether an algorithm is suited to the problem.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the algorithm is suited to the problem, False otherwise.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>def is_algorithm_suited(self, name: str) -&gt; bool:\n    \"\"\"Check whether an algorithm is suited to the problem.\n\n    Args:\n        name: The name of the algorithm.\n\n    Returns:\n        True if the algorithm is suited to the problem, False otherwise.\n    \"\"\"\n    library = OptimizationLibraryFactory().create(name)\n    return library.is_algorithm_suited(library.ALGORITHM_INFOS[name], self._problem)\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.load_start_point","title":"load_start_point","text":"<pre><code>load_start_point(path: Path) -&gt; None\n</code></pre> <p>Load the start points from a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>def load_start_point(self, path: Path) -&gt; None:\n    \"\"\"Load the start points from a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    self.start_points = load(path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.plot_histories","title":"plot_histories","text":"<pre><code>plot_histories(\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: Path | None = None,\n    plot_all_histories: bool = False,\n    alpha: float = 0.3,\n    markevery: MarkeveryType | None = None,\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int | None = None,\n    use_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Plot the histories of a problem.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>results</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to display the plot.</p> </li> <li> <code>file_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path where to save the plot. If <code>None</code>, the plot is not saved.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>0.3</code> )           \u2013            <p>The opacity level for overlapping areas. Refer to the Matplotlib documentation.</p> </li> <li> <code>markevery</code>               (<code>MarkeveryType | None</code>, default:                   <code>None</code> )           \u2013            <p>The sampling parameter for the markers of the plot. Refer to the Matplotlib documentation.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_eval_number</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum evaluations number displayed. If <code>None</code>, this value is inferred from the longest history.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale on the value axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>def plot_histories(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    results: Results,\n    show: bool = False,\n    file_path: Path | None = None,\n    plot_all_histories: bool = False,\n    alpha: float = 0.3,\n    markevery: MarkeveryType | None = None,\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int | None = None,\n    use_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Plot the histories of a problem.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        results: The paths to the reference histories for each algorithm.\n        show: Whether to display the plot.\n        file_path: The path where to save the plot.\n            If ``None``, the plot is not saved.\n        plot_all_histories: Whether to plot all the performance histories.\n        alpha: The opacity level for overlapping areas.\n            Refer to the Matplotlib documentation.\n        markevery: The sampling parameter for the markers of the plot.\n            Refer to the Matplotlib documentation.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_eval_number: The maximum evaluations number displayed.\n            If ``None``, this value is inferred from the longest history.\n        use_log_scale: Whether to use a logarithmic scale on the value axis.\n    \"\"\"\n    figure = plt.figure()\n    axes = figure.gca()\n\n    # Plot the target values\n    objective_targets = [target.objective_value for target in self.target_values]\n    for objective_target in objective_targets:\n        plt.axhline(objective_target, color=\"red\", linestyle=\"--\")\n\n    # Get the histories of the cumulated minima\n    minima, max_feasible_objective = self.__get_cumulated_minimum_histories(\n        algos_configurations, results, infeasibility_tolerance, max_eval_number\n    )\n    if max_eval_number is None:\n        max_eval_number = max(\n            len(hist) for histories in minima.values() for hist in histories\n        )\n\n    y_relative_margin = 0.03\n    max_feasible_objective = self.__get_infeasible_items_objective(\n        max_feasible_objective, y_relative_margin\n    )\n\n    # Plot the histories\n    minimum_values = []\n    for configuration_name, color, marker in zip(\n        algos_configurations.names, COLORS_CYCLE, get_markers_cycle()\n    ):\n        minimum_value = minima[configuration_name].plot_algorithm_histories(\n            axes,\n            configuration_name,\n            max_feasible_objective,\n            plot_all_histories,\n            color=color,\n            marker=marker,\n            alpha=alpha,\n            markevery=markevery,\n        )\n        if minimum_value is not None:\n            minimum_values.append(minimum_value)\n\n    plt.legend()\n\n    # Ensure the x-axis ticks are integers\n    axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n    if use_log_scale:\n        axes.set_yscale(\"log\")\n\n    plt.margins(x=0.1)\n    plt.xlabel(\"Number of functions evaluations\")\n    plt.xlim(1, max_eval_number)\n\n    # Set the y-axis margins to zero to get the tight y-limits\n    plt.autoscale(enable=True, axis=\"y\", tight=True)\n    y_min, y_max = axes.get_ylim()\n    # Adjust the y-limits relative to the target values\n    if len(objective_targets) &gt; 1:\n        y_max = max(*objective_targets, *minimum_values)\n        y_min = min(*objective_targets, *minimum_values)\n    margin = 0.03 * (y_max - y_min)\n    plt.ylim(bottom=y_min - margin, top=y_max + margin)\n    plt.ylabel(\"Objective value\")\n\n    # Add ticks for the targets values on a right-side axis\n    twin_axes = axes.twinx()\n    twin_axes.set_ylim(axes.get_ylim())\n    twin_axes.set_yticks(objective_targets)\n    twin_axes.set_yticklabels([f\"{value:.2g}\" for value in objective_targets])\n    twin_axes.set_ylabel(\"Target values\", rotation=270)\n    if use_log_scale:\n        twin_axes.set_yscale(\"log\")\n\n    plt.title(\"Convergence histories\")\n    save_show_figure(figure, show, file_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem.Problem.save_start_points","title":"save_start_points","text":"<pre><code>save_start_points(path: Path) -&gt; None\n</code></pre> <p>Save the start points as a NumPy binary.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the NumPy binary.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problem.py</code> <pre><code>def save_start_points(self, path: Path) -&gt; None:\n    \"\"\"Save the start points as a NumPy binary.\n\n    Args:\n        path: The path to the NumPy binary.\n    \"\"\"\n    save(path, array(self.start_points))\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problem/#gemseo_benchmark.problems.problem-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/","title":"Problems group","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group","title":"problems_group","text":"<p>Grouping of reference problems for benchmarking.</p>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup","title":"ProblemsGroup","text":"<pre><code>ProblemsGroup(\n    name: str,\n    problems: Iterable[Problem],\n    description: str = \"\",\n)\n</code></pre> <p>A group of reference problems for benchmarking.</p> <p>.. note::</p> <p>Reference problems should be grouped based on common characteristics such as    functions smoothness and constraint set geometry.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the group of problems.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the group of problems.</p> </li> <li> <code>problems</code>               (<code>Iterable[Problem]</code>)           \u2013            <p>The benchmarking problems of the group.</p> </li> <li> <code>description</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The description of the group of problems.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problems_group.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    problems: Iterable[Problem],\n    description: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Args:\n        name: The name of the group of problems.\n        problems: The benchmarking problems of the group.\n        description: The description of the group of problems.\n    \"\"\"  # noqa: D205, D212, D415\n    self.name = name\n    self.__problems = problems\n    self.description = description\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup.compute_data_profile","title":"compute_data_profile","text":"<pre><code>compute_data_profile(\n    algos_configurations: AlgorithmsConfigurations,\n    histories_paths: Results,\n    show: bool = True,\n    plot_path: str | Path | None = None,\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int = 0,\n    plot_kwargs: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n    grid_kwargs: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Generate the data profiles of given algorithms relative to the problems.</p> <p>Parameters:</p> <ul> <li> <code>algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The algorithms configurations.</p> </li> <li> <code>histories_paths</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm.</p> </li> <li> <code>show</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, show the plot.</p> </li> <li> <code>plot_path</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path where to save the plot. By default the plot is not saved.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>max_eval_number</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The maximum evaluations number to be displayed. If 0, this value is inferred from the longest history.</p> </li> <li> <code>plot_kwargs</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> <li> <code>grid_kwargs</code>               (<code>Mapping[str, str]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.pyplot.grid</code>.</p> </li> <li> <code>use_evaluation_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the number of function evaluations axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problems_group.py</code> <pre><code>def compute_data_profile(\n    self,\n    algos_configurations: AlgorithmsConfigurations,\n    histories_paths: Results,\n    show: bool = True,\n    plot_path: str | Path | None = None,\n    infeasibility_tolerance: float = 0.0,\n    max_eval_number: int = 0,\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n    grid_kwargs: Mapping[str, str] = READ_ONLY_EMPTY_DICT,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Generate the data profiles of given algorithms relative to the problems.\n\n    Args:\n        algos_configurations: The algorithms configurations.\n        histories_paths: The paths to the reference histories for each algorithm.\n        show: If True, show the plot.\n        plot_path: The path where to save the plot.\n            By default the plot is not saved.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        max_eval_number: The maximum evaluations number to be displayed.\n            If 0, this value is inferred from the longest history.\n        plot_kwargs: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n        grid_kwargs: The keyword arguments of `matplotlib.pyplot.grid`.\n        use_evaluation_log_scale: Whether to use a logarithmic scale\n            for the number of function evaluations axis.\n    \"\"\"\n    # Initialize the data profile\n    target_values = {\n        problem.name: problem.target_values for problem in self.__problems\n    }\n    data_profile = DataProfile(target_values)\n\n    # Generate the performance histories\n    for configuration_name in algos_configurations.names:\n        for problem in self.__problems:\n            for history_path in histories_paths.get_paths(\n                configuration_name, problem.name\n            ):\n                history = PerformanceHistory.from_file(history_path)\n                if max_eval_number:\n                    history = history.shorten(max_eval_number)\n                history.apply_infeasibility_tolerance(infeasibility_tolerance)\n                data_profile.add_history(\n                    problem.name,\n                    configuration_name,\n                    history.objective_values,\n                    history.infeasibility_measures,\n                )\n\n    # Plot and/or save the data profile\n    data_profile.plot(\n        show=show,\n        file_path=plot_path,\n        plot_kwargs=plot_kwargs,\n        grid_kwargs=grid_kwargs,\n        use_evaluation_log_scale=use_evaluation_log_scale,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup.compute_targets","title":"compute_targets","text":"<pre><code>compute_targets(\n    targets_number: int,\n    ref_algos_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n) -&gt; None\n</code></pre> <p>Generate targets for all the problems based on given reference algorithms.</p> <p>Parameters:</p> <ul> <li> <code>targets_number</code>               (<code>int</code>)           \u2013            <p>The number of targets to generate.</p> </li> <li> <code>ref_algos_configurations</code>               (<code>AlgorithmsConfigurations</code>)           \u2013            <p>The configurations of the reference algorithms.</p> </li> <li> <code>only_feasible</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate only feasible targets.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problems_group.py</code> <pre><code>def compute_targets(\n    self,\n    targets_number: int,\n    ref_algos_configurations: AlgorithmsConfigurations,\n    only_feasible: bool = True,\n) -&gt; None:\n    \"\"\"Generate targets for all the problems based on given reference algorithms.\n\n    Args:\n        targets_number: The number of targets to generate.\n        ref_algos_configurations: The configurations of the reference algorithms.\n        only_feasible: Whether to generate only feasible targets.\n    \"\"\"\n    for problem in self.__problems:\n        problem.compute_targets(\n            targets_number, ref_algos_configurations, only_feasible\n        )\n</code></pre>"},{"location":"reference/gemseo_benchmark/problems/problems_group/#gemseo_benchmark.problems.problems_group.ProblemsGroup.is_algorithm_suited","title":"is_algorithm_suited","text":"<pre><code>is_algorithm_suited(name: str) -&gt; bool\n</code></pre> <p>Check whether an algorithm is suited to all the problems in the group.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the algorithm is suited.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/problems/problems_group.py</code> <pre><code>def is_algorithm_suited(self, name: str) -&gt; bool:\n    \"\"\"Check whether an algorithm is suited to all the problems in the group.\n\n    Args:\n        name: The name of the algorithm.\n\n    Returns:\n        True if the algorithm is suited.\n    \"\"\"\n    return all(problem.is_algorithm_suited(name) for problem in self.__problems)\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/","title":"Report","text":""},{"location":"reference/gemseo_benchmark/report/#gemseo_benchmark.report","title":"report","text":"<p>Generation of a benchmarking report.</p>"},{"location":"reference/gemseo_benchmark/report/conf/","title":"Conf","text":""},{"location":"reference/gemseo_benchmark/report/conf/#gemseo_benchmark.report.conf","title":"conf","text":"<p>Configuration of the benchmarking report.</p>"},{"location":"reference/gemseo_benchmark/report/report/","title":"Report","text":""},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report","title":"report","text":"<p>Generation of a benchmarking report.</p>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.DirectoryName","title":"DirectoryName","text":"<p>               Bases: <code>Enum</code></p> <p>The name of a report directory.</p>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.FileName","title":"FileName","text":"<p>               Bases: <code>Enum</code></p> <p>The name of a report file.</p>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.Report","title":"Report","text":"<pre><code>Report(\n    root_directory_path: str | Path,\n    algos_configurations_groups: Iterable[\n        AlgorithmsConfigurations\n    ],\n    problems_groups: Iterable[ProblemsGroup],\n    histories_paths: Results,\n    custom_algos_descriptions: (\n        Mapping[str, str] | None\n    ) = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_kwargs: Mapping[\n        str, ConfigurationPlotOptions\n    ] = READ_ONLY_EMPTY_DICT,\n)\n</code></pre> <p>A benchmarking report.</p> <p>Parameters:</p> <ul> <li> <code>root_directory_path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the root directory of the report.</p> </li> <li> <code>algos_configurations_groups</code>               (<code>Iterable[AlgorithmsConfigurations]</code>)           \u2013            <p>The groups of algorithms configurations.</p> </li> <li> <code>problems_groups</code>               (<code>Iterable[ProblemsGroup]</code>)           \u2013            <p>The groups of reference problems.</p> </li> <li> <code>histories_paths</code>               (<code>Results</code>)           \u2013            <p>The paths to the reference histories for each algorithm and reference problem.</p> </li> <li> <code>custom_algos_descriptions</code>               (<code>Mapping[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom descriptions of the algorithms, to be printed in the report instead of the default ones coded in GEMSEO.</p> </li> <li> <code>max_eval_number_per_group</code>               (<code>dict[str, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum evaluations numbers to be displayed on the graphs of each group. The keys are the groups names and the values are the maximum evaluations numbers for the graphs of the group. If <code>None</code>, all the evaluations are displayed. If the key of a group is missing, all the evaluations are displayed for the group.</p> </li> <li> <code>plot_kwargs</code>               (<code>Mapping[str, ConfigurationPlotOptions]</code>, default:                   <code>READ_ONLY_EMPTY_DICT</code> )           \u2013            <p>The keyword arguments of <code>matplotlib.axes.Axes.plot</code> for each algorithm configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an algorithm has no associated histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/report.py</code> <pre><code>def __init__(\n    self,\n    root_directory_path: str | Path,\n    algos_configurations_groups: Iterable[AlgorithmsConfigurations],\n    problems_groups: Iterable[ProblemsGroup],\n    histories_paths: Results,\n    custom_algos_descriptions: Mapping[str, str] | None = None,\n    max_eval_number_per_group: dict[str, int] | None = None,\n    plot_kwargs: Mapping[str, ConfigurationPlotOptions] = READ_ONLY_EMPTY_DICT,\n) -&gt; None:\n    \"\"\"\n    Args:\n        root_directory_path: The path to the root directory of the report.\n        algos_configurations_groups: The groups of algorithms configurations.\n        problems_groups: The groups of reference problems.\n        histories_paths: The paths to the reference histories for each algorithm\n            and reference problem.\n        custom_algos_descriptions: Custom descriptions of the algorithms,\n            to be printed in the report instead of the default ones coded in GEMSEO.\n        max_eval_number_per_group: The maximum evaluations numbers to be displayed\n            on the graphs of each group.\n            The keys are the groups names and the values are the maximum\n            evaluations numbers for the graphs of the group.\n            If ``None``, all the evaluations are displayed.\n            If the key of a group is missing, all the evaluations are displayed\n            for the group.\n        plot_kwargs: The keyword arguments of `matplotlib.axes.Axes.plot`\n            for each algorithm configuration.\n\n    Raises:\n        ValueError: If an algorithm has no associated histories.\n    \"\"\"  # noqa: D205, D212, D415\n    names = []\n    for group in algos_configurations_groups:\n        for name in group.names:\n            if name not in names:\n                names.append(name)\n\n    self.__plot_kwargs = plot_kwargs\n    self.__root_directory = Path(root_directory_path)\n    self.__algorithms_configurations_groups = algos_configurations_groups\n    self.__problems_groups = problems_groups\n    self.__histories_paths = histories_paths\n    if custom_algos_descriptions is None:\n        custom_algos_descriptions = {}\n\n    self.__custom_algos_descriptions = custom_algos_descriptions\n    algos_diff = set().union(*[\n        group.names for group in algos_configurations_groups\n    ]) - set(histories_paths.algorithms)\n    if algos_diff:\n        msg = (\n            f\"Missing histories for algorithm{'s' if len(algos_diff) &gt; 1 else ''} \"\n            f\"{', '.join([f'{name!r}' for name in sorted(algos_diff)])}.\"\n        )\n        raise ValueError(msg)\n\n    self.__max_eval_numbers = max_eval_number_per_group or {\n        group.name: None for group in problems_groups\n    }\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.Report-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report.Report.generate","title":"generate","text":"<pre><code>generate(\n    to_html: bool = True,\n    to_pdf: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    plot_all_histories: bool = True,\n    use_log_scale: bool = False,\n    plot_only_median: bool = False,\n    use_time_log_scale: bool = False,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None\n</code></pre> <p>Generate the benchmarking report.</p> <p>Parameters:</p> <ul> <li> <code>to_html</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to generate the report in HTML format.</p> </li> <li> <code>to_pdf</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to generate the report in PDF format.</p> </li> <li> <code>infeasibility_tolerance</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The tolerance on the infeasibility measure.</p> </li> <li> <code>plot_all_histories</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>use_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale on the value axis.</p> </li> <li> <code>plot_only_median</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot only the median and no other centile.</p> </li> <li> <code>use_time_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the time axis.</p> </li> <li> <code>use_evaluation_log_scale</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use a logarithmic scale for the number of function evaluations axis.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/report/report.py</code> <pre><code>def generate(\n    self,\n    to_html: bool = True,\n    to_pdf: bool = False,\n    infeasibility_tolerance: float = 0.0,\n    plot_all_histories: bool = True,\n    use_log_scale: bool = False,\n    plot_only_median: bool = False,\n    use_time_log_scale: bool = False,\n    use_evaluation_log_scale: bool = False,\n) -&gt; None:\n    \"\"\"Generate the benchmarking report.\n\n    Args:\n        to_html: Whether to generate the report in HTML format.\n        to_pdf: Whether to generate the report in PDF format.\n        infeasibility_tolerance: The tolerance on the infeasibility measure.\n        plot_all_histories: Whether to plot all the performance histories.\n        use_log_scale: Whether to use a logarithmic scale on the value axis.\n        plot_only_median: Whether to plot only the median and no other centile.\n        use_time_log_scale: Whether to use a logarithmic scale\n            for the time axis.\n        use_evaluation_log_scale: Whether to use a logarithmic scale\n            for the number of function evaluations axis.\n    \"\"\"\n    self.__create_root_directory()\n    self.__create_algos_file()\n    self.__create_problems_files()\n    self.__create_results_files(\n        infeasibility_tolerance,\n        plot_all_histories,\n        use_log_scale,\n        plot_only_median,\n        use_time_log_scale,\n        use_evaluation_log_scale,\n    )\n    self.__create_index()\n    self.__build_report(to_html, to_pdf)\n</code></pre>"},{"location":"reference/gemseo_benchmark/report/report/#gemseo_benchmark.report.report-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/","title":"Results","text":""},{"location":"reference/gemseo_benchmark/results/#gemseo_benchmark.results","title":"results","text":"<p>Management of paths to performance histories.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/","title":"History item","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item","title":"history_item","text":"<p>A performance history item.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem","title":"HistoryItem","text":"<pre><code>HistoryItem(\n    objective_value: float,\n    infeasibility_measure: float,\n    n_unsatisfied_constraints: int | None = None,\n)\n</code></pre> <p>A performance history item.</p> <p>Parameters:</p> <ul> <li> <code>objective_value</code>               (<code>float</code>)           \u2013            <p>The objective function value of the item.</p> </li> <li> <code>infeasibility_measure</code>               (<code>float</code>)           \u2013            <p>The infeasibility measure of the item.</p> </li> <li> <code>n_unsatisfied_constraints</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of unsatisfied constraints of the item. If <code>None</code>, it will be set to 0 if the infeasibility measure is zero, and if the infeasibility measure is positive it will be set to None.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>def __init__(\n    self,\n    objective_value: float,\n    infeasibility_measure: float,\n    n_unsatisfied_constraints: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        objective_value: The objective function value of the item.\n        infeasibility_measure: The infeasibility measure of the item.\n        n_unsatisfied_constraints: The number of unsatisfied constraints of the\n            item.\n            If ``None``, it will be set to 0 if the infeasibility measure is zero,\n            and if the infeasibility measure is positive it will be set to None.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__objective_value = objective_value\n    (\n        self.__infeas_measure,\n        self.__n_unsatisfied_constraints,\n    ) = HistoryItem.__get_infeasibility(\n        infeasibility_measure, n_unsatisfied_constraints\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.infeasibility_measure","title":"infeasibility_measure  <code>property</code>","text":"<pre><code>infeasibility_measure: float\n</code></pre> <p>The infeasibility measure of the history item.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the infeasibility measure is negative.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.is_feasible","title":"is_feasible  <code>property</code>","text":"<pre><code>is_feasible: bool\n</code></pre> <p>Whether the history item is feasible.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.n_unsatisfied_constraints","title":"n_unsatisfied_constraints  <code>property</code>","text":"<pre><code>n_unsatisfied_constraints: int | None\n</code></pre> <p>The number of unsatisfied constraints.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.objective_value","title":"objective_value  <code>property</code>","text":"<pre><code>objective_value: float\n</code></pre> <p>The objective value of the history item.</p>"},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/history_item/#gemseo_benchmark.results.history_item.HistoryItem.apply_infeasibility_tolerance","title":"apply_infeasibility_tolerance","text":"<pre><code>apply_infeasibility_tolerance(\n    infeasibility_tolerance: float,\n) -&gt; None\n</code></pre> <p>Apply a tolerance on the infeasibility measure.</p> <p>Mark the history item as feasible if its infeasibility measure is below the tolerance.</p> <p>Parameters:</p> <ul> <li> <code>infeasibility_tolerance</code>               (<code>float</code>)           \u2013            <p>the tolerance on the infeasibility measure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/history_item.py</code> <pre><code>def apply_infeasibility_tolerance(self, infeasibility_tolerance: float) -&gt; None:\n    \"\"\"Apply a tolerance on the infeasibility measure.\n\n    Mark the history item as feasible if its infeasibility measure is below the\n    tolerance.\n\n    Args:\n        infeasibility_tolerance: the tolerance on the infeasibility measure.\n    \"\"\"\n    if self.__infeas_measure &lt;= infeasibility_tolerance:\n        self.__infeas_measure = 0.0\n        self.__n_unsatisfied_constraints = 0\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/","title":"Performance histories","text":""},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories","title":"performance_histories","text":"<p>A class to implement a collection of performance histories.</p>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories","title":"PerformanceHistories","text":"<pre><code>PerformanceHistories(*histories: PerformanceHistory)\n</code></pre> <p>               Bases: <code>MutableSequence</code></p> <p>A collection of performance histories.</p> <p>Parameters:</p> <ul> <li> <code>*histories</code>               (<code>PerformanceHistory</code>, default:                   <code>()</code> )           \u2013            <p>The performance histories.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def __init__(self, *histories: PerformanceHistory) -&gt; None:\n    \"\"\"\n    Args:\n        *histories: The performance histories.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__histories = list(histories)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.compute_maximum","title":"compute_maximum","text":"<pre><code>compute_maximum() -&gt; PerformanceHistory\n</code></pre> <p>Return the itemwise maximum history of the collection.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The itemwise maximum history of the collection.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def compute_maximum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the itemwise maximum history of the collection.\n\n    Returns:\n        The itemwise maximum history of the collection.\n    \"\"\"\n    return self.__compute_itemwise_statistic(max)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.compute_median","title":"compute_median","text":"<pre><code>compute_median(\n    compute_low_median: bool = True,\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the itemwise median history of the collection.</p> <p>Parameters:</p> <ul> <li> <code>compute_low_median</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to compute the low median (rather than the high median).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The itemwise median history of the collection.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def compute_median(self, compute_low_median: bool = True) -&gt; PerformanceHistory:\n    \"\"\"Return the itemwise median history of the collection.\n\n    Args:\n        compute_low_median: Whether to compute the low median\n            (rather than the high median).\n\n    Returns:\n        The itemwise median history of the collection.\n    \"\"\"\n    if compute_low_median:\n        return self.__compute_itemwise_statistic(statistics.median_low)\n\n    return self.__compute_itemwise_statistic(statistics.median_high)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.compute_minimum","title":"compute_minimum","text":"<pre><code>compute_minimum() -&gt; PerformanceHistory\n</code></pre> <p>Return the itemwise minimum history of the collection.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The itemwise minimum history of the collection.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def compute_minimum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the itemwise minimum history of the collection.\n\n    Returns:\n        The itemwise minimum history of the collection.\n    \"\"\"\n    return self.__compute_itemwise_statistic(min)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.cumulate_minimum","title":"cumulate_minimum","text":"<pre><code>cumulate_minimum() -&gt; PerformanceHistories\n</code></pre> <p>Return the histories of the minimum.</p> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def cumulate_minimum(self) -&gt; PerformanceHistories:\n    \"\"\"Return the histories of the minimum.\"\"\"\n    return PerformanceHistories(*[\n        history.compute_cumulated_minimum() for history in self\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.get_equal_size_histories","title":"get_equal_size_histories","text":"<pre><code>get_equal_size_histories() -&gt; PerformanceHistories\n</code></pre> <p>Return the histories extended to the maximum size.</p> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def get_equal_size_histories(self) -&gt; PerformanceHistories:\n    \"\"\"Return the histories extended to the maximum size.\"\"\"\n    return PerformanceHistories(*[\n        history.extend(self.__maximum_size) for history in self\n    ])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.insert","title":"insert","text":"<pre><code>insert(index: int, history: PerformanceHistory) -&gt; None\n</code></pre> <p>Insert a performance history in the collection.</p> <p>Parameters:</p> <ul> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>The index where to insert the performance history.</p> </li> <li> <code>history</code>               (<code>PerformanceHistory</code>)           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def insert(self, index: int, history: PerformanceHistory) -&gt; None:\n    \"\"\"Insert a performance history in the collection.\n\n    Args:\n        index: The index where to insert the performance history.\n        history: The performance history.\n    \"\"\"\n    self.__histories.insert(index, history)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_algorithm_histories","title":"plot_algorithm_histories","text":"<pre><code>plot_algorithm_histories(\n    axes: Axes,\n    algorithm_name: str,\n    max_feasible_objective: float,\n    plot_all: bool,\n    color: str,\n    marker: str,\n    alpha: float,\n    markevery: MarkeveryType,\n) -&gt; float | None\n</code></pre> <p>Plot the histories associated with an algorithm.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes on which to plot the performance histories.</p> </li> <li> <code>algorithm_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> <li> <code>max_feasible_objective</code>               (<code>float</code>)           \u2013            <p>The ordinate for infeasible history items.</p> </li> <li> <code>plot_all</code>               (<code>bool</code>)           \u2013            <p>Whether to plot all the performance histories.</p> </li> <li> <code>color</code>               (<code>str</code>)           \u2013            <p>The color of the plot.</p> </li> <li> <code>marker</code>               (<code>str</code>)           \u2013            <p>The marker type of the plot.</p> </li> <li> <code>alpha</code>               (<code>float</code>)           \u2013            <p>The opacity level for overlapping areas. Refer to the Matplotlib documentation.</p> </li> <li> <code>markevery</code>               (<code>MarkeveryType</code>)           \u2013            <p>The sampling parameter for the markers of the plot. Refer to the Matplotlib documentation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float | None</code>           \u2013            <p>The minimum feasible objective value of the median history or <code>None</code> if the median history has no feasible item.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def plot_algorithm_histories(\n    self,\n    axes: Axes,\n    algorithm_name: str,\n    max_feasible_objective: float,\n    plot_all: bool,\n    color: str,\n    marker: str,\n    alpha: float,\n    markevery: MarkeveryType,\n) -&gt; float | None:\n    \"\"\"Plot the histories associated with an algorithm.\n\n    Args:\n        axes: The axes on which to plot the performance histories.\n        algorithm_name: The name of the algorithm.\n        max_feasible_objective: The ordinate for infeasible history items.\n        plot_all: Whether to plot all the performance histories.\n        color: The color of the plot.\n        marker: The marker type of the plot.\n        alpha: The opacity level for overlapping areas.\n            Refer to the Matplotlib documentation.\n        markevery: The sampling parameter for the markers of the plot.\n            Refer to the Matplotlib documentation.\n\n    Returns:\n        The minimum feasible objective value of the median history\n        or ``None`` if the median history has no feasible item.\n    \"\"\"\n    # Plot all the performance histories\n    if plot_all:\n        for history in self:\n            history.plot(axes, only_feasible=True, color=color, alpha=alpha)\n\n    # Get the minimum history, starting from its first feasible item\n    abscissas, minimum_items = self.compute_minimum().get_plot_data(feasible=True)\n    minimum_ordinates = [item.objective_value for item in minimum_items]\n\n    # Get the maximum history for the same abscissas as the minimum history\n    maximum_items = self.compute_maximum().items\n    # Replace the infeasible objective values with the maximum value\n    # N.B. Axes.fill_between requires finite values, that is why the infeasible\n    # objective values are replaced with a finite value rather than with infinity.\n    maximum_ordinates = self.__get_penalized_objective_values(\n        maximum_items, abscissas, max_feasible_objective\n    )\n\n    # Plot the area between the minimum and maximum histories.\n    axes.fill_between(abscissas, minimum_ordinates, maximum_ordinates, alpha=alpha)\n    axes.plot(abscissas, minimum_ordinates, color=color, alpha=alpha)\n    # Replace the infeasible objective values with infinity\n    maximum_ordinates = self.__get_penalized_objective_values(\n        maximum_items, abscissas, numpy.inf\n    )\n    axes.plot(abscissas, maximum_ordinates, color=color, alpha=alpha)\n\n    # Plot the median history\n    median = self.compute_median()\n    median.plot(\n        axes,\n        only_feasible=True,\n        label=algorithm_name,\n        color=color,\n        marker=marker,\n        markevery=markevery,\n    )\n\n    # Return the smallest objective value of the median\n    _, history_items = median.get_plot_data(feasible=True)\n    if history_items:\n        return min(history_items).objective_value\n    return None\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_centiles_range","title":"plot_centiles_range  <code>staticmethod</code>","text":"<pre><code>plot_centiles_range(\n    histories: ndarray,\n    axes: Axes,\n    centile_range: tuple[float, float],\n    fill_between_kwargs: Mapping[str, str],\n    infinity: float | None,\n) -&gt; None\n</code></pre> <p>Plot a range of centiles of histories data.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>ndarray</code>)           \u2013            <p>The histories data.</p> </li> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>centile_range</code>               (<code>tuple[float, float]</code>)           \u2013            <p>The range of centiles to be drawn.</p> </li> <li> <code>fill_between_kwargs</code>               (<code>Mapping[str, str]</code>)           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.fill_between</code>.</p> </li> <li> <code>infinity</code>               (<code>float | None</code>)           \u2013            <p>The substitute value for infinite ordinates.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>@staticmethod\ndef plot_centiles_range(\n    histories: numpy.ndarray,\n    axes: Axes,\n    centile_range: tuple[float, float],\n    fill_between_kwargs: Mapping[str, str],\n    infinity: float | None,\n) -&gt; None:\n    \"\"\"Plot a range of centiles of histories data.\n\n    Args:\n        histories: The histories data.\n        axes: The axes of the plot.\n        centile_range: The range of centiles to be drawn.\n        fill_between_kwargs: Keyword arguments\n            for `matplotlib.axes.Axes.fill_between`.\n        infinity: The substitute value for infinite ordinates.\n    \"\"\"\n    method = \"inverted_cdf\"  # supports numpy.inf\n    histories = numpy.nan_to_num(histories, nan=numpy.inf)\n    lower_centile = numpy.percentile(\n        histories, min(centile_range), 0, method=method\n    )\n    first_index = next(\n        (i for i, value in enumerate(lower_centile) if numpy.isfinite(value)),\n        len(lower_centile),\n    )\n    axes.plot(  # hack to get same limits/ticks\n        range(1, first_index + 1),\n        numpy.full(\n            first_index,\n            lower_centile[first_index]\n            if first_index &lt; len(lower_centile)\n            else numpy.nan,\n        ),\n        alpha=0,\n    )\n    upper_centile = numpy.percentile(\n        histories[:, first_index:], max(centile_range), 0, method=method\n    )\n\n    if infinity is not None:\n        upper_centile = numpy.nan_to_num(upper_centile, posinf=infinity)\n\n    axes.fill_between(\n        range(first_index + 1, histories.shape[1] + 1),\n        lower_centile[first_index:],\n        upper_centile,\n        **fill_between_kwargs,\n    )\n    axes.xaxis.set_major_locator(MaxNLocator(integer=True))\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_infeasibility_measure_distribution","title":"plot_infeasibility_measure_distribution","text":"<pre><code>plot_infeasibility_measure_distribution(axes: Axes) -&gt; None\n</code></pre> <p>Plot the distribution of the infeasibility measure.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def plot_infeasibility_measure_distribution(self, axes: Axes) -&gt; None:\n    \"\"\"Plot the distribution of the infeasibility measure.\n\n    Args:\n        axes: The axes of the plot.\n    \"\"\"\n    self.__plot_distribution(\n        numpy.array([history.infeasibility_measures for history in self]),\n        axes,\n        \"Infeasibility measure\",\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_median","title":"plot_median  <code>staticmethod</code>","text":"<pre><code>plot_median(\n    histories: ndarray,\n    axes: Axes,\n    plot_kwargs: Mapping[str, str | int | float],\n) -&gt; None\n</code></pre> <p>Plot a range of centiles of histories data.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>ndarray</code>)           \u2013            <p>The histories data.</p> </li> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>plot_kwargs</code>               (<code>Mapping[str, str | int | float]</code>)           \u2013            <p>Keyword arguments for <code>matplotlib.axes.Axes.plot</code>.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>@staticmethod\ndef plot_median(\n    histories: numpy.ndarray,\n    axes: Axes,\n    plot_kwargs: Mapping[str, str | int | float],\n) -&gt; None:\n    \"\"\"Plot a range of centiles of histories data.\n\n    Args:\n        histories: The histories data.\n        axes: The axes of the plot.\n        plot_kwargs: Keyword arguments for `matplotlib.axes.Axes.plot`.\n    \"\"\"\n    median = numpy.median(numpy.nan_to_num(histories, nan=numpy.inf), 0)\n    # Skip infinite values to support the ``markevery`` option.\n    first_index = next(\n        (index for index, value in enumerate(median) if numpy.isfinite(value)),\n        histories.shape[1],\n    )\n    axes.plot(\n        range(first_index + 1, histories.shape[1] + 1),\n        median[first_index:],\n        **plot_kwargs,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_number_of_unsatisfied_constraints_distribution","title":"plot_number_of_unsatisfied_constraints_distribution","text":"<pre><code>plot_number_of_unsatisfied_constraints_distribution(\n    axes: Axes,\n) -&gt; None\n</code></pre> <p>Plot the distribution of the number of unsatisfied constraints.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def plot_number_of_unsatisfied_constraints_distribution(self, axes: Axes) -&gt; None:\n    \"\"\"Plot the distribution of the number of unsatisfied constraints.\n\n    Args:\n        axes: The axes of the plot.\n    \"\"\"\n    self.__plot_distribution(\n        numpy.array([\n            [\n                numpy.nan if n is None else n\n                for n in history.n_unsatisfied_constraints\n            ]\n            for history in self\n        ]),\n        axes,\n        \"Number of unsatisfied constraints\",\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_histories/#gemseo_benchmark.results.performance_histories.PerformanceHistories.plot_performance_measure_distribution","title":"plot_performance_measure_distribution","text":"<pre><code>plot_performance_measure_distribution(\n    axes: Axes, max_feasible_objective: float | None = None\n) -&gt; None\n</code></pre> <p>Plot the distribution of the performance measure.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes of the plot.</p> </li> <li> <code>max_feasible_objective</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum feasible objective value.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_histories.py</code> <pre><code>def plot_performance_measure_distribution(\n    self, axes: Axes, max_feasible_objective: float | None = None\n) -&gt; None:\n    \"\"\"Plot the distribution of the performance measure.\n\n    Args:\n        axes: The axes of the plot.\n        max_feasible_objective: The maximum feasible objective value.\n    \"\"\"\n    if max_feasible_objective is None:\n        max_feasible_objective = max(\n            item.objective_value\n            for history in self\n            for item in history\n            if item.is_feasible\n        )\n\n    self.__plot_distribution(\n        numpy.array([\n            [\n                item.objective_value if item.is_feasible else numpy.nan\n                for item in history\n            ]\n            for history in self.get_equal_size_histories()\n        ]),\n        axes,\n        \"Performance measure\",\n        max_feasible_objective,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/","title":"Performance history","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history","title":"performance_history","text":"<p>Class that generates performance measures out of data generated by an algorithm.</p> <p>Iterative algorithms that solve, for example, optimization problems or equations produce histories of data such as the value of the objective to minimize, or the size of the equation residual, at each iteration. The best value obtained up until each iteration can be generated out of this data. Here we call \"performance history\" the history of the best values obtained up until each iteration.</p> <p>Infeasible data can be discarded based upon histories of infeasibility measures or boolean feasibility statuses.</p> <p>Performance histories can be used to generate target values for a problem, or to generate the data profile of an algorithm.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory","title":"PerformanceHistory","text":"<pre><code>PerformanceHistory(\n    objective_values: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    n_unsatisfied_constraints: Sequence[int] | None = None,\n    problem_name: str | None = None,\n    objective_name: str | None = None,\n    constraints_names: Sequence[str] | None = None,\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: (\n        AlgorithmConfiguration | None\n    ) = None,\n    number_of_variables: int | None = None,\n)\n</code></pre> <p>               Bases: <code>Sequence</code></p> <p>A history of performance measures generated by an algorithm.</p> <p>A :class:<code>.PerformanceHistory</code> is a sequence of :class:<code>.HistoryItem</code>\\ s.</p> <p>Attributes:</p> <ul> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> <li> <code>total_time</code>               (<code>float</code>)           \u2013            <p>The run time of the algorithm.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>objective_values</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of the quantity to be minimized. If <code>None</code>, will be considered empty.</p> </li> <li> <code>infeasibility_measures</code>               (<code>Sequence[float] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of infeasibility measures. An infeasibility measure is a non-negative real number representing the gap between the design and the feasible space, a zero value meaning feasibility. If <code>None</code> and <code>feasibility_statuses</code> is not None then the infeasibility measures are set to zero in case of feasibility, and set to infinity otherwise. If <code>None</code> and <code>feasibility_statuses</code> is None then every infeasibility measure is set to zero.</p> </li> <li> <code>feasibility_statuses</code>               (<code>Sequence[bool] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of the (boolean) feasibility statuses. If <code>infeasibility_measures</code> is not None then <code>feasibility_statuses</code> is disregarded. If <code>None</code> and 'infeasibility_measures' is None then every infeasibility measure is set to zero.</p> </li> <li> <code>n_unsatisfied_constraints</code>               (<code>Sequence[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The history of the number of unsatisfied constraints. If <code>None</code>, the entries will be set to 0 for feasible entries and None for infeasible entries.</p> </li> <li> <code>problem_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the problem. If <code>None</code>, it will not be set.</p> </li> <li> <code>objective_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the objective function. If <code>None</code>, it will not be set.</p> </li> <li> <code>constraints_names</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>The names the scalar constraints. Each name must correspond to a scalar value. If <code>None</code>, it will not be set.</p> </li> <li> <code>doe_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The size of the initial design of experiments. If <code>None</code>, it will not be set.</p> </li> <li> <code>total_time</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>The total time of the optimization, in seconds. If <code>None</code>, it will not be set.</p> </li> <li> <code>algorithm_configuration</code>               (<code>AlgorithmConfiguration | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the algorithm which generated the history. If <code>None</code>, it will not be set.</p> </li> <li> <code>number_of_variables</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of optimization variables. If <code>None</code>, it will not be set.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the lengths of the histories do not match.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def __init__(\n    self,\n    objective_values: Sequence[float] | None = None,\n    infeasibility_measures: Sequence[float] | None = None,\n    feasibility_statuses: Sequence[bool] | None = None,\n    n_unsatisfied_constraints: Sequence[int] | None = None,\n    problem_name: str | None = None,\n    objective_name: str | None = None,\n    constraints_names: Sequence[str] | None = None,\n    doe_size: int | None = None,\n    total_time: float | None = None,\n    algorithm_configuration: AlgorithmConfiguration | None = None,\n    number_of_variables: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        objective_values: The history of the quantity to be minimized.\n            If ``None``, will be considered empty.\n        infeasibility_measures: The history of infeasibility measures.\n            An infeasibility measure is a non-negative real number representing\n            the gap between the design and the feasible space,\n            a zero value meaning feasibility.\n            If ``None`` and `feasibility_statuses` is not None\n            then the infeasibility measures are set to zero in case of feasibility,\n            and set to infinity otherwise.\n            If ``None`` and `feasibility_statuses` is None\n            then every infeasibility measure is set to zero.\n        feasibility_statuses: The history of the (boolean) feasibility statuses.\n            If `infeasibility_measures` is not None then `feasibility_statuses` is\n            disregarded.\n            If ``None`` and 'infeasibility_measures' is None\n            then every infeasibility measure is set to zero.\n        n_unsatisfied_constraints: The history of the number of unsatisfied\n            constraints.\n            If ``None``, the entries will be set to 0 for feasible entries\n            and None for infeasible entries.\n        problem_name: The name of the problem.\n            If ``None``, it will not be set.\n        objective_name: The name of the objective function.\n            If ``None``, it will not be set.\n        constraints_names: The names the scalar constraints.\n            Each name must correspond to a scalar value.\n            If ``None``, it will not be set.\n        doe_size: The size of the initial design of experiments.\n            If ``None``, it will not be set.\n        total_time: The total time of the optimization, in seconds.\n            If ``None``, it will not be set.\n        algorithm_configuration: The name of the algorithm which generated the\n            history.\n            If ``None``, it will not be set.\n        number_of_variables: The number of optimization variables.\n            If ``None``, it will not be set.\n\n    Raises:\n        ValueError: If the lengths of the histories do not match.\n    \"\"\"  # noqa: D205, D212, D415\n    if constraints_names is None:\n        self._constraints_names = []\n    else:\n        self._constraints_names = constraints_names\n\n    self._objective_name = objective_name\n    self.algorithm_configuration = algorithm_configuration\n    self.doe_size = doe_size\n    self.items = self.__get_history_items(\n        objective_values,\n        infeasibility_measures,\n        feasibility_statuses,\n        n_unsatisfied_constraints,\n    )\n    self.problem_name = problem_name\n    self._number_of_variables = number_of_variables\n    self.total_time = total_time\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.infeasibility_measures","title":"infeasibility_measures  <code>property</code>","text":"<pre><code>infeasibility_measures: list[float]\n</code></pre> <p>The infeasibility measures.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.items","title":"items  <code>property</code> <code>writable</code>","text":"<pre><code>items: list[HistoryItem]\n</code></pre> <p>The history items.</p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If an item is set with a type different from HistoryItem.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.n_unsatisfied_constraints","title":"n_unsatisfied_constraints  <code>property</code>","text":"<pre><code>n_unsatisfied_constraints: list[int]\n</code></pre> <p>The numbers of unsatisfied constraints.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.objective_values","title":"objective_values  <code>property</code>","text":"<pre><code>objective_values: list[float]\n</code></pre> <p>The objective values.</p>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.apply_infeasibility_tolerance","title":"apply_infeasibility_tolerance","text":"<pre><code>apply_infeasibility_tolerance(\n    infeasibility_tolerance: float,\n) -&gt; None\n</code></pre> <p>Apply a tolerance on the infeasibility measures of the history items.</p> <p>Mark the history items with an infeasibility measure below the tolerance as feasible.</p> <p>Parameters:</p> <ul> <li> <code>infeasibility_tolerance</code>               (<code>float</code>)           \u2013            <p>the tolerance on the infeasibility measure.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def apply_infeasibility_tolerance(self, infeasibility_tolerance: float) -&gt; None:\n    \"\"\"Apply a tolerance on the infeasibility measures of the history items.\n\n    Mark the history items with an infeasibility measure below the tolerance\n    as feasible.\n\n    Args:\n        infeasibility_tolerance: the tolerance on the infeasibility measure.\n    \"\"\"\n    for item in self.items:\n        item.apply_infeasibility_tolerance(infeasibility_tolerance)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.compute_cumulated_minimum","title":"compute_cumulated_minimum","text":"<pre><code>compute_cumulated_minimum() -&gt; PerformanceHistory\n</code></pre> <p>Return the history of the cumulated minimum.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The history of the cumulated minimum.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def compute_cumulated_minimum(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history of the cumulated minimum.\n\n    Returns:\n        The history of the cumulated minimum.\n    \"\"\"\n    minimum_history = copy(self)\n    minimum_history.items = [\n        reduce(min, self.__items[: i + 1]) for i in range(len(self))\n    ]\n    return minimum_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.compute_maximum_history","title":"compute_maximum_history  <code>staticmethod</code>","text":"<pre><code>compute_maximum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the maximum of several performance histories.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>Iterable[PerformanceHistory]</code>)           \u2013            <p>The performance histories</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The maximum history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@staticmethod\ndef compute_maximum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory:\n    \"\"\"Return the maximum of several performance histories.\n\n    Args:\n        histories: The performance histories\n\n    Returns:\n        The maximum history.\n    \"\"\"\n    from gemseo_benchmark.results.performance_histories import PerformanceHistories\n\n    return PerformanceHistories(*histories).compute_maximum()\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.compute_median_history","title":"compute_median_history  <code>staticmethod</code>","text":"<pre><code>compute_median_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the median of several performance histories.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>Iterable[PerformanceHistory]</code>)           \u2013            <p>The performance histories</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The median history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@staticmethod\ndef compute_median_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory:\n    \"\"\"Return the median of several performance histories.\n\n    Args:\n        histories: The performance histories\n\n    Returns:\n        The median history.\n    \"\"\"\n    from gemseo_benchmark.results.performance_histories import PerformanceHistories\n\n    return PerformanceHistories(*histories).compute_median()\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.compute_minimum_history","title":"compute_minimum_history  <code>staticmethod</code>","text":"<pre><code>compute_minimum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory\n</code></pre> <p>Return the minimum of several performance histories.</p> <p>Parameters:</p> <ul> <li> <code>histories</code>               (<code>Iterable[PerformanceHistory]</code>)           \u2013            <p>The performance histories</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The minimum history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@staticmethod\ndef compute_minimum_history(\n    histories: Iterable[PerformanceHistory],\n) -&gt; PerformanceHistory:\n    \"\"\"Return the minimum of several performance histories.\n\n    Args:\n        histories: The performance histories\n\n    Returns:\n        The minimum history.\n    \"\"\"\n    from gemseo_benchmark.results.performance_histories import PerformanceHistories\n\n    return PerformanceHistories(*histories).compute_minimum()\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.extend","title":"extend","text":"<pre><code>extend(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Extend the performance history by repeating its last item.</p> <p>If the history is longer than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the extended performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The extended performance history.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the expected size is smaller than the history size.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def extend(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Extend the performance history by repeating its last item.\n\n    If the history is longer than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the extended performance history.\n\n    Returns:\n        The extended performance history.\n\n    Raises:\n        ValueError: If the expected size is smaller than the history size.\n    \"\"\"\n    if size &lt; len(self):\n        msg = (\n            f\"The expected size ({size}) is smaller than \"\n            f\"the history size ({len(self)}).\"\n        )\n        raise ValueError(msg)\n\n    history = copy(self)\n    history.items = list(chain(self, repeat(self[-1], (size - len(self)))))\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str | Path) -&gt; PerformanceHistory\n</code></pre> <p>Create a new performance history from a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str | Path) -&gt; PerformanceHistory:\n    \"\"\"Create a new performance history from a file.\n\n    Args:\n        path: The path to the file.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    with Path(path).open(\"r\") as file:\n        data = json.load(file)\n\n    # Cover deprecated performance history files\n    if isinstance(data, list):\n        history = cls()\n        history.items = [\n            HistoryItem(\n                item_data[PerformanceHistory.__PERFORMANCE],\n                item_data[PerformanceHistory.__INFEASIBILITY],\n                item_data.get(PerformanceHistory.__N_UNSATISFIED_CONSTRAINTS),\n            )\n            for item_data in data\n        ]\n        return history\n\n    history = cls()\n    history.problem_name = data.get(cls.__PROBLEM)\n    history._number_of_variables = data.get(cls.__NUMBER_OF_VARIABLES)\n    history._objective_name = data[cls.__OBJECTIVE_NAME]\n    history._constraints_names = data.get(cls.__CONSTRAINTS_NAMES, [])\n    if cls.__ALGORITHM_CONFIGURATION in data:\n        history.algorithm_configuration = AlgorithmConfiguration.from_dict(\n            data[cls.__ALGORITHM_CONFIGURATION]\n        )\n\n    history.doe_size = data.get(cls.__DOE_SIZE)\n    history.total_time = data.get(cls.__EXECUTION_TIME)\n    history.items = [\n        HistoryItem(\n            item_data[PerformanceHistory.__PERFORMANCE],\n            item_data[PerformanceHistory.__INFEASIBILITY],\n            item_data.get(PerformanceHistory.__N_UNSATISFIED_CONSTRAINTS),\n        )\n        for item_data in data[cls.__HISTORY_ITEMS]\n    ]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.from_problem","title":"from_problem  <code>classmethod</code>","text":"<pre><code>from_problem(\n    problem: OptimizationProblem,\n    problem_name: str | None = None,\n) -&gt; PerformanceHistory\n</code></pre> <p>Create a performance history from a solved optimization problem.</p> <p>Parameters:</p> <ul> <li> <code>problem</code>               (<code>OptimizationProblem</code>)           \u2013            <p>The optimization problem.</p> </li> <li> <code>problem_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the problem. If <code>None</code>, the name of the problem is not set.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>@classmethod\ndef from_problem(\n    cls,\n    problem: OptimizationProblem,\n    problem_name: str | None = None,\n) -&gt; PerformanceHistory:\n    \"\"\"Create a performance history from a solved optimization problem.\n\n    Args:\n        problem: The optimization problem.\n        problem_name: The name of the problem.\n            If ``None``, the name of the problem is not set.\n\n    Returns:\n        The performance history.\n    \"\"\"\n    obj_name = problem.objective.name\n    obj_values = []\n    infeas_measures = []\n    feas_statuses = []\n    n_unsatisfied_constraints = []\n    functions_names = {obj_name, *problem.constraints.get_names()}\n    for design_values, output_values in problem.database.items():\n        # Only consider points with all functions values\n        if not functions_names &lt;= set(output_values.keys()):\n            continue\n\n        x_vect = design_values.unwrap()\n        obj_values.append(atleast_1d(output_values[obj_name]).real[0])\n        feasibility, measure = problem.history.check_design_point_is_feasible(\n            x_vect\n        )\n        number_of_unsatisfied_constraints = (\n            problem.constraints.get_number_of_unsatisfied_constraints(output_values)\n        )\n        infeas_measures.append(measure)\n        feas_statuses.append(feasibility)\n        n_unsatisfied_constraints.append(number_of_unsatisfied_constraints)\n\n    return cls(\n        obj_values,\n        infeas_measures,\n        feas_statuses,\n        n_unsatisfied_constraints,\n        problem_name,\n        problem.objective.name,\n        problem.scalar_constraint_names,\n        number_of_variables=problem.design_space.dimension,\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.get_plot_data","title":"get_plot_data","text":"<pre><code>get_plot_data(\n    feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]\n</code></pre> <p>Return the data to plot the performance history.</p> <p>Parameters:</p> <ul> <li> <code>feasible</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get only feasible values.</p> </li> <li> <code>minimum_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to get the history of the cumulated minimum instead of the history of the objective value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[int], list[HistoryItem]]</code>           \u2013            <p>The abscissas and the ordinates of the plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def get_plot_data(\n    self, feasible: bool = False, minimum_history: bool = False\n) -&gt; tuple[list[int], list[HistoryItem]]:\n    \"\"\"Return the data to plot the performance history.\n\n    Args:\n        feasible: Whether to get only feasible values.\n        minimum_history: Whether to get the history of the cumulated minimum\n            instead of the history of the objective value.\n\n    Returns:\n        The abscissas and the ordinates of the plot.\n    \"\"\"\n    history = self.compute_cumulated_minimum() if minimum_history else self\n\n    # Find the index of the first feasible history item\n    if feasible:\n        first_feasible_index = len(history)\n        for index, item in enumerate(history):\n            if item.is_feasible:\n                first_feasible_index = index\n                break\n    else:\n        first_feasible_index = 0\n\n    return (\n        list(range(first_feasible_index + 1, len(history) + 1)),\n        history[first_feasible_index:],\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.plot","title":"plot","text":"<pre><code>plot(\n    axes: Axes, only_feasible: bool, **kwargs: str | float\n) -&gt; None\n</code></pre> <p>Plot the performance history.</p> <p>Parameters:</p> <ul> <li> <code>axes</code>               (<code>Axes</code>)           \u2013            <p>The axes on which to plot the performance history.</p> </li> <li> <code>only_feasible</code>               (<code>bool</code>)           \u2013            <p>Whether to plot the feasible items only.</p> </li> <li> <code>**kwargs</code>               (<code>str | float</code>, default:                   <code>{}</code> )           \u2013            <p>The options to be passed to Axes.plot.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def plot(self, axes: Axes, only_feasible: bool, **kwargs: str | float) -&gt; None:\n    \"\"\"Plot the performance history.\n\n    Args:\n        axes: The axes on which to plot the performance history.\n        only_feasible: Whether to plot the feasible items only.\n        **kwargs: The options to be passed to Axes.plot.\n    \"\"\"\n    abscissas, history_items = self.get_plot_data(feasible=only_feasible)\n    ordinates = [item.objective_value for item in history_items]\n    axes.plot(abscissas, ordinates, **kwargs)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.remove_leading_infeasible","title":"remove_leading_infeasible","text":"<pre><code>remove_leading_infeasible() -&gt; PerformanceHistory\n</code></pre> <p>Return the history starting from the first feasible item.</p> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The truncated performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def remove_leading_infeasible(self) -&gt; PerformanceHistory:\n    \"\"\"Return the history starting from the first feasible item.\n\n    Returns:\n        The truncated performance history.\n    \"\"\"\n    first_feasible = len(self)\n    for index, item in enumerate(self):\n        if item.is_feasible:\n            first_feasible = index\n            break\n\n    truncated_history = copy(self)\n    if first_feasible is not None:\n        truncated_history.items = self.items[first_feasible:]\n\n    return truncated_history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.shorten","title":"shorten","text":"<pre><code>shorten(size: int) -&gt; PerformanceHistory\n</code></pre> <p>Shorten the performance history to a given size.</p> <p>If the history is shorter than the expected size then it will not be altered.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The expected size of the shortened performance history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PerformanceHistory</code>           \u2013            <p>The shortened performance history.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def shorten(self, size: int) -&gt; PerformanceHistory:\n    \"\"\"Shorten the performance history to a given size.\n\n    If the history is shorter than the expected size then it will not be altered.\n\n    Args:\n        size: The expected size of the shortened performance history.\n\n    Returns:\n        The shortened performance history.\n    \"\"\"\n    history = copy(self)\n    history.items = self.items[:size]\n    return history\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/performance_history/#gemseo_benchmark.results.performance_history.PerformanceHistory.to_file","title":"to_file","text":"<pre><code>to_file(path: str | Path) -&gt; None\n</code></pre> <p>Save the performance history in a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path where to write the file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/performance_history.py</code> <pre><code>def to_file(\n    self,\n    path: str | Path,\n) -&gt; None:\n    \"\"\"Save the performance history in a file.\n\n    Args:\n        path: The path where to write the file.\n    \"\"\"\n    items_data = []\n    # Add each history item in dictionary format\n    for item in self.items:\n        data_item = {\n            PerformanceHistory.__PERFORMANCE: item.objective_value,\n            PerformanceHistory.__INFEASIBILITY: item.infeasibility_measure,\n        }\n        if item.n_unsatisfied_constraints is not None:\n            # N.B. type int64 is not JSON serializable\n            data_item[PerformanceHistory.__N_UNSATISFIED_CONSTRAINTS] = int(\n                item.n_unsatisfied_constraints\n            )\n\n        items_data.append(data_item)\n\n    data = {}\n    if self.problem_name is not None:\n        data[self.__PROBLEM] = self.problem_name\n\n    if self._number_of_variables is not None:\n        data[self.__NUMBER_OF_VARIABLES] = self._number_of_variables\n\n    data[self.__OBJECTIVE_NAME] = self._objective_name\n    if self._constraints_names:\n        data[self.__CONSTRAINTS_NAMES] = self._constraints_names\n\n    if self.algorithm_configuration is not None:\n        data[self.__ALGORITHM_CONFIGURATION] = self.algorithm_configuration.to_dict(\n            True\n        )\n\n    if self.doe_size is not None:\n        data[self.__DOE_SIZE] = self.doe_size\n\n    if self.total_time is not None:\n        data[self.__EXECUTION_TIME] = self.total_time\n\n    data[self.__HISTORY_ITEMS] = items_data\n    with Path(path).open(\"w\") as file:\n        json.dump(data, file, indent=2, separators=(\",\", \": \"))\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/","title":"Results","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results","title":"results","text":"<p>A class to collect the paths to performance histories.</p>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results-classes","title":"Classes","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results","title":"Results","text":"<pre><code>Results(path: str | Path | None = None)\n</code></pre> <p>A collection of paths to performance histories.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the JSON file from which to load the paths. If <code>None</code>, the collection is initially empty.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def __init__(self, path: str | Path | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        path: The path to the JSON file from which to load the paths.\n            If ``None``, the collection is initially empty.\n    \"\"\"  # noqa: D205, D212, D415\n    self.__dict = {}\n    if path is not None:\n        self.from_file(path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results-attributes","title":"Attributes","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.algorithms","title":"algorithms  <code>property</code>","text":"<pre><code>algorithms: list[str]\n</code></pre> <p>Return the names of the algorithms configurations.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>The names of the algorithms configurations.</p> </li> </ul>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results-functions","title":"Functions","text":""},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.add_path","title":"add_path","text":"<pre><code>add_path(\n    algorithm_configuration_name: str,\n    problem_name: str,\n    path: str | Path,\n) -&gt; None\n</code></pre> <p>Add a path to a performance history.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_configuration_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration associated with the history.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem associated with the history.</p> </li> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the history.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the path to the history does not exist.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def add_path(\n    self, algorithm_configuration_name: str, problem_name: str, path: str | Path\n) -&gt; None:\n    \"\"\"Add a path to a performance history.\n\n    Args:\n        algorithm_configuration_name: The name of the algorithm configuration\n            associated with the history.\n        problem_name: The name of the problem associated with the history.\n        path: The path to the history.\n\n    Raises:\n        FileNotFoundError: If the path to the history does not exist.\n    \"\"\"\n    try:\n        absolute_path = Path(path).resolve(strict=True)\n    except FileNotFoundError:\n        msg = f\"The path to the history does not exist: {path}.\"\n        raise FileNotFoundError(msg) from None\n    if algorithm_configuration_name not in self.__dict:\n        self.__dict[algorithm_configuration_name] = {}\n\n    if problem_name not in self.__dict[algorithm_configuration_name]:\n        self.__dict[algorithm_configuration_name][problem_name] = []\n\n    self.__dict[algorithm_configuration_name][problem_name].append(absolute_path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.contains","title":"contains","text":"<pre><code>contains(\n    algo_name: str, problem_name: str, path: Path\n) -&gt; bool\n</code></pre> <p>Check whether a result is stored.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the performance history</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>Whether the result is stored.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def contains(self, algo_name: str, problem_name: str, path: Path) -&gt; bool:\n    \"\"\"Check whether a result is stored.\n\n    Args:\n        algo_name: The name of the algorithm configuration.\n        problem_name: The name of the problem.\n        path: The path to the performance history\n\n    Returns:\n        Whether the result is stored.\n    \"\"\"\n    return (\n        algo_name in self.__dict\n        and problem_name in self.__dict[algo_name]\n        and path in self.__dict[algo_name][problem_name]\n    )\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.from_file","title":"from_file","text":"<pre><code>from_file(path: str | Path) -&gt; None\n</code></pre> <p>Load paths to performance histories from a JSON file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to the JSON file.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def from_file(self, path: str | Path) -&gt; None:\n    \"\"\"Load paths to performance histories from a JSON file.\n\n    Args:\n        path: The path to the JSON file.\n    \"\"\"\n    if not Path(path).is_file():\n        msg = f\"The path to the JSON file does not exist: {path}.\"\n        raise FileNotFoundError(msg)\n\n    with Path(path).open(\"r\") as file:\n        histories = json.load(file)\n    for algo_name, problems in histories.items():\n        for problem_name, paths in problems.items():\n            for path in paths:\n                self.add_path(algo_name, problem_name, path)\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.get_paths","title":"get_paths","text":"<pre><code>get_paths(algo_name: str, problem_name: str) -&gt; list[Path]\n</code></pre> <p>Return the paths associated with an algorithm and a problem.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm.</p> </li> <li> <code>problem_name</code>               (<code>str</code>)           \u2013            <p>The name of the problem.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Path]</code>           \u2013            <p>The paths to the performance histories.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm name is unknown, or if the problem name is unknown.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def get_paths(self, algo_name: str, problem_name: str) -&gt; list[Path]:\n    \"\"\"Return the paths associated with an algorithm and a problem.\n\n    Args:\n        algo_name: The name of the algorithm.\n        problem_name: The name of the problem.\n\n    Returns:\n        The paths to the performance histories.\n\n    Raises:\n        ValueError: If the algorithm name is unknown,\n            or if the problem name is unknown.\n    \"\"\"\n    if algo_name not in self.__dict:\n        msg = f\"Unknown algorithm name: {algo_name}.\"\n        raise ValueError(msg)\n\n    if problem_name not in self.__dict[algo_name]:\n        msg = f\"Unknown problem name: {problem_name}.\"\n        raise ValueError(msg)\n\n    return self.__dict[algo_name][problem_name]\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.get_problems","title":"get_problems","text":"<pre><code>get_problems(algo_name: str) -&gt; list[str]\n</code></pre> <p>Return the names of the problems for a given algorithm configuration.</p> <p>Parameters:</p> <ul> <li> <code>algo_name</code>               (<code>str</code>)           \u2013            <p>The name of the algorithm configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>The names of the problems.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the algorithm configuration name is unknown.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def get_problems(self, algo_name: str) -&gt; list[str]:\n    \"\"\"Return the names of the problems for a given algorithm configuration.\n\n    Args:\n        algo_name: The name of the algorithm configuration.\n\n    Returns:\n        The names of the problems.\n\n    Raises:\n        ValueError: If the algorithm configuration name is unknown.\n    \"\"\"\n    if algo_name not in self.__dict:\n        msg = f\"Unknown algorithm name: {algo_name}.\"\n        raise ValueError(msg)\n\n    return list(self.__dict[algo_name])\n</code></pre>"},{"location":"reference/gemseo_benchmark/results/results/#gemseo_benchmark.results.results.Results.to_file","title":"to_file","text":"<pre><code>to_file(\n    path: str | Path, indent: int | None = None\n) -&gt; None\n</code></pre> <p>Save the histories paths to a JSON file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path where to save the JSON file.</p> </li> <li> <code>indent</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The indent level of the JSON serialization.</p> </li> </ul> Source code in <code>src/gemseo_benchmark/results/results.py</code> <pre><code>def to_file(self, path: str | Path, indent: int | None = None) -&gt; None:\n    \"\"\"Save the histories paths to a JSON file.\n\n    Args:\n        path: The path where to save the JSON file.\n        indent: The indent level of the JSON serialization.\n    \"\"\"\n    # Convert the paths to strings to be JSON serializable\n    serializable = {}\n    for algo_name, problems in self.__dict.items():\n        serializable[algo_name] = {}\n        for problem_name, paths in problems.items():\n            serializable[algo_name][problem_name] = [str(path) for path in paths]\n    with Path(path).open(\"w\") as file:\n        json.dump(serializable, file, indent=indent)\n</code></pre>"},{"location":"user_guide/","title":"User guide","text":""},{"location":"user_guide/#user-guide","title":"User guide","text":"<p>The gemseo_benchmark package provides functionalities to benchmark optimization algorithms, that is, to measure and compare their performances.</p> <p>A typical use of this package consists of the following steps:</p> <ol> <li>define the     algorithms configurations to be compared,</li> <li>define the     benchmarking problems     that will serve as landmarks for the analysis,</li> <li>execute a benchmarking scenario to produce<ol> <li>the results of the     algorithms configurations on the benchmarking problems,</li> <li>a benchmarking report     in HTML or PDF format illustrated with     data profiles.</li> </ol> </li> </ol> <p>Note</p> <p>Other algorithms will be supported in the future (ex: root-finding algorithms).</p> <p>The following sections present the sub-packages of gemseo_benchmark.</p>"},{"location":"user_guide/#algorithms-configurations","title":"Algorithms configurations","text":"<p>The gemseo_benchmark.algorithms sub-package is responsible for the definition of the algorithms configurations to be investigated in a benchmarking study.</p> <p>An AlgorithmConfiguration contains:</p> <ul> <li>the name of an algorithm,</li> <li>optionally, a name for the configuration (it will be generated     automatically if unspecified),</li> <li>the values passed as its options (default values are used for the     unspecified options).</li> </ul> <p>For example, we may consider the L-BFGS-B algorithm with its <code>maxcor</code> option (the maximum number of corrections of the Hessian approximation) set to 2.</p> <pre><code>lbfgsb_2_corrections = AlgorithmConfiguration(\n   \"L-BFGS-B\",\n   \"L-BFGS-B with 2 Hessian corrections\",\n   maxcor=2,\n)\n</code></pre> <p>Additionally, we may consider the same algorithm with a different option value, say 20.</p> <pre><code>lbfgsb_20_corrections = AlgorithmConfiguration(\n   \"L-BFGS-B\",\n   \"L-BFGS-B with 20 Hessian corrections\",\n   maxcor=20,\n)\n</code></pre> <p>Of course it is also possible to consider an algorithm with all its options set to their default values.</p> <pre><code>slsqp_default = AlgorithmConfiguration(\"SLSQP\")\n</code></pre> <p>The class AlgorithmsConfigurations is useful to gather algorithms configurations in groups so that they be treated together in a benchmarking report.</p> <pre><code>lbfgsb_configurations = AlgorithmsConfigurations(\n    lbfgsb_2_corrections,\n    lbfgsb_20_corrections,\n    name=\"L-BFGS-B configurations\",\n)\n</code></pre>"},{"location":"user_guide/#benchmarking-problems","title":"Benchmarking problems","text":"<p>The gemseo_benchmark.problems sub-package handles the benchmarking problems, on which the performances of the algorithms configurations is to be measured.</p> <p>A Problem contains the mathematical definition of the problem, as an OptimizationProblem, and requires three other features.</p> <ol> <li>The starting points, from which the algorithms configurations should     be launched on the problem. Indeed, an algorithm may be quite     dependent on the starting point. Therefore, in the context of a     benchmarking study, it is advised to consider several starting     points.<ol> <li>One can pass the starting points directly,</li> <li>or configure their generation as a design of experiments (DOE).</li> </ol> </li> <li>The best objective value known for the problem.</li> <li>The target values,     necessary to compute     data profiles:     typically, a scale of objective functions values ranging from a     relatively easily achievable value to the best value known.     Similarly to starting points, the target values can be either passed     directly, or their generation can be configured.</li> </ol> <p>For example, we define below benchmarking problems based on Rastrigin and Rosenbrock respectively, where</p> <ul> <li> <p>5 starting points are computed by Latin hypercube sampling (LHS),</p> <pre><code>doe_settings = {\"doe_size\": 5, \"doe_algo_name\": \"lhs\"}\n</code></pre> </li> <li> <p>and the target values are passed directly as an exponential scale     towards the minimum (zero).</p> <pre><code>target_values = TargetValues([10**-4, 10**-5, 10**-6, 0.0])\n</code></pre> </li> </ul> <p>(The class TargetValues will be presented further down.)</p> <pre><code>rastrigin = Problem(\n    \"Rastrigin\",\n    Rastrigin,\n    target_values=target_values,\n    **doe_settings,\n    optimum=0.0,\n)\n\nrosenbrock = Problem(\n    \"Rosenbrock\",\n    Rosenbrock,\n    target_values=target_values,\n    **doe_settings,\n    optimum=0.0,\n)\n</code></pre> <p>Note that the second argument of Problem must be callable. For example, a five-variables benchmarking problem based on Rosenbrock may be defined as follows.</p> <pre><code>rosenbrock_5d = Problem(\n    \"Rosenbrock 5D\",\n    lambda: Rosenbrock(5),\n    target_values=target_values,\n    **doe_settings,\n    optimum=0.0,\n)\n</code></pre> <p>The class ProblemsGroup is useful to gather reference problems in groups so that they be treated together in a benchmarking report.</p> <pre><code>problems_2D = ProblemsGroup(\n    \"2-variabbles functions\",\n    [rastrigin, rosenbrock],\n    description=\"Unconstrained functions depending on 2 variables.\",\n)\n</code></pre>"},{"location":"user_guide/#results","title":"Results","text":"<p>The gemseo_benchmark.results sub-package manages the results produced by the algorithms configurations on the benchmarking problems.</p> <p>The history of the data produced by an algorithm configuration on a benchmarking problem is stored in a PerformanceHistory. More precisely:</p> <ul> <li>A value of interest in the benchmarking of algorithms is defined and     named performance value.     The most telling performance value is the     value of the objective function for an optimization problem, or the     value of a residual for a nonlinear equation.</li> <li>Each performance value is stored in a     HistoryItem, along with an     infeasibility measure (especially for problems subject to     constraints).</li> <li>A PerformanceHistory is a     sequence of HistoryItems.     The index of the sequence is understood as the 0-based number of     functions evaluations.</li> </ul> <p>A PerformanceHistory may be saved to a file in JavaScript Object Notation (JSON).</p> <p>The class Results gathers the paths to each PerformanceHistory in a benchmarking study. In practice, Results are generated by a benchmarking scenario, thanks to Benchmarker.execute.</p>"},{"location":"user_guide/#benchmarker","title":"Benchmarker","text":"<p>The gemseo_benchmark.benchmarker sub-package is responsible for the generation of the results.</p> <p>The class Benchmarker is responsible for two tasks:</p> <ol> <li>executing (possibly in parallel) the algorithms configurations on     the reference problems,</li> <li>saving the performance histories to files, and storing their paths     in Results.</li> </ol>"},{"location":"user_guide/#data-profiles","title":"Datas profiles","text":"<p>The gemseo_benchmark.data_profiles sub-package handles the computation of data profiles.</p> <p>A data profile is a graph that represents the extent to which an algorithm solves a problem (or a group of problems) for a given number of functions evaluations. To clarify this definition we need to introduce target values.</p>"},{"location":"user_guide/#target-values","title":"Target values","text":"<p>The difficulty of a benchmarking problem is represented by a scale of performance values, called target values, ranging from a relatively easily achievable value to the best value known. The most telling example of target value is the optimal value of the objective function. Target values can be thought as milestones on the trajectory towards the best value known.</p> <pre><code>target_values = TargetValues([10**-4, 10**-5, 10**-6, 0.0])\n</code></pre> <p>Since a sequence of target values is in fact a sequence of HistoryItems, the class TargetValues is a subclass of PerformanceHistory.</p>"},{"location":"user_guide/#targets-generator","title":"Targets generator","text":"<p>The target values of a problem can be handpicked but they can also be automatically computed with a generator of target values.</p> <p>A TargetsGenerator relies on algorithms chosen as references.</p> <ol> <li>The problem is solved with the reference algorithms from each     starting point.</li> <li>Instances of PerformanceHistory     representing the history of the best performance value (which is     decreasing) are computed, e.g.     \\(\\{\\min_{0\\leq i \\leq k} f(x_i)\\}_{0 \\leq k \\leq K}\\) where \\(f\\) is     the objective function and \\(x_k\\) are the values of the design     variables at iteration \\(k\\).</li> <li>A notion of median history is computed from these histories.</li> <li>Performance values are picked at uniform intervals in the median     history: these are the target values.</li> </ol>"},{"location":"user_guide/#data-profile","title":"Data profile","text":"<p>The data profile of an algorithm relative to a benchmarking problem (or a group of benchmarking problems) is the graph representing the ratio of target values reached by the algorithm relative to the number functions evaluations performed by the algorithm.</p> <p></p>"},{"location":"user_guide/#report","title":"Report","text":"<p>The gemseo_benchmark.report sub-package manages the automatic generation of a benchmarking report in PDF or HTML format describing:</p> <ul> <li>the algorithms configurations,</li> <li>the benchmarking problems,</li> <li>the results generated by the algorithms on the problems, especially     in the form of data profiles.</li> </ul>"},{"location":"user_guide/#scenario","title":"Scenario","text":"<p>The class Scenario is the highest-level class of the package: it lets the user execute the algorithms configurations on the problems and generate a benchmarking report by calling a single method.</p> <pre><code>scenario_dir = Path(\"scenario\")\nscenario_dir.mkdir()\nscenario = Scenario([lbfgsb_configurations], scenario_dir)\nresults = scenario.execute([problems_2D])\n</code></pre>"}]}